@ARTICLE{Cartwright2011-jb,
  title     = "A philosopher's view of the long road from {RCTs} to
               effectiveness",
  author    = "Cartwright, Nancy",
  journal   = "Lancet",
  publisher = "Elsevier BV",
  volume    =  377,
  number    =  9775,
  pages     = "1400--1401",
  month     =  apr,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Faul2007-le,
  title     = "{G*Power} 3: a flexible statistical power analysis program for
               the social, behavioral, and biomedical sciences",
  author    = "Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and
               Buchner, Axel",
  abstract  = "G*Power (Erdfelder, Faul, \& Buchner, 1996) was designed as a
               general stand-alone power analysis program for statistical tests
               commonly used in social and behavioral research. G*Power 3 is a
               major extension of, and improvement over, the previous versions.
               It runs on widely used computer platforms (i.e., Windows XP,
               Windows Vista, and Mac OS X 10.4) and covers many different
               statistical tests of the t, F, and chi2 test families. In
               addition, it includes power analyses for z tests and some exact
               tests. G*Power 3 provides improved effect size calculators and
               graphic options, supports both distribution-based and
               design-based input modes, and offers all types of power analyses
               in which users might be interested. Like its predecessors,
               G*Power 3 is free.",
  journal   = "Behav. Res. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  39,
  number    =  2,
  pages     = "175--191",
  month     =  may,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Garcia-Sifuentes2021-pa,
  title     = "Reporting and misreporting of sex differences in the biological
               sciences",
  author    = "Garcia-Sifuentes, Yesenia and Maney, Donna L",
  abstract  = "As part of an initiative to improve rigor and reproducibility in
               biomedical research, the U.S. National Institutes of Health now
               requires the consideration of sex as a biological variable in
               preclinical studies. This new policy has been interpreted by
               some as a call to compare males and females with each other.
               Researchers testing for sex differences may not be trained to do
               so, however, increasing risk for misinterpretation of results.
               Using a list of recently published articles curated by Woitowich
               et al. (eLife, 2020; 9:e56344), we examined reports of sex
               differences and non-differences across nine biological
               disciplines. Sex differences were claimed in the majority of the
               147 articles we analyzed; however, statistical evidence
               supporting those differences was often missing. For example,
               when a sex-specific effect of a manipulation was claimed,
               authors usually had not tested statistically whether females and
               males responded differently. Thus, sex-specific effects may be
               over-reported. In contrast, we also encountered practices that
               could mask sex differences, such as pooling the sexes without
               first testing for a difference. Our findings support the need
               for continuing efforts to train researchers how to test for and
               report sex differences in order to promote rigor and
               reproducibility in biomedical research.",
  journal   = "Elife",
  publisher = "eLife Sciences Publications, Ltd",
  volume    =  10,
  month     =  nov,
  year      =  2021,
  keywords  = "medicine; meta-research; neuroscience; none; sex differences;
               sex inclusion",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Karp2021-lc,
  title     = "What is the optimum design for my animal experiment?",
  author    = "Karp, Natasha A and Fry, Derek",
  abstract  = "Within preclinical research, attention has focused on
               experimental design and how current practices can lead to poor
               reproducibility. There are numerous decision points when
               designing experiments. Ethically, when working with animals we
               need to conduct a harm-benefit analysis to ensure the animal use
               is justified for the scientific gain. Experiments should be
               robust, not use more or fewer animals than necessary, and truly
               add to the knowledge base of science. Using case studies to
               explore these decision points, we consider how individual
               experiments can be designed in several different ways. We use
               the Experimental Design Assistant (EDA) graphical summary of
               each experiment to visualise the design differences and then
               consider the strengths and weaknesses of each design. Through
               this format, we explore key and topical experimental design
               issues such as pseudo-replication, blocking, covariates, sex
               bias, inference space, standardisation fallacy and factorial
               designs. There are numerous articles discussing these critical
               issues in the literature, but here we bring together these
               topics and explore them using real-world examples allowing the
               implications of the choice of design to be considered.
               Fundamentally, there is no perfect experiment; choices must be
               made which will have an impact on the conclusions that can be
               drawn. We need to understand the limitations of an experiment's
               design and when we report the experiments, we need to share the
               caveats that inherently exist.",
  journal   = "BMJ Open Sci.",
  publisher = "Portico",
  volume    =  5,
  number    =  1,
  pages     = "e100126",
  month     =  mar,
  year      =  2021,
  keywords  = "animal; biostatistics; disease models; models; research design",
  language  = "en"
}

@ARTICLE{Kass1995-hs,
  title     = "Bayes factors",
  author    = "Kass, Robert E and Raftery, Adrian E",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Informa UK Limited",
  volume    =  90,
  number    =  430,
  pages     = "773--795",
  month     =  jun,
  year      =  1995,
  language  = "en"
}

@ARTICLE{Kilkenny2009-cn,
  title     = "Survey of the quality of experimental design, statistical
               analysis and reporting of research using animals",
  author    = "Kilkenny, Carol and Parsons, Nick and Kadyszewski, Ed and
               Festing, Michael F W and Cuthill, Innes C and Fry, Derek and
               Hutton, Jane and Altman, Douglas G",
  abstract  = "For scientific, ethical and economic reasons, experiments
               involving animals should be appropriately designed, correctly
               analysed and transparently reported. This increases the
               scientific validity of the results, and maximises the knowledge
               gained from each experiment. A minimum amount of relevant
               information must be included in scientific publications to
               ensure that the methods and results of a study can be reviewed,
               analysed and repeated. Omitting essential information can raise
               scientific and ethical concerns. We report the findings of a
               systematic survey of reporting, experimental design and
               statistical analysis in published biomedical research using
               laboratory animals. Medline and EMBASE were searched for studies
               reporting research on live rats, mice and non-human primates
               carried out in UK and US publicly funded research
               establishments. Detailed information was collected from 271
               publications, about the objective or hypothesis of the study,
               the number, sex, age and/or weight of animals used, and
               experimental and statistical methods. Only 59\% of the studies
               stated the hypothesis or objective of the study and the number
               and characteristics of the animals used. Appropriate and
               efficient experimental design is a critical component of
               high-quality science. Most of the papers surveyed did not use
               randomisation (87\%) or blinding (86\%), to reduce bias in
               animal selection and outcome assessment. Only 70\% of the
               publications that used statistical methods described their
               methods and presented the results with a measure of error or
               variability. This survey has identified a number of issues that
               need to be addressed in order to improve experimental design and
               reporting in publications describing research using animals.
               Scientific publication is a powerful and important source of
               information; the authors of scientific publications therefore
               have a responsibility to describe their methods and results
               comprehensively, accurately and transparently, and peer
               reviewers and journal editors share the responsibility to ensure
               that published studies fulfil these criteria.",
  journal   = "PLoS One",
  publisher = "Public Library of Science (PLoS)",
  volume    =  4,
  number    =  11,
  pages     = "e7824",
  month     =  nov,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Kilkenny2010-cp,
  title     = "Improving bioscience research reporting: the {ARRIVE} guidelines
               for reporting animal research",
  author    = "Kilkenny, Carol and Browne, William J and Cuthill, Innes C and
               Emerson, Michael and Altman, Douglas G",
  journal   = "PLoS Biol.",
  publisher = "Public Library of Science (PLoS)",
  volume    =  8,
  number    =  6,
  pages     = "e1000412",
  month     =  jun,
  year      =  2010,
  language  = "en"
}

@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97â€“111},
  numpages = {15}
}

@ARTICLE{Mayr2007-ev,
  title     = "A short tutorial of {GPower}",
  author    = "Mayr, Susanne and Erdfelder, Edgar and Buchner, Axel and Faul,
               Franz",
  journal   = "Tutor. Quant. Methods Psychol.",
  publisher = "The Quantitative Methods for Psychology",
  volume    =  3,
  number    =  2,
  pages     = "51--59",
  month     =  sep,
  year      =  2007
}

@ARTICLE{Nieuwenhuis2011-nl,
  title     = "Erroneous analyses of interactions in neuroscience: a problem of
               significance",
  author    = "Nieuwenhuis, Sander and Forstmann, Birte U and Wagenmakers,
               Eric-Jan",
  abstract  = "In theory, a comparison of two experimental effects requires a
               statistical test on their difference. In practice, this
               comparison is often based on an incorrect procedure involving
               two separate tests in which researchers conclude that effects
               differ when one effect is significant (P 0.05). We reviewed 513
               behavioral, systems and cognitive neuroscience articles in five
               top-ranking journals (Science, Nature, Nature Neuroscience,
               Neuron and The Journal of Neuroscience) and found that 78 used
               the correct procedure and 79 used the incorrect procedure. An
               additional analysis suggests that incorrect analyses of
               interactions are even more common in cellular and molecular
               neuroscience. We discuss scenarios in which the erroneous
               procedure is particularly beguiling.",
  journal   = "Nat. Neurosci.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  14,
  number    =  9,
  pages     = "1105--1107",
  month     =  aug,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Vorland2021-wx,
  title     = "Sex difference analyses under scrutiny",
  author    = "Vorland, Colby J",
  abstract  = "A survey reveals that many researchers do not use appropriate
               statistical analyses to evaluate sex differences in biomedical
               research.",
  journal   = "Elife",
  publisher = "eLife Sciences Publications, Ltd",
  volume    =  10,
  month     =  nov,
  year      =  2021,
  keywords  = "medicine; meta-research; methodological weakness; neuroscience;
               none; sex differences; sex inclusion; statistics; study design",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}
