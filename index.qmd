---
title: "MP968 Experimental Design Workshop"
author: 
  - name: "Leighton Pritchard"
    affiliation: "University of Strathclyde"
date: "2025-11-24"

format:
  revealjs:
    slide-number: true
    controls: true
    preview-links: true
    footer: "MP968 Experimental Design Workshop"
    logo: "assets/images/sipbs_compbio_800.png"
    theme: [default, ./includes/custom.scss]
    width: 1280
    height: 720
    self-contained: true
    
revealjs-plugins:
  - quiz
  
# Location of BibTeX format reference file; may not need to be changed
bibliography: references.bib
---

```{r setup}
library(dplyr)
library(stringr)
library(tidyr)
library(ggpubr)
library(ggplot2)
library(latex2exp)

# Plot normal curves for of height counts for men and women separately and together
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

heights <- data.frame(height=54:75,
                      female=c(80,107,296,695,1612,2680,4645,8201,9948,11733,
                               10270,9942,6181,3990,2131,1154,245,257,0,
                               0,0,0)*10339/74167,
                      male=c(0,0,0,0,0,0,0,542,668,1221,2175,4213,5535,7980,
                             9566,9578,8867,6716,5019,2745,
                             1464,1263)*9983/67552) %>%
  pivot_longer(c(female, male), names_to="sex", values_to="count")

heights_f <- ggplot(heights %>% filter(sex=="female"), aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 1e4 * dnorm(x, 63.7, 2.7),
                color="orange", lwd=2) +
  xlim(53, 83) + ylim(0, 1700) +
  labs(title="Heights of female adults (US)") +
  theme_minimal()

heights_m <- ggplot(heights %>% filter(sex=="male"), aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 1e4 * dnorm(x, 69.1, 2.9),
                color="orange", lwd=2) +  
  xlim(53, 83) + ylim(0, 1700) +
  labs(title="Heights of male adults (US)") +
  theme_minimal()

heights_adults <- ggplot(heights, aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 2.1e4 * (0.52 * dnorm(x, 63.7, 2.7) +
                                       0.48 * dnorm(x, 69.1, 2.9)),
                color="orange", lwd=2) +
  xlim(53, 83) + ylim(0, 2100) +
  labs(title="Heights of all adults (US)") +
  theme_minimal()

# Example binomial and poisson distributions
# A data frame of values 1:20, with some binomial and poisson distributed values
bin_pois_data <- data.frame(count=0:20) %>%
  mutate(pois1=dpois(count, 1)) %>%
  mutate(pois4=dpois(count, 4)) %>%
  mutate(pois10=dpois(count, 10)) %>%
  mutate(binom0=dbinom(count, 20, 0.05)) %>%
  mutate(binom1=dbinom(count, 20, 0.3)) %>%
  mutate(binom2=dbinom(count, 20, 0.5)) %>%
  mutate(binom3=dbinom(count, 40, 0.5)) %>%
  pivot_longer(cols=-count, names_to="params", values_to="freq")

# Informative labels for legends
poisson_labels <- c("lambda=1", "lambda=4", "lambda=10")
binomial_labels <- c("n=20, p=0.01", "n=20, p=0.3", "n=20, p=0.7", "n=40, p=0.3")

poisson_dist <- ggplot(bin_pois_data %>% filter(str_detect(params, "^pois")), aes(x=count, color=params)) +
  geom_line(aes(y=freq), alpha=0.5) +
  geom_point(aes(y=freq)) +
  xlim(-3,20) +
  ylim(0, 0.4) +
  xlab("counts/rate") +
  labs(title="Poisson distributions") +
  scale_color_discrete(labels=poisson_labels) +
  coord_fixed(ratio=50) +
  theme_minimal()

binomial_dist <- ggplot(bin_pois_data %>% filter(str_detect(params, "^binom")), aes(x=count, color=params)) +
  geom_line(aes(y=freq), alpha=0.5) +
  geom_point(aes(y=freq)) +
  xlim(-3,20) +
  ylim(0, 0.4) +
  xlab("successes") +
  labs(title="Binomial distributions") +
  scale_color_discrete(labels=binomial_labels) +
  coord_fixed(ratio=50) +
  theme_minimal()

# Normal distribution demonstration plots
normal_distplot <- function(mu, sd) {
  # Plot figurative null hypothesis distribution
  ggplot() +
    stat_function(fun = dnorm,
                  args = list(mean = mu, sd = sd),
                  geom = "line") +
    xlim(-4 * sd, 4 * sd) +
    xlab(TeX("true difference, $\\theta$")) +
    ylab("density") +
    theme_minimal()
}

# Shade a normal curve between zstart and zend
shade_normal <- function(mu, sd, zstart, zend, fill="#00998a", alpha=0.5, textoffset=0.01) {
  xmin <- qnorm(zstart, mu, sd)
  xmax <- qnorm(zend, mu, sd)
  
  # Mesh of distribution points
  data = data.frame(x=seq(mu - 4 * sd, mu + 4 * sd, by=0.01)) %>%
    mutate(y=dnorm(x, mu, sd))
  
  # Return ggplot2 shaded area as list of <ggproto> objects
  list(
    geom_area(data=subset(data, x >= xmin & x <= xmax),
              aes(x=x, y=y), fill=fill, color=NA, alpha=0.5),
    annotate("segment", color=fill,
             x=xmin, xend=xmin,
             y=0, yend=dnorm(xmin, mu, sd)),
    annotate("text", color=fill,
            x=xmin, y=-textoffset,
            label=ifelse(zstart == 0, "", paste(100 * zstart, "%", sep=""))),
    annotate("segment", color=fill,
             x=xmax, xend=xmax,
             y=0, yend=dnorm(xmax, mu, sd)),
    annotate("text", color=fill,
             x=xmax, y=-textoffset,
             label=ifelse(zend == 1, "", paste(100 * zend, "%", sep="")))
  )
  
}

# Add a CI bar to a normal curve
ci_normal <- function(mu, sd, ci, alpha=0.4) {
  xmin <- qnorm(0.5 * (1 - ci), mu, sd)
  xmax <- qnorm(1 - 0.5 * (1 - ci), mu, sd)
  
  # Return ggplot2 annotation as list of<ggproto>s
  list(
    annotate("pointrange",
             xmin=xmin, xmax=xmax,
             x=mu, y=dnorm(xmax, mu, sd), alpha=alpha),
    annotate("text",
             x=mu, y=dnorm(xmax, mu, sd)-0.01,
             label=paste(100 * ci, "%CI", sep=""))
  )
}

# Add a dashed line marker at a given x position
add_x_marker <- function(x, y, label, linecolor, textcolor) {
  list(
    annotate("segment", # show the observed difference as a dashed line
      x = x, xend = x,
      y = 0, yend = y,
      colour = linecolor,
      size = 1, linetype = "dashed"
    ),
    annotate("text", x = x, y = y + 0.01, label = label, color = textcolor)
  )
}

# Add shaded normal curve in specific colour with labels
shaded_normal <- function(mu, sd, zstart, zend, fill="orange", color="orange", label="", alpha=0.5, textoffset=0.01) {
  list(
    stat_function(fun=dnorm, args=list(mean=mu, sd=sd), geom="line", colour=color),
    shade_normal(mu=mu, sd=sd, zstart=zstart, zend=zend, fill=fill, alpha=alpha, textoffset=textoffset),
    annotate("text", colour=color, x=mu, y=dnorm(mu, mu, sd) + 2 * textoffset, label=label),
    annotate("segment", colour=color, x=mu, xend=mu, y=0, yend=dnorm(mu, mu, sd))
  )
}

```

# Why do we need experimental design?

::: { .notes }
- Indeed, why _do_ we need experimental design?
- Well, in general it's because if your experiment is designed poorly you may not get the answer you're trying to find or - even worse - might get entirely the wrong answer to that question
- But in this workshop we're going to focus on why we need experimental design - _and its conjoined sibling, statistics_ - in the context of experiments that involve animals
:::

## We should not cause unnecessary suffering

::: { .callout-important title="We should always minimise suffering" }
**This may mean not performing an experiment at all**. Not all new knowledge or understanding is _worth_ causing suffering to obtain it.

Where there _is_ sufficient justification to perform an experiment, **we are ethically obliged to minimise the amount of distress or suffering that is caused**, by designing the experiment to achieve this.
:::

::: { .callout-warning title="Why we need statistics" }
It may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.

But what is an _acceptable_ (i.e. the _least possible_) amount of suffering necessary to obtain an informative result?
:::

::: { .notes }
- No-one like talking about animal experiments.
- It's a difficult, emotive topic that crosses people's moral red lines.
- Animal experiments may be the only practical way to gain essential scientific knowledge
- Whatever you believe "suffering" means for an animal, our _ethical premise_ is that this suffering should not be in vain
:::

## Challenge { .quiz-question }

::: { .callout-warning title="Quiz question" }
Suppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. **Which of the following choices will cause the least amount of suffering?**
:::

- [Use three subjects per group so a standard deviation can be calculated]{data-explanation="In many cases this is likely to be too few individuals for the result to be reliable"}
- [Use just enough subjects to establish that the outcome is likely to be correct]{.correct}
- [Use just enough subjects to be certain that the outcome is correct]{data-explanation="It is often impossible to determine whether the outcome of an experiment is 'correct'."}
- [Use as many subjects as you have available, to avoid wastage]{data-explanation="There may be too few individuals available, or more than are required, to obtain a reliable result."}

::: { .notes }
We carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is _more likely_ or _less likely_ to be the null hypothesis than some alternative), rather than 100% certain.

If we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. **The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering**.

If we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. **The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering**.

If we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. **The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering**.
:::

## How many individuals?

::: { .callout-tip title="The appropriate number of subjects" }
The appropriate number of animal subjects to use in an experiment is _always_ the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.

- What assumptions are reasonable?
- What is an appropriate level of certainty?

**By convention**[^1] the usual level of certainty for a hypothesis test is: "we have an 80% chance of getting the correct true/false answer for the hypothesis being tested"
:::

[^1]: Conventions are guidelines, not rigid standards, and you should always consider whether a convention is appropriate in your use case

::: { .notes }
The appropriate number of animal subjects to use in an experiment is _always_ the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.

This may sound like a very flexible statement. _What assumptions are reasonable? What is an appropriate level of certainty?_

We'll consider these questions again later but, for now, just know that **_by convention_ the usual level of certainty is: "we have an 80% chance of getting the correct true/false answer for the hypothesis being tested"._**

Note though that the appropriate level of certainty may change depending on the nature of the question being asked.
:::

## Design experiments to minimise suffering

::: { .callout-important title="Experimental design and statistics are intertwined" }
Once a research hypothesis has been devised:

- _Experimental design_ is the process of devising a practical way of answering the question
- _Statistics_ informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results
:::

::: { .callout-warning title="Design your experiment for…" }
- your **population** or subject group (e.g. sex, age, prior history, etc.)
- your **intervention** (e.g. drug treatment)
- your **contrast** or comparison between groups (e.g. lung capacity, drug concentration, etc.)
- your **outcome** (i.e. is there a _measurable_ or _clinically relevant_ effect)
:::

::: { .notes }
Once a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.

Statistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.

You should design your experiment **specifically** for your combination of population/subject group, the intervention you're applying, the contrast or comparison you're making, and the outcome you're expecting to see - specifically a measurable or clinically relevant effect.
:::

# The 2009 NC3Rs systematic survey

## The importance of experimental design { .smaller }

:::: { .columns }

::: { .column width="60%" }
> "For scientific, ethical and economic reasons, **experiments involving animals should be appropriately designed, correctly analysed and transparently reported**. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns." (@Kilkenny2009-cn)

::: { .callout-important title="We rely on the reporting of the experiment to know if it was appropriate"}
:::
:::

::: { .column width="40%" }
![](assets/images/kilkenny.png)
:::

::::

::: { .notes }
The National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.

In 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.

**It did not make for pleasant reading.**
:::



## Causes for concern 1 { .smaller }

:::: { .columns }

::: { .column width="60%" }
> "Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. **Only 59% of the studies stated the hypothesis** or objective of the study and the number and characteristics of the animals used. […]  **Most of the papers surveyed did not use randomisation (87%) or blinding (86%)**, to reduce bias in animal selection and outcome assessment. **Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability**." (@Kilkenny2009-cn)
:::

::: { .column width="40%" }
![](assets/images/kilkenny_blinding.png)
:::

::: { .callout-warning title="We cannot rely on the literature for good examples of experimental design" }
:::

::::

::: { .notes }
The state of the published literature around animal experiments was not good in 2009.

- 40% of studies did not state the hypothesis or objective of the experiment
- Most papers did not use randomisation or blinding, although this is an essential practice to avoid bias
- Only 70% of publications described their statistical methods at all
:::

## Causes for concern 2 { .smaller }

::: { .callout-important title="No publication explained their choice for the number of animals used" }
![](assets/images/kilkenny_sample_size.png){width="55%" fig-align="center"}

:::

::: { .callout-warning title="We cannot rely on the verbal authority of 'published scientists' or 'experienced scientists' for good experimental design" }
:::

::: { .notes }
- One of the most shocking pieces of information is that, of the 48 papers surveyed, **not a single one explained why they used the number of animals that they did**.
- We cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.
- These papers are published. They are written, and the experiments conducted, by "experienced scientists"
- Being an "experienced" or "published" scientist is clearly not a benchmark for good experimental design
:::

## Very strong cause for concern { .smaller }

> "**Power analysis or other very simple calculations**, which are widely used in human clinical trials and are often **expected by regulatory authorities** in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a **scientifically robust and efficient** way of determining animal numbers and may ultimately **help to prevent animals being used unnecessarily**. _Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results._" (@Kilkenny2009-cn)

::: { .callout-important }
As scientists, you - **yourselves** - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.

**You cannot simply rely on the word of "experienced scientists" for this.**
:::

::: { .notes }
- The Kilkenny paper does propose solutions to this problem
- They require the use and reporting of straightforward statistical calculations
- It is up to **you** as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods.
:::

## The ARRIVE guidelines

:::: { .columns }

::: { .column width="70%" }
The following year @Kilkenny2010-cp proposed the [ARRIVE guidelines](https://nc3rs.org.uk/our-portfolio/arrive-animal-research-reporting-vivo-experiments): a checklist to help researchers report their animal research transparently and reproducibly.

- Good reporting is essential for peer review and to inform future research
- Reporting guidelines measurably improve reporting quality
- Improved reporting maximises the output of published research
:::

::: { .column width="30%" }
![](assets/images/arrive.png)
:::

::::

## ARRIVE guidelines highlightes

Many journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (@Kilkenny2010-cp):

::: { .callout-tip title="ARRIVE guidelines (highlights)" }
- 4. **Objectives**: primary and any secondary objectives of the study, or specific hypotheses being tested
- 6. **Study design**: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit
- 10. **Sample size**: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided
- 13. **Statistical methods**: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach
- 16. **Outcomes and estimation**: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval).
:::

## A vital step

:::: { .columns }

::: { .column width="60%" }
::: { .callout-warning }
"A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and **that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues**."
:::

::: { .callout-tip title="Recommended reading" }
@Bate_Clark_2014
:::
:::

::: { .column width="40%" }
![](assets/images/bate_and_clark.jpg)
:::

::::

# Some Statistical Concepts

::: { .notes }
Before we get into the detail and fun of calculating appropriate group sizes for your experiments, we're going to refresh some relevant statistical concepts.
:::

## Random variables

Your experimental measurements are _random variables_

::: { .callout-important }
**This does not mean that your measurements are entirely random numbers**
:::

::: { .callout-caution }
Random variables are values whose range is _subject to some element of chance_, e.g. variation between individuals

- Tail length (e.g. timing of developmental signals, distribution of nutrients)
- Blood concentrations (e.g. circulatory heterogeneity, transient measurement differences)
- Survival time (e.g. determining point of death)
:::

::: { .notes }
- When you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a _random variable_
  - **This doesn't mean that the number is chosen randomly**
- What it means is that the number is subject to the influence of some kind of random variation
  - So tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals
  - Blood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration
  - Survival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time
:::

## Probability distributions { .smaller }

The probability distribution of a random variable $z$ (e.g. what you measure in an experiment) takes on some range of values[^infinite]

::: { .callout-tip title="The mean of the distribution of $z$" }
- The _mean_ (aka _expected value_ or _expectation_) is the average of all the values in $z$
  - Equivalently: the _mean_ is the value that is obtained **on average** from a random sample from the distribution
- Written as $\mu_{z}$ or $E(z)$
:::
  
::: { .callout-tip title="The variance of a distribution of $z$" }
- The _variance_ of the distribution of $z$ represents the _expected_ mean squared difference from the mean $\mu_z$ (or $E(z)$) of a random sample from the distribution.
  - $\textrm{variance} = E((z - \mu_z)^2)$
:::

[^infinite]: Here we are considering a populations of an infinite number of measurements of $z$

::: { .notes }
- A probability distribution describes the range of values that a random variable - let's call it $z$ - takes.
  - You can think of $z$ being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it
- Probability distributions can describe many measures, including
  - heights of men, incomes of women, political party preference, and so on
- The _mean_ of a probability distribution - its _expected value_ or _expectation_ is the average of all numbers in the distribution
  - This may be infinite, and there is an equivalent definition: the _mean_ is the value obtained _on average_ from a random sample taken from the distribution
- The _variance_ of a probability distribution expresses how much the individual values might differ from that mean.
  - Variance is defined as the _mean_ of the **square of the difference between each individual value in the distribution and the mean of the distribution**
:::

## Understanding variance

::: { .callout-caution title="A distribution where all values of $z$ are the same" }
- Every single value in the distribution ($z$) is also the mean value ($\mu_z$), therefore

$$z = \mu_z \implies z - \mu_z = 0 \implies (z - \mu_z)^2 = 0$$
$$\textrm{variance} = E((z - \mu_z)^2) = E(0^2) = 0$$

:::

::: { .callout-tip title="All other distributions" }
In **every** other distribution, there are some values of $z$ that differ so, for at least some values of $z$

$$z \neq \mu_z \implies  z - \mu_z \neq 0 \implies (z - \mu_z)^2 \gt 0 $$
$$\implies \textrm{variance} = E((z - \mu_z)^2) \gt 0 $$

:::

::: { .notes }
- To get some intuition about the idea of variance, imagine that you have a set of values $z$ that are all the same, so there's no randomness
- If the value are all the same, then they all have the same value as the _mean_ of the distribution, and $z - \mu_z = 0$
- This means that, for all $z$, $(z - \mu_z)^2$ is also zero, and the variance is zero.
- But if any value of $z$ is different then, for at least that value, $z - \mu_z \neq 0$, so the square of that value is greater than zero. It follows that the variance must then be greater than zero: $E((z - \mu_z)^2) \gt 0$

- In any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value
:::

## Standard deviation

::: { .callout-warning title="_Standard deviation_ is the square root of the variance" }

$$\textrm{standard deviation} = \sigma_z = \sqrt{\textrm{variance}} = \sqrt{E((z - \mu_z)^2)} $$
:::

::: { .callout-tip title="Advantages" }
- The _standard deviation_ (unlike variance) takes values on the same scale as the original distribution
  - Standard deviation is a more "natural-seeming" interpretation of variation
:::

::: { .callout-note }
We can calculate _mean_, _variance_, and _standard deviation_ for **any** probability distribution.
:::

::: { .notes }
- You are probably more familiar with the _standard deviation_ of a distribution.
- The standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance

- While we can calculate the mean, variance and standard deviation for _any_ distribution, this does **not** mean that they are equally informative for all distributions
- Let's look at some examples
:::

## Normal Distribution 1

$$ z \sim \textrm{normal}(\mu_z, \sigma_z) $$

::: { .callout-note }
We only need to know the mean and standard deviation to define a unique normal distribution
:::

::: { .callout-tip }
Measurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution
:::

::: { .callout-important }
**There is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!**
:::

::: { .notes }
- You may see the normal distribution written like this, as "z is distributed as the Normal distribution with mean $\mu_z$ and standard deviation $\sigma_z$."
  - We only need to know the mean and standard deviation to define a unique normal distribution
- Values we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors
  - This is a consequence of the Central Limit Theorem
- **But there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution**
  - This is especially the case if there is a large factor affecting variation of the variable
:::

## Normal Distribution 2 { .smaller }

```{r norm_density}
#| fig-align: center

# Plot normal curves for of height counts for men and women
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html
ggarrange(heights_f, heights_m, ncol=2)
```

::: { .callout-tip }
- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean
:::

::: { .notes }
- As an example, we can look at some real data, the heights of men and women in the US.
  - Here we have counts of individuals whose heights are measured to the nearest inch
- These are two different _random variables_: one for men, one for women
- For both distributions, we can calculate a mean, and a standard deviation (or variance)
  - Therefore we can calculate a normal distribution representing both men's and women's heights (orange curve)
  - We calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)
- The distributions of heights for each sex, separately, follow an approximate normal distribution

- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean
:::

## (Non-)Normal Distribution 3 { .smaller }

```{r all_heights}
#| fig-align: center

# Plot normal curves for of height counts for all us_adults
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

heights_adults
```

::: { .callout-tip }
- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean
:::

::: { .notes }
- By contrast, the distribution of heights of _all_ adults in the US is not close to a normal curve
- This is because there is an extraneous factor, **sex**, that represents much of the total variation in values
  - We will come back to this idea later
  
- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean

- Most data you receive will not be normally distributed
:::

## Binomial Distribution 1

:::: { .columns }

::: { .column width="60%" }
Suppose you're taking shots in basketball

- how many shots?
- how likely are you to score?
- what is the distribution of the number of successful shots?

::: { .callout-tip }
This kind of process generates a random variable approximating a probability distribution called a **binomial distribution**.

It is different from a normal distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- Suppose that instead of measuring height, weight, concentration or something like that, you're measuring _event outcomes_
  - These do not follow a normal distribution

- If you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding
- The number of successful shots is going to depend on the number of shots you take, and how likely you are to score
  - Michael Jordan is much more likely to score any individual attempt than I am
- The number of successful shots is a random variable with a probability distribution

- This kind of process generates a probability distribution that approximates the **binomial distribution**
  - It's the same one you get for coin tosses (or any yes/no process)
- It is different from a normal distribution
  - If your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution
:::

## Binomial Distribution 2 { .smaller }

:::: { .columns }

::: { .column width="60%" }
$$ z \sim \textrm{binomial}(n, p) $$

::: { .callout-tip }
- number of shots, $n = 20$
- probability of scoring, $p = 0.3$

$$z \sim \textrm{binomial}(20, 0.3) $$
:::

::: { .callout-caution title="mean and sd"}
$$ \textrm{mean} = n \times p $$
$$ \textrm{sd} = \sqrt{n \times p \times (1-p)}$$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data

- E.g., does $p$ differ between two conditions?
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- Imagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot
- The distribution of shots you scored would follow a binomial distribution with $n=20$ and $p=0.3$
- You'd expect to score, on average, $20 \times 0.3 = 6$ times
- The standard deviation we'd expect would be $\sqrt{20 \times 0.3 \times (1 - 0.3)} = \sqrt{6 \times (0.7)} = \sqrt{4.2} = 2.05$
  - So you'd expect to score about 4 to 8 shots, most of the time
:::

## Poisson distribution 1

:::: { .columns }

::: { .column width="60%" }
> In prior experiments the frequency of calcium events in WKY was 3.8 $\pm$ 1.1 events/field/min compared to 18.9 $\pm$ 7.1 in SHR

::: { .callout-warning title="This is not normal (or binomial)" }
Something that happens a certain number of times in a fixed interval generates a _Poisson distribution_. 

This is different from a normal or binomial distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/calcium_events.png)
:::

::::

::: { .notes }
- Suppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR
  - You count the number of times the calcium spikes in a minute, in your field of view
- This data is _rate_ data
  - A count of events per unit (here, unit time and unit area) _interval_
  
- The idealised representation of data generated by this process is a **Poisson distribution**
  - This differs from a normal or binomial distribution
:::

## Poisson distribution 2 { .smaller }

$$z \sim \textrm{poisson}(\lambda)$$

::: { .callout-tip title="Poisson distribution" }
$$ \textrm{mean} = \lambda $$
$$ \textrm{sd} = \sqrt{\lambda} $$
:::

::: { .callout-warning title="Expectation ($\lambda$)"}
- Only one parameter is provided, $\lambda$: the rate with which the measured event happens

- Suppose a county has population 100,000, and average rate of cancer is 45.2mn people each year
  

$$z \sim \textrm{poisson}(45,200,000/100,000) = \textrm{poisson}(4.52) $$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data

- E.g., does $\lambda$ differ between two conditions?
:::


::: { .notes }
- Count or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the _Poisson distribution_
- Examples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.

- The distribution takes only one parameter: the **expectation**, lambda

- If your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions
:::

## Binomial and Poisson distributions { .smaller }

```{r binomial_poisson}
#| fig-align: center

ggarrange(binomial_dist, poisson_dist, ncol=2)
```

::: { .callout-important title="Some important features"}
- All measured values (and $n$) are positive whole numbers or zero; $\lambda$, $p$ may be positive _real numbers_ or zero
- The distributions may not be unimodal
- The mean is not always the peak value (_mode_)
- The distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)
:::

::: { .notes }
- Binomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings
  - The distributions are only defined at whole values - the lines are to guide your eye

- For binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases
  - More attempts also increases the variance of the distribution

- For poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value

- **In both cases, all values - counts/rates or successes - are positive whole numbers**
:::

## Distributions in Practice

::: { .callout-caution title="Distributions are starting points" }
- Distributions arise from and represent distinct _generation processes_ (relate this to your biological system)
  - Normal distributions are generated by sums, differences, and averages
  - Poisson distributions are generated by counts (per unit interval)
  - Binomial distributions are generated by success/failure outcomes
- **Design experiments with analyses that reflect these processes**
:::

::: { .callout-warning }
- All statistical distributions are **idealisations that ignore many features of real data**
- No real world data should be expected to exactly match _any_ statistical distribution
- Poisson models tend to need adjustment for _overdispersion_
:::

::: { .notes }
- The different distributions we've looked at arise from distinct generation processes that have parallels in the real world
  - Normal distributions arise when independent values are summed, or we take differences or averages of them
  - Poisson distributions arise from counts, or rates (i.e. counts per unit)
  - Binomial distributions arise from success/failure outcomes like tossing a coin
  
- We need to take care though, as these distributions are idealised outcomes from those processes
- In the real world there are many other influences that cause collected data to deviate from these idealisations
  - There is **no reason** to expect real world data to exactly match _any_ statistical distribution
  - Though there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)
:::

## Normal Distribution Redux { .smaller }

```{r normal}
#| fig-align: center

# Normal distribution with standard deviation masses indicated
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

par(mar=c(2,0,2,0), tck=-.01)
curve(dnorm(x), -4, 4, ylim=c(0, 0.4), xlab="", ylab="", bty="n", yaxs="i", main="normal distribution, mean=0 sd=1", xaxt="n", yaxt="n")
axis(1, c(-4, -3, -2, -1,  0,  1, 2, 3, 4), c("", "-3", "-2", "-1",  "0",  "1", "2", "3", ""), mgp=c(1.5, .5, 0), cex.axis=1.2)
colors <- c("gray70", "gray50", "gray30")
for (i in 3:1){
  grid <- seq(-i, i, .01)
  polygon(c(grid, i, -i), c(dnorm(grid), 0, 0), col=colors[i])
}
text(0, .35*dnorm(0), "68%", cex=1.3)
text(-1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
text(1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
```

::: { .callout-tip title="Probability mass" }
- approximately 50% of the distribution lies in the range $\mu \pm 0.68\sigma$
- approximately 68% of the distribution lies in the range $\mu \pm \sigma$
- approximately 95% of the distribution lies in the range $\mu \pm 2\sigma$
- approximately 99.7% of the distribution lies in the range $\mu \pm 3\sigma$
:::

::: { .notes }
- An intuition worth developing is how much of a (normal) distribution lies within some range of the mean
  - This is going to be useful whenthinking about _p_-values and hypothesis tests

- Here we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.
  - It's a normal distribution, so symmetrical about the mean
  - The area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution
  - The area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution
  
- It will be useful to have this intuition for the next section
:::

# Estimates, standard errors, and confidence intervals

::: { .notes }
We're going to review some statistical terms, to get ready for discussing hypothesis tests
:::

## Parameters

_Parameters_ are unknown numbers that determine a statistical model

::: { .callout-tip title="A linear regression" }
$$ y_i = a + b x_i $$

- Parameters are:
  - $a$ (the intercept)
  - $b$ (the gradient)
:::

::: { .callout-warning title="A normal distribution representing your data" }
$$ z \sim \textrm{normal}(\mu_z, \sigma) $$

- Parameters are: $\mu_z$ and $\sigma$
:::

::: { .notes }
- Parameters are numbers that you _don't know_ in a statistical model
  - You estimate parameter values from the data you collect
  
- So, if you have a linear regression and are estimating the intercept and gradient in the equation $y_i = a + b x_i$
  - The intercept $a$, and gradient $b$, are the parameters you are estimating
  
- You may be representing the data you collect by a normal distribution
  - This is described by the parameters $\mu_z$ and $\sigma$, which you are estimating from your data
:::

## Estimands { .smaller }

An _estimand_ (or _quantity of interest_) is a value that we are interested in estimating

::: { .callout-tip title="A linear regression" }
$$ y_i = a + b x_i$$

- We want to estimate values for:
  - $a$ (the intercept)
  - $b$ (the gradient)
  - **predicted outcomes at important values of $x_i$**

These are all **estimands**, and **estimates** are represented using the "hat" symbol: $\hat{a}$, $\hat{b}$, etc.
:::

::: { .callout-warning title="A normal distribution representing your data" }
$$ z \sim \textrm{normal}(\mu_z, \sigma) $$

- Estimands are: $\mu_z$ and $\sigma$
  - Maybe you want to determine the 95% confidence interval - **this is also an estimand**
:::

## Standard Errors and Confidence Intervals { .smaller }

- The _standard error_ is the _estimated standard deviation of an estimate_
  - It is a measure of our uncertainty about the quantity of interest


::: { .callout-note }
- Standard error gets smaller as sample size gets larger
  - You know more about the most likely value, the more data/information you collect
  - Standard error tends to zero as sample size gets large enough
:::

- The _confidence interval_ (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data

::: { .callout-important }
- In repeated applications, the 50% confidence interval will include the true value 50% of the time
  - A 95% confidence interval will include the true value 95% of the time
:::

::: { .callout-tip }
- The usual 95% confidence interval rule of thumb for large samples (_assuming a normal distribution_) is to take the estimate $\pm$ two standard errors
:::

::: { .notes }
- Now I've mentioned confidence intervals we'll need to talk about them
:::

# Statistical significance and hypothesis testing

::: { .notes }
- A major concern when performing data analysis in general, which is heightened when animal suffering is a possibility, is that we might _mistakenly) come to strong conclusions that do not replicate or that do not reflect real patterns in the underlying population.
- Statistical theories of hypothesis testing and error analysis have been developed to quantify these possibilities, **to help with decision making**
:::

## Statistical significance 1

- Some scientists choose to consider a result to be "stable" or "real" if it is "_statistically significant_"
- They may also consider "non-signifcant" results to be noisy or less reliable

::: { .callout-warning }
I, and many other statisticians, do not recommend this approach.

However, the concept is widespread and we need to discuss it
:::

::: { .notes }
- Even if you're not very familiar with what it means precisely, I'm sure you've come across "statistical" significance in at least one scientific context.

- "Statistical significance" is a **decision rule** used by some scientists to consider whether a result is "stable" or "real"
  - They may also use "statistical significance" to exclude some results as noisy or unreliable
  
- I and many other statisticians do not recommend this approach, and we'll see why
:::

## Statistical significance 2

::: { .callout-caution title="A common definition" }
- Statistical significance is conventionally defined as a threshold (commonly, a $p$-value less than 0.05) relative to some _null hypothesis_ or _prespecified value_ that indicates no effect is present.

- E.g., an estimate may be considered "statistically significant at $P < 0.05$" if it:
  - lies at least two standard errors from the mean
  - is a difference that lies at least two standard errors from zero
  
- More generally, an estimate is "**not** statistically significant" if, e.g.
  - the observed value can reasonably be explained by chance variation
  - it is a difference that lies less than two standard errors from zero
:::

::: { .callout-important title="Most tests rely on probability distributions" }
- We need to relate the measured values in the real world to an appropriate distribution that _approximates_ them
:::

::: { .notes }
- The way you're likely to see statistical significance presented is that some threshold - usually a p-value of less than 0.05 - is applied in a statistical test, relative to some _null hypothesis_ or _prespecified value_ that indicates the absence of an effect
- So you are likely to see an estimate be considered "statistically significant at P<0.05" if it lies at least two SEs from the mean, or is a difference that lies at least two SEs from zero
- Conversely, you will see estimates be considered "not statistically significant at P<0.05" if they lie less than two SEs from the mean, or the observed value can reasonably be explained by chance variation alone

- As you might have guessed from the talk of standard errors and distances from means, we map our measured values in the real world to statistical probability distributions to calculate these values.
:::

## A simple example: The experiment

::: { .callout-tip title="The experiment" }
- Two drugs, $C$ and $T$ lower cholesterol[^3], and we want to compare their effectiveness
- We randomise assignment of $C$ and $T$ to members of a single cohort of comparable individuals, whose pre-treatment cholesterol level is assumed to be drawn from the same distribution (i.e. be approximately the same)
- We measure the post-treatment cholesterol levels $y_T$ and $y_C$ for each individual in the two groups.
- We calculate the average measured $\bar{y}_T$ and $\bar{y}_C$ for the _treatment_ and _control_ groups as **estimates** for the true post-treatment levels $\theta_T$ and $\theta_C$.
  - We also calculate standard deviation for the two groups, $\sigma_T$ and $\sigma_C$
:::

::: { .notes }
- Let's consider a simple experiment
- We want to compare the efficacy of two drugs, C and T, in their ability to lower cholesterol
  - We assume a uniform pool of individuals, and randomly assign C and T to two equally-sized groups drawn from that cohort
  - **This implies that the starting cholesterol level of all individuals is drawn from the same distribution**
  
- We administer the drugs in the same way, for the same period of time, and measure the post-treatment cholesterol level in each individual
  - We calculate the mean and standard deviation for the two groups
  - These allow us to estimate the underlying normal distribution of post-treatment cholesterol levels in each group
- We can use the estimated means, $\bar{y}_T$ and $\bar{y}_C$ for the _treatment_ and _control_ groups as **estimates** for the true post-treatment levels $\theta_T$ and $\theta_C$.
:::

## A simple example: The hypotheses

- We want to know if the treatments have different sizes of effect
  - If they do, there should be a difference between the (average) post-treatment cholesterol level in each group
  - The _true_ post-treatment levels are $\theta_T$ and $\theta_C$ 
  - We have estimated means, $\bar{y}_T$ and $\bar{y}_C$ for post-treatment levels

::: { .callout-warning title="The hypotheses" }
- We are interested in $\theta = \theta_T - \theta_C$, the expected post-test difference in cholesterol between the two groups $T$ and $C$.
- Our **null hypothesis** ($H_0$) is that $\theta = 0$, i.e. **there is no difference** ($\theta_C = \theta_T$)
- Our **alternative hypothesis** ($H_1$) is that **there is a difference**, so $\theta \neq 0$,  (i.e. $\theta_C \neq \theta_T$)
:::

::: { .notes }
- Our goal in this experiment is to determine whether the treatments have different effects on lowering cholesterol
  - If they do, then there should be a difference between the (average) post-treatment cholesterol levels of groups T and C
  
- There are _true_ post treatment levels for each group: $\theta_T$ and $\theta_C$
  - But we have only estimated them as $\bar{y}_T$ and $\bar{y}_C$
  - So we turn to a hypothesis test to determine how likely it is our estimates support there being a difference in the true levels between the two groups
  
- We set up a **null hypothesis** that there is no difference between the groups: $\theta = 0$
  - In this hypothesis, any deviation from zero we see is considered the result of chance variation in the data, or error we cannot eliminate from measurements
- We also define an **alternative hypothesis** that there _is_ a difference between the groups: $\theta \neq 0$
  - We're not insisting that $\theta$ is greater than or less than zero, just that it's not zero
  - And also that any deviation from zero is not accounted for by the null hypothesis
:::

## A simple example: The distribution 1

- To perform a statistical test, we may assume a distribution and parameters for the _null hypothesis_
  - We can then test the observed estimate against that distribution to see how likely it is that the _null hypothesis_ would have generated it

::: { .callout-note title="The distribution" }
- We use a probability distribution reflecting generation of the null hypothesis: $\theta_C = \theta_T$
  - This allows us to define a test statistic $T$ (i.e. _a threshold probability of "significance"_) **in advance**
- We test the estimated value from the experiment ($\bar{y}_T - \bar{y}_C$) to calculate a $p$-value for our estimate: $p = \textrm{Pr}(T(y^{\textrm{null}}) > T(\bar{y}_T - \bar{y}_C))$
:::

[^3]: $C$ for _control_, the current best-in class; $T$ for _treatment_, the new compound

::: { .notes }
- At this point we haven't defined what our null hypothesis distribution should be, but we need to do so
  - Once we have this, we can test our observed estimate of $\hat{\theta} = \bar{y}_T - \bar{y}_C$ against the distribution to see how likely it is to be explained by the null hypothesis
  
- We need to choose a probability distribution that reflects the process generating the null hypothesis
  - In this case, we could shoose a normal distribution, which is typically appropriate for this kind of measurement
- Then we can define a test statistic $T$ **in advance**
  - $T$ represents a threshold value in the null distribution - one where we would consider a larger difference to be meaningful
  - You may be familiar with setting P<0.05, where the probability of seeing some value in the null distribution is 0.05 or less, and we might choose to set a value of $T$ that corresponds to this
- We then calculate the test statistic corresponding to our actual estimate, and calculate the $p$-value
  - This is the probability of observing our estimate, or a more extreme value, under the assumptions of the null model distribution
:::

## A simple example: The null hypothesis

::: { .callout-important title="The null hypothesis" }
- Assume that the true difference $\theta$ is normally-distributed with $\mu_\theta=0$, $\sigma_\theta=1$
:::

```{r h0_dist}
#| fig-align: center

normal_distplot(mu=0, sd=1)
```

::: { .notes }
- Suppose that the true difference between means is zero units, with standard error one unit, and that it is normally distributed
- The probability distribution would look like this
:::

## A simple example: The estimated difference

::: { .callout-important title="Observed between post-treatment levels: $\bar{y}_T - \bar{y}_C = -1.4$" }
- Is this an unlikely outcome given the null hypothesis?
:::

```{r estimate}
#| fig-align: center

normal_distplot(mu=0, sd=1) +
  add_x_marker(-1.7, 0.3, "estimate = -1.7", "darkorange1", "darkorange3")

```

::: { .notes }
- Suppose that we measured the post treatment levels in each group, and found the difference between the means to be -1.4 units
  - That is, the $T$ drug group had 1.4 units lower cholesterol than the $C$ group
- Is that unlikely, given the null hypothesis?

- Well, we don't know if it's unlikely because **we haven't decided what "unlikely" means**
  - To do that we need to set a significance threshold
:::

## A simple example: A significance threshold

::: { .callout-important title="We choose a significance threshold _in advance_" }
- Suppose we set a threshold $T$ corresponding to the 90% confidence interval (i.e. $P<0.1$)
  - If the estimate is not in the central 90% of the distribution, we'll say it's "significant"
:::

```{r h0_90ci}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.05, zend=0.95) +
  ci_normal(mu=0, sd=1, ci=0.90)
```

::: { .notes }
- We must always decide what our significance threshold is **in advance**
- Suppose that we decide the central 90% of the distribution/null hypothesis is "not significant"
  - This is the 90% confidence interval around the mean
  - We can shade this in to see it better
:::

## A simple example: Compare the estimate

::: { .callout-important title="Compare the estimate to the threshold" }
- The estimate lies outwith the threshold, so we call the difference "significant"
:::


```{r h0_90ci_est}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.05, zend=0.95) +
  ci_normal(mu=0, sd=1, ci=0.90) +
  add_x_marker(-1.7, 0.3, "estimate = -1.7", "darkorange1", "darkorange3")

```

::: { .notes }
- We can overlay the observed difference on the distribution here, and see that the estimate lies outside the 90% confidence interval
  - With our stated assumption of a $P < 0.1$ significance threshold, we would call this difference "significant"
:::

## A simple example: Another threshold

::: { .callout-important title="We choose a significance threshold _in advance_" }
- Suppose we set the threshold $T$ corresponding to the 95% confidence interval (i.e. $P<0.05$) instead?
:::

```{r h0_95ci}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +
  ci_normal(mu=0, sd=1, ci=0.90) +
  ci_normal(mu=0, sd=1, ci=0.95)
  
```

::: { .notes }
- Suppose that we had decided in advance that the central 95% of the distribution/null hypothesis was "not significant" instead
  - This is the 95% confidence interval around the mean
:::

## A simple example: Another outcome

::: { .callout-important title="Compare the estimate to the threshold" }
- The estimate lies _within_ the threshold, so the difference is "not significant"
:::

```{r h0_95ci_est}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +
  ci_normal(mu=0, sd=1, ci=0.95) +
  add_x_marker(-1.7, 0.3, "estimate = -1.7", "purple", "purple")

```

::: { .notes }
- Suppose that we had decided in advance that the central 95% of the distribution/null hypothesis was "not significant" instead
  - This is the 95% confidence interval around the mean
:::

## A simple example: What changed?

::: { .callout-tip title="What did not change" }
- The null hypothesis was the same
- The observed estimate of difference was the same
:::

::: { .callout-warning title="What changed" }
- Our choice of significance threshold changed
:::

::: { .callout-important title="Significance threshold choice" }
- Once the estimate is known, it is _always_ possible to find a threshold that makes it "significant" or "not significant"
- It is dishonest to select a threshold deliberately to make your result "significant" or "not significant"
- **Always choose _and record_ (preregister) your threshold for significance ahead of the experiment**
:::

## Tailed tests: two-tailed

::: { .callout-tip title="Use two tails if direction of change doesn't matter"}
- With a two-tailed hypothesis test, we do not care which direction of change is significant
:::

```{r two_tails}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +
  ci_normal(mu=0, sd=1, ci=0.95) +
  add_x_marker(-2.1, 0.35, "estimate = -2.1, significant", "darkorange1", "darkorange3") +
  add_x_marker(-1.7, 0.3, "estimate = -1.7, not significant", "purple", "purple") +
  add_x_marker(1.2, 0.3, "estimate = 1.2, not significant", "purple", "purple") +
  add_x_marker(3, 0.35, "estimate = 3, significant", "darkorange1", "darkorange3")

```

::: { .notes }
- The example we've been looking at is a two-tailed test
- This means that we're concerned with whether the estimate lies within the _central mass_ of the null distribution
  - The central 95% of the distribution has 2.5% of the distribution to the left, and 2.5% of the distribution to the right
  - If the estimate lies in either of those extreme regions, it's in the "extreme 5%" from the mean, and we call it "significant"
  - Any estimate lying within the central 95% of the distribution is "not significant"
:::

## Tailed tests: one-tailed (left)

::: { .callout-tip title="Use one-tailed tests when direction matters"}
- If we're testing specifically for a significant negative difference/reduction, use a left-tailed test
- e.g. if we wanted to know if $T$ reduced post-test levels with respect to $C$ at a threshold of $P < 0.05$
:::

```{r left_tail}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0.05, zend=1) +
  add_x_marker(-2.1, 0.35, "estimate = -2.1, significant", "purple", "purple") +
  add_x_marker(-1.7, 0.3, "estimate = -1.7, significant", "purple", "purple") +
  add_x_marker(1.2, 0.3, "estimate = 1.2, not significant", "darkorange1", "darkorange3") +
  add_x_marker(3, 0.35, "estimate = 3, not significant", "darkorange1", "darkorange3")

```

::: { .notes }
- When we know we want to test for change in a specific direction, we should use a one-tailed test
- When checking specifically for a negative change where $P < 0.05$, say that $T$ reduces cholesterol level by more than $C$, we would use a left-tailed test

- This means that we're concerned with whether the estimate lies _outside_ the right-most 95% of the distribution mass
  - This means that results in the left-most 5% of the mass are "significant"
  - Any estimate lying in the right-most 95% of the distribution is "not significant"
:::

## Tailed tests: one-tailed (right)

::: { .callout-tip title="Use one-tailed tests when direction matters"}
- If we're testing specifically for a positive difference/increase, use a right-tailed test
- e.g. if we wanted to know if $T$ _increased_ post-test levels with respect to $C$ at a threshold of $P < 0.05$
:::

```{r right_tail}
#| fig-align: center

normal_distplot(mu=0, sd=1) + 
  shade_normal(mu=0, sd=1, zstart=0, zend=0.95) +
  add_x_marker(-2.1, 0.35, "estimate = -2.1, not significant", "darkorange1", "darkorange3") +
  add_x_marker(-1.7, 0.3, "estimate = -1.7, not significant", "darkorange1", "darkorange3") +
  add_x_marker(1.2, 0.3, "estimate = 1.2, not significant", "darkorange1", "darkorange3") +
  add_x_marker(3, 0.35, "estimate = 3, significant", "purple", "purple")

```

::: { .notes }
- Similarly, if checking specifically for a postitive change where $P < 0.05$, say that $C$ reduces cholesterol level by more than $T$, we would use a left-tailed test

- This means that we're concerned with whether the estimate lies _outside_ the left-most 95% of the distribution mass
  - This means that results in the right-most 5% of the mass are "significant"
  - Any estimate lying in the left-most 95% of the distribution is "not significant"
  
- It's possible to dishonestly switch a result between "significant" and "not significant" by choosing to use a one-tailed or two-tailed test
:::

## Problems with statistical significance 1

::: { .callout-warning }
It is a common error to summarise comparisons by statistical significance into "significant" and "non-significant" results
:::

::: { .callout-important title="Statistical significance is not the same as practical importance" }
- Suppose a treatment increased earnings by £10 per year with a standard error of £2 (average salary £25,000).
  - This would be statistically, but not practically, significant
  
- Suppose a different treatment increased earnings by £10,000 per year with a standard error of £10,000
  - This would not be statistically significant, but could be important in practice
:::

## Problems with statistical significance 2

::: { .callout-warning }
It is a common error to summarise comparisons by statistical significance into "significant" and "non-significant" results
:::

::: { .callout-important title="Non-significance is not the same as zero" }
- Suppose an arterial stent treatment group outperforms the control
  - mean difference in treadmill time: 16.6s (standard error 9.8)
  - the 95% confidence interval for the effect includes zero, $p ≈ 0.20$
- It's not clear whether the net treatment effect is positive or negative
  - but we can't say that stents have no effect
:::

## Problems with statistical significance 3

::: { .callout-important title="The difference between 'significant' and 'not significant' is not statistically significant" }
1. At a $P<0.05$ threshold, only a small change is required to move from $P < 0.051$ to $P < 0.049$
2. Large changes in significance can correspond to non-significant differences in the underlying variables
:::

```{r nonsigdiff1}
#| fig-align: center

ggplot() +
  shaded_normal(10, 10, 0.1, 1, fill="orange", color="darkorange3", label="mean=10, sd=10", textoffset=0.001) +
  shaded_normal(25, 10, 0.01, 1, fill="purple", color="purple", label="mean=25, sd=10", textoffset=0.001) +
  annotate("segment", colour="black", lty="dashed", x=0, xend=0, y=0, yend=0.05) +
  xlim(-20, 60) +
  ylim(-0.001, 0.05) +
  xlab("effect estimate") + 
  ylab("density") +
  theme_minimal()
```

::: { .notes }
- It should hopefully be obvious that any significance threshold is arbitrary, and the difference between a value being on one side of the threshold or the other may be arbitrarily small
- Less obviously, consider two independent studies - orange and purple here
  - The purple study estimates an effect size of 25 with standard error of 10 units and is "statistically significant" in that it does not include zero at the 1% significance level 
  - The orange study estimates an effect size of 10 with standard error of 10 units and is "statistically not significant" as it includes zero even at the 10% significance level
:::

## Problems with statistical significance 4

::: { .callout-important title="The difference between 'significant' and 'not significant' is not statistically significant" }
1. At a $P<0.05$ threshold, only a small change is required to move from $P < 0.051$ to $P < 0.049$
2. Large changes in significance can correspond to non-significant differences in the underlying variables
:::

```{r nonsigdiff2}
#| fig-align: center

ggplot() +
  shaded_normal(10, 10, 0.025, 0.975, fill="orange", color="darkorange3", label="mean=10, sd=10", textoffset=0.001) +
  shaded_normal(25, 10, 0.025, 0.975, fill="purple", color="purple", label="mean=25, sd=10", textoffset=0.001) +
  annotate("segment", colour="black", lty="dashed", x=0, xend=0, y=0, yend=0.05) +
  xlim(-20, 60) +
  ylim(-0.001, 0.05) +
  xlab("effect estimate") + 
  ylab("density") +
  theme_minimal()
```

::: { .notes }
- However, the effect sizes estimated by the two studies are not different from each other at a "statistical significance" of 95%
  - In both cases, the central mass of each distribution includes the mean of the other
:::

# Experimental design and sample size decisions

::: { .notes }
- The goal of experimental design is not to attain some level of statistical significance with high probability
- The purpose of experimental design is to ensure we have a sense, **before and after data have been collected**, of what can realistically be learned from the experiment
:::

## Sample size 

# References

## References