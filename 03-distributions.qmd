```{r setup_03}
library(dplyr)
library(stringr)
library(tidyr)
library(ggpubr)
library(ggplot2)
library(latex2exp)

# Plot normal curves for of height counts for men and women separately and together
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

heights <- data.frame(height=54:75,
                      female=c(80,107,296,695,1612,2680,4645,8201,9948,11733,
                               10270,9942,6181,3990,2131,1154,245,257,0,
                               0,0,0)*10339/74167,
                      male=c(0,0,0,0,0,0,0,542,668,1221,2175,4213,5535,7980,
                             9566,9578,8867,6716,5019,2745,
                             1464,1263)*9983/67552) %>%
  pivot_longer(c(female, male), names_to="sex", values_to="count")

heights_f <- ggplot(heights %>% filter(sex=="female"), aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 1e4 * dnorm(x, 63.7, 2.7),
                color="orange", lwd=2) +
  xlim(53, 83) + ylim(0, 1700) +
  labs(title="Heights of female adults (US)") +
  theme_minimal()

heights_m <- ggplot(heights %>% filter(sex=="male"), aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 1e4 * dnorm(x, 69.1, 2.9),
                color="orange", lwd=2) +  
  xlim(53, 83) + ylim(0, 1700) +
  labs(title="Heights of male adults (US)") +
  theme_minimal()

heights_adults <- ggplot(heights, aes(x=height, y=count)) +
  geom_col() + 
  stat_function(fun=function(x) 2.1e4 * (0.52 * dnorm(x, 63.7, 2.7) +
                                       0.48 * dnorm(x, 69.1, 2.9)),
                color="orange", lwd=2) +
  xlim(53, 83) + ylim(0, 2100) +
  labs(title="Heights of all adults (US)") +
  theme_minimal()

# Example binomial and poisson distributions
# A data frame of values 1:20, with some binomial and poisson distributed values
bin_pois_data <- data.frame(count=0:20) %>%
  mutate(pois1=dpois(count, 1)) %>%
  mutate(pois4=dpois(count, 4)) %>%
  mutate(pois10=dpois(count, 10)) %>%
  mutate(binom0=dbinom(count, 20, 0.05)) %>%
  mutate(binom1=dbinom(count, 20, 0.3)) %>%
  mutate(binom2=dbinom(count, 20, 0.5)) %>%
  mutate(binom3=dbinom(count, 40, 0.5)) %>%
  pivot_longer(cols=-count, names_to="params", values_to="freq")

# Informative labels for legends
poisson_labels <- c("lambda=1", "lambda=4", "lambda=10")
binomial_labels <- c("n=20, p=0.01", "n=20, p=0.3", "n=20, p=0.7", "n=40, p=0.3")

poisson_dist <- ggplot(bin_pois_data %>% filter(str_detect(params, "^pois")), aes(x=count, color=params)) +
  geom_line(aes(y=freq), alpha=0.5) +
  geom_point(aes(y=freq)) +
  xlim(-3,20) +
  ylim(0, 0.4) +
  xlab("counts/rate") +
  labs(title="Poisson distributions") +
  scale_color_discrete(labels=poisson_labels) +
  coord_fixed(ratio=50) +
  theme_minimal()

binomial_dist <- ggplot(bin_pois_data %>% filter(str_detect(params, "^binom")), aes(x=count, color=params)) +
  geom_line(aes(y=freq), alpha=0.5) +
  geom_point(aes(y=freq)) +
  xlim(-3,20) +
  ylim(0, 0.4) +
  xlab("successes") +
  labs(title="Binomial distributions") +
  scale_color_discrete(labels=binomial_labels) +
  coord_fixed(ratio=50) +
  theme_minimal()
```

# Some Statistical Concepts

::: { .notes }
Before we get into the detail and fun of calculating appropriate group sizes for your experiments, we're going to refresh some relevant statistical concepts.

I know many biologists chose biology to get away from maths and stats, but the sad reality for those people is that being a scientist demands competence in this area. You will not be able to avoid this if you want to make a career out of research.
:::

## Random variables

Your experimental measurements are _random variables_

::: { .callout-important }
**This does not mean that your measurements are entirely random numbers**
:::

::: { .callout-caution }
Random variables are values whose range is _subject to some element of chance_, e.g. variation between individuals

- Tail length (e.g. timing of developmental signals, distribution of nutrients)
- Blood concentrations (e.g. circulatory heterogeneity, transient measurement differences)
- Survival time (e.g. determining point of death)
:::

::: { .notes }
- When you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a _random variable_
  - **This doesn't mean that the number is chosen randomly**
- What it means is that the number is subject to the influence of some kind of random variation
  - So tail length might be partially determined by variations in influences on overall growth of an individual (like nutrition), genetic propensity, and some random redistribution of nutrients within the individual or random timing of developmental signals
  - Blood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration
  - Survival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time
- There are numerous ways that random effects ("noise") can impinge on the "signal" we're trying to measure
:::

## Probability distributions { .smaller }

The probability distribution of a random variable $z$ (e.g. what you measure in an experiment) takes on some range of values[^infinite]

::: { .callout-tip title="The mean of the distribution of $z$" }
- The _mean_ (aka _expected value_ or _expectation_) is the average of all the values in $z$
  - Equivalently: the _mean_ is the value that is obtained **on average** from a random sample from the distribution
- Written as $\mu_{z}$ or $E(z)$
:::
  
::: { .callout-tip title="The variance of a distribution of $z$" }
- The _variance_ of the distribution of $z$ represents the _expected_ mean squared difference from the mean $\mu_z$ (or $E(z)$) of a random sample from the distribution.
  - $\textrm{variance} = E((z - \mu_z)^2)$
:::

[^infinite]: Here we are considering a populations of an infinite number of measurements of $z$

::: { .notes }
- A probability distribution describes the range of values that a random variable - let's call it $z$ - takes.
  - So if you're measuring tail length, and that's "constant" for a population, but under the influence of the random variation in how much nutrition an individual can get, the distribution is representing that random effect around the "constant" value.
  - In more traditional statistical terms, you can think of $z$ being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it
- Probability distributions can describe many measures, including
  - heights of men, incomes of women, political party preference, and so on
- The _mean_ of a probability distribution - its _expected value_ or _expectation_ - is the average of all numbers in the distribution
  - There may be an infinite amount of numbers in the dataset (in principle), and there is an equivalent definition: the _mean_ is the value obtained _on average_ in a random sample taken from the distribution
- The _variance_ of a probability distribution expresses how much the individual values might differ from that mean.
  - Variance is defined as the _mean_ of the **square of the difference between each individual value in the distribution and the mean of the distribution**
  - So, for each ball we draw from the bag, we treat the number as $z$ and calculate the _sum of squares of differences from the mean_ - this is the variance.
:::

## Understanding variance

::: { .callout-caution title="A distribution where all values of $z$ are the same" }
- Every single value in the distribution ($z$) is also the mean value ($\mu_z$), therefore

$$z = \mu_z \implies z - \mu_z = 0 \implies (z - \mu_z)^2 = 0$$
$$\textrm{variance} = E((z - \mu_z)^2) = E(0^2) = 0$$

:::

::: { .callout-tip title="All other distributions" }
In **every** other distribution, there are some values of $z$ that differ so, for at least some values of $z$

$$z \neq \mu_z \implies  z - \mu_z \neq 0 \implies (z - \mu_z)^2 \gt 0 $$
$$\implies \textrm{variance} = E((z - \mu_z)^2) \gt 0 $$

:::

::: { .notes }
- To get some intuition about the idea of variance, imagine that you have a set of values $z$ that are all the same, so there's no randomness
  - So you have a bag of balls, and all the balls in the bag have the number three on them, say
- If the value are all the same, then they all have the same value as the _mean_ of the distribution, and $z - \mu_z = 0$
- This means that, for all $z$, $(z - \mu_z)^2$ is also zero, and the variance is zero.
- But if any value of $z$ is different then, for at least that value, $z - \mu_z \neq 0$, so the square of that value is greater than zero. It follows that the variance must then be greater than zero: $E((z - \mu_z)^2) \gt 0$

- **In any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value**
  - **All measurements you make will have variability in them! This variability must be accounted for.**
:::

## Standard deviation

::: { .callout-warning title="_Standard deviation_ is the square root of the variance" }

$$\textrm{standard deviation} = \sigma_z = \sqrt{\textrm{variance}} = \sqrt{E((z - \mu_z)^2)} $$
:::

::: { .callout-tip title="Advantages" }
- The _standard deviation_ (unlike variance) takes values on the same scale as the original distribution
  - Standard deviation is a more "natural-seeming" interpretation of variation
:::

::: { .callout-note }
We can calculate _mean_, _variance_, and _standard deviation_ for **any** probability distribution.
:::

::: { .notes }
- You are probably more familiar with the _standard deviation_ of a distribution.
- The standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance

- While we can calculate the mean, variance and standard deviation for _any_ distribution, this does **not** mean that they are equally informative for all distributions
- Let's look at some examples
:::

## Normal Distribution 1

$$ z \sim \textrm{normal}(\mu_z, \sigma_z) $$

::: { .callout-note }
We only need to know the mean and standard deviation to define a unique normal distribution
:::

::: { .callout-tip }
Measurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution
:::

::: { .callout-important }
**There is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!**
:::

::: { .notes }
- You may see the normal distribution written like this, as "$z$ is distributed as the Normal distribution with mean $\mu_z$ and standard deviation $\sigma_z$."
  - We only need to know the mean and standard deviation to define a unique normal distribution
- Values we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors
  - This is a consequence of the Central Limit Theorem
  - Many real-world variables do follow normal distributions
- **But there is no reason to expect that any random variable representing any direct measurement you make will have this property, or follow a normal distribution**
  - This is especially the case if there is a large factor affecting variation of the variable (e.g. sex)
:::

## Normal Distribution 2 { .smaller }

```{r norm_density}
#| fig-align: center

# Plot normal curves for of height counts for men and women
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html
library(ggpubr)

ggarrange(heights_f, heights_m, ncol=2)
```

::: { .callout-tip }
- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean
:::

::: { .notes }
- As an example, we can look at some real data, the heights of men and women in the US.
  - Here we have counts of individuals whose heights are measured to the nearest inch - they're shown as counts in the histograms
- These are two different _random variables_: one for men, one for women
- For both distributions, we can calculate a mean, and a standard deviation (or variance)
  - Therefore we can calculate a normal distribution representing both men's and women's heights (orange curve)
  - We calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)
- The distributions of heights for each sex, separately, follow an approximate normal distribution

- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean
:::

## (Non-)Normal Distribution 3 { .smaller }

```{r all_heights}
#| fig-align: center

# Plot normal curves for of height counts for all us_adults
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

heights_adults
```

::: { .callout-tip }
- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean
:::

::: { .notes }
- By contrast, the distribution of heights of _all_ adults in the US is not close to a normal curve
- This is because there is an extraneous factor, **sex**, that represents much of the total variation in values

- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean

- Data you receive may not be normally distributed
  - (possibly even most of the data you receive!)
:::

## Binomial Distribution 1

:::: { .columns }

::: { .column width="60%" }
Suppose you're taking shots in basketball

- how many shots?
- how likely are you to score?
- what is the distribution of the number of successful shots?

::: { .callout-tip }
This kind of process generates a random variable approximating a probability distribution called a **binomial distribution**.

It is different from a normal distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- I mentioned earlier that a normal distribution arises when a process produces the a sum of many independent random variables.
  - **In general statistical distributions are idealised repreentations of the outcomes of distinct kinds of real-world processes**

- Suppose that instead of measuring height, weight, concentration or something like that, you're measuring _event outcomes_
  - These are not the result of summing independent random variables, and do not follow a normal distribution

- If you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding
- The number of successful shots is going to depend on the number of shots you take, and how likely you are to score
  - Michael Jordan is much more likely to score any individual attempt than I am
- The number of successful shots is a random variable with a probability distribution

- This kind of process generates a probability distribution that approximates the **binomial distribution**
  - It's the same one you get for coin tosses (or any yes/no process)
- It is different from a normal distribution
  - If your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one suitable for a binomial distribution
:::

## Binomial Distribution 2 { .smaller }

:::: { .columns }

::: { .column width="60%" }
$$ z \sim \textrm{binomial}(n, p) $$

::: { .callout-tip }
- number of shots, $n = 20$, probability of scoring, $p = 0.3$

$$z \sim \textrm{binomial}(20, 0.3) $$
:::

::: { .callout-caution title="mean and sd"}
$$ \textrm{mean} = n \times p $$
$$ \textrm{sd} = \sqrt{n \times p \times (1-p)}$$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data. E.g., does $p$ differ between two conditions?
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- We write the definition of a binomial distribution as $z \sim \textrm{binomial}(n, p)$
  - Unique distributions are defined by combinations of the number of events, and the probability with which each event occurs

- Imagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot
- The distribution of shots you scored would follow a binomial distribution with $n=20$ and $p=0.3$
- You'd expect to score, on average, $20 \times 0.3 = 6$ times
- I'm not going to derive the variance and standard deviation expression
  - The standard deviation we'd expect would be $\sqrt{20 \times 0.3 \times (1 - 0.3)} = \sqrt{6 \times (0.7)} = \sqrt{4.2} = 2.05$
  - So you'd expect to score about 2 to 10 shots, most of the time
:::

## Poisson distribution 1

:::: { .columns }

::: { .column width="60%" }
> In prior experiments the frequency of calcium events in WKY was 3.8 $\pm$ 1.1 events/field/min compared to 18.9 $\pm$ 7.1 in SHR

::: { .callout-warning title="This is not normal (or binomial)" }
Something that happens a certain number of times in a fixed interval generates a _Poisson distribution_. 

This is different from a normal or binomial distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/calcium_events.png)
:::

::::

::: { .notes }
- Suppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR
  - You count the number of times the calcium spikes in a minute, in your field of view
- This data is _rate_ data
  - A count of events per unit (here, unit time and unit area) _interval_
  
- The idealised representation of data generated by this process is a **Poisson distribution**
  - This differs from a normal or binomial distribution
:::

## Poisson distribution 2 { .smaller }

$$z \sim \textrm{poisson}(\lambda)$$

::: { .callout-tip title="Poisson distribution" }
$$ \textrm{mean} = \lambda $$
$$ \textrm{sd} = \sqrt{\lambda} $$
:::

::: { .callout-warning title="Expectation ($\lambda$)"}
- Only one parameter is provided, $\lambda$: the rate with which the measured event happens

- Suppose a county has population 100,000, and average rate of cancer is 45.2mn people each year
  

$$z \sim \textrm{poisson}(45,200,000/100,000) = \textrm{poisson}(4.52) $$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data

- E.g., does $\lambda$ differ between two conditions?
:::


::: { .notes }
- Count or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the _Poisson distribution_
  - Unique Poisson distributions are described as $z \sim \textrm{poisson}(\lambda)$, defined only by the _expected value_ $\lambda$
- Examples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.

- The distribution takes only one parameter: the **expectation**, lambda

- If your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions
:::

## Binomial and Poisson distributions { .smaller }

```{r binomial_poisson}
#| fig-align: center

library(ggpubr)

ggarrange(binomial_dist, poisson_dist, ncol=2)
```

::: { .callout-important title="Some important features"}
- All measured values (and $n$) are positive whole numbers or zero; $\lambda$, $p$ may be positive _real numbers_ or zero
- The distributions may not be unimodal
- The mean is not always the peak value (_mode_)
- The distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)
:::

::: { .notes }
- Binomial and poisson distributions are related, and they look like this for the indicated parameter settings
  - The distributions are only defined at whole values - the lines are to guide your eye

- For binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases
  - More attempts also increases the variance of the distribution

- For poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value
  - The distribution also becomes more symmetric

- **In both cases, all values - counts/rates or successes - are positive whole numbers**
  - (though $\lambda$ and $p$ can be real values)
:::

## Distributions in Practice

::: { .callout-caution title="Distributions are starting points" }
- Distributions arise from and represent distinct _generation processes_ (relate this to your biological system)
  - Normal distributions are generated by sums, differences, and averages
  - Poisson distributions are generated by counts (per unit interval)
  - Binomial distributions are generated by success/failure outcomes
- **Design experiments with analyses that reflect these processes**
:::

::: { .callout-warning }
- All statistical distributions are **idealisations that ignore many features of real data**
- No real world data should be expected to exactly match _any_ statistical distribution
- Poisson models tend to need adjustment for _overdispersion_
:::

::: { .notes }
- The different distributions we've looked at arise from distinct generation processes that have parallels in the real world
  - Normal distributions arise when independent values are summed, or we take differences or averages of them
  - Poisson distributions arise from counts, or rates (i.e. counts per unit)
  - Binomial distributions arise from success/failure outcomes like tossing a coin
  
- We need to take care though, as these distributions are idealised outcomes from those processes
- In the real world there are many other influences that cause collected data to deviate from these idealisations
  - There is **no reason** to expect real world data to exactly match _any_ statistical distribution
  - Though there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)
  
- **In general we _approximate_ our real data with these idealised distributions in order to obtain more statistical power**
:::

## Normal Distribution Redux { .smaller }

```{r normal}
#| fig-align: center

# Normal distribution with standard deviation masses indicated
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

par(mar=c(2,0,2,0), tck=-.01)
curve(dnorm(x), -4, 4, ylim=c(0, 0.4), xlab="", ylab="", bty="n", yaxs="i", main="normal distribution, mean=0 sd=1", xaxt="n", yaxt="n")
axis(1, c(-4, -3, -2, -1,  0,  1, 2, 3, 4), c("", "-3", "-2", "-1",  "0",  "1", "2", "3", ""), mgp=c(1.5, .5, 0), cex.axis=1.2)
colors <- c("gray70", "gray50", "gray30")
for (i in 3:1){
  grid <- seq(-i, i, .01)
  polygon(c(grid, i, -i), c(dnorm(grid), 0, 0), col=colors[i])
}
text(0, .35*dnorm(0), "68%", cex=1.3)
text(-1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
text(1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
```

::: { .callout-tip title="Probability mass" }
- approximately 50% of the distribution lies in the range $\mu \pm 0.68\sigma$
- approximately 68% of the distribution lies in the range $\mu \pm \sigma$
- approximately 95% of the distribution lies in the range $\mu \pm 2\sigma$
- approximately 99.7% of the distribution lies in the range $\mu \pm 3\sigma$
:::

::: { .notes }
- An intuition worth developing is how much of a (normal) distribution lies within some range of the mean
  - This is going to be useful when thinking about _p_-values and hypothesis tests

- Here we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.
  - It's a normal distribution, so symmetrical about the mean
  - The area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution
  - The area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution
  
- It will be useful to have this intuition for the next section
:::
