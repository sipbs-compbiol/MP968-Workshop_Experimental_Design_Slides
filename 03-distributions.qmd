# Some Statistical Concepts

::: { .notes }
Before we get into the detail and fun of calculating appropriate group sizes for your experiments, we're going to refresh some relevant statistical concepts.
:::

## Random variables

Your experimental measurements are _random variables_

::: { .callout-important }
**This does not mean that your measurements are entirely random numbers**
:::

::: { .callout-caution }
Random variables are values whose range is _subject to some element of chance_, e.g. variation between individuals

- Tail length (e.g. timing of developmental signals, distribution of nutrients)
- Blood concentrations (e.g. circulatory heterogeneity, transient measurement differences)
- Survival time (e.g. determining point of death)
:::

::: { .notes }
- When you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a _random variable_
  - **This doesn't mean that the number is chosen randomly**
- What it means is that the number is subject to the influence of some kind of random variation
  - So tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals
  - Blood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration
  - Survival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time
:::

## Probability distributions { .smaller }

The probability distribution of a random variable $z$ (e.g. what you measure in an experiment) takes on some range of values[^infinite]

::: { .callout-tip title="The mean of the distribution of $z$" }
- The _mean_ (aka _expected value_ or _expectation_) is the average of all the values in $z$
  - Equivalently: the _mean_ is the value that is obtained **on average** from a random sample from the distribution
- Written as $\mu_{z}$ or $E(z)$
:::
  
::: { .callout-tip title="The variance of a distribution of $z$" }
- The _variance_ of the distribution of $z$ represents the _expected_ mean squared difference from the mean $\mu_z$ (or $E(z)$) of a random sample from the distribution.
  - $\textrm{variance} = E((z - \mu_z)^2)$
:::

[^infinite]: Here we are considering a populations of an infinite number of measurements of $z$

::: { .notes }
- A probability distribution describes the range of values that a random variable - let's call it $z$ - takes.
  - You can think of $z$ being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it
- Probability distributions can describe many measures, including
  - heights of men, incomes of women, political party preference, and so on
- The _mean_ of a probability distribution - its _expected value_ or _expectation_ is the average of all numbers in the distribution
  - This may be infinite, and there is an equivalent definition: the _mean_ is the value obtained _on average_ from a random sample taken from the distribution
- The _variance_ of a probability distribution expresses how much the individual values might differ from that mean.
  - Variance is defined as the _mean_ of the **square of the difference between each individual value in the distribution and the mean of the distribution**
:::

## Understanding variance

::: { .callout-caution title="A distribution where all values of $z$ are the same" }
- Every single value in the distribution ($z$) is also the mean value ($\mu_z$), therefore

$$z = \mu_z \implies z - \mu_z = 0 \implies (z - \mu_z)^2 = 0$$
$$\textrm{variance} = E((z - \mu_z)^2) = E(0^2) = 0$$

:::

::: { .callout-tip title="All other distributions" }
In **every** other distribution, there are some values of $z$ that differ so, for at least some values of $z$

$$z \neq \mu_z \implies  z - \mu_z \neq 0 \implies (z - \mu_z)^2 \gt 0 $$
$$\implies \textrm{variance} = E((z - \mu_z)^2) \gt 0 $$

:::

::: { .notes }
- To get some intuition about the idea of variance, imagine that you have a set of values $z$ that are all the same, so there's no randomness
- If the value are all the same, then they all have the same value as the _mean_ of the distribution, and $z - \mu_z = 0$
- This means that, for all $z$, $(z - \mu_z)^2$ is also zero, and the variance is zero.
- But if any value of $z$ is different then, for at least that value, $z - \mu_z \neq 0$, so the square of that value is greater than zero. It follows that the variance must then be greater than zero: $E((z - \mu_z)^2) \gt 0$

- In any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value
:::

## Standard deviation

::: { .callout-warning title="_Standard deviation_ is the square root of the variance" }

$$\textrm{standard deviation} = \sigma_z = \sqrt{\textrm{variance}} = \sqrt{E((z - \mu_z)^2)} $$
:::

::: { .callout-tip title="Advantages" }
- The _standard deviation_ (unlike variance) takes values on the same scale as the original distribution
  - Standard deviation is a more "natural-seeming" interpretation of variation
:::

::: { .callout-note }
We can calculate _mean_, _variance_, and _standard deviation_ for **any** probability distribution.
:::

::: { .notes }
- You are probably more familiar with the _standard deviation_ of a distribution.
- The standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance

- While we can calculate the mean, variance and standard deviation for _any_ distribution, this does **not** mean that they are equally informative for all distributions
- Let's look at some examples
:::

## Normal Distribution 1

$$ z \sim \textrm{normal}(\mu_z, \sigma_z) $$

::: { .callout-note }
We only need to know the mean and standard deviation to define a unique normal distribution
:::

::: { .callout-tip }
Measurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution
:::

::: { .callout-important }
**There is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!**
:::

::: { .notes }
- You may see the normal distribution written like this, as "z is distributed as the Normal distribution with mean $\mu_z$ and standard deviation $\sigma_z$."
  - We only need to know the mean and standard deviation to define a unique normal distribution
- Values we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors
  - This is a consequence of the Central Limit Theorem
- **But there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution**
  - This is especially the case if there is a large factor affecting variation of the variable
:::

## Normal Distribution 2 { .smaller }

```{r norm_density}
#| fig-align: center

# Plot normal curves for of height counts for men and women
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html
library(ggpubr)

ggarrange(heights_f, heights_m, ncol=2)
```

::: { .callout-tip }
- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean
:::

::: { .notes }
- As an example, we can look at some real data, the heights of men and women in the US.
  - Here we have counts of individuals whose heights are measured to the nearest inch
- These are two different _random variables_: one for men, one for women
- For both distributions, we can calculate a mean, and a standard deviation (or variance)
  - Therefore we can calculate a normal distribution representing both men's and women's heights (orange curve)
  - We calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)
- The distributions of heights for each sex, separately, follow an approximate normal distribution

- For a normal distribution, the mean value is the value at the peak of the curve
- The curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean
:::

## (Non-)Normal Distribution 3 { .smaller }

```{r all_heights}
#| fig-align: center

# Plot normal curves for of height counts for all us_adults
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

heights_adults
```

::: { .callout-tip }
- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean
:::

::: { .notes }
- By contrast, the distribution of heights of _all_ adults in the US is not close to a normal curve
- This is because there is an extraneous factor, **sex**, that represents much of the total variation in values
  - We will come back to this idea later
  
- Here, the mean may not be the same value as the peak of the curve (i.e. the _mode_)
- The curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean

- Most data you receive will not be normally distributed
:::

## Binomial Distribution 1

:::: { .columns }

::: { .column width="60%" }
Suppose you're taking shots in basketball

- how many shots?
- how likely are you to score?
- what is the distribution of the number of successful shots?

::: { .callout-tip }
This kind of process generates a random variable approximating a probability distribution called a **binomial distribution**.

It is different from a normal distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- Suppose that instead of measuring height, weight, concentration or something like that, you're measuring _event outcomes_
  - These do not follow a normal distribution

- If you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding
- The number of successful shots is going to depend on the number of shots you take, and how likely you are to score
  - Michael Jordan is much more likely to score any individual attempt than I am
- The number of successful shots is a random variable with a probability distribution

- This kind of process generates a probability distribution that approximates the **binomial distribution**
  - It's the same one you get for coin tosses (or any yes/no process)
- It is different from a normal distribution
  - If your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution
:::

## Binomial Distribution 2 { .smaller }

:::: { .columns }

::: { .column width="60%" }
$$ z \sim \textrm{binomial}(n, p) $$

::: { .callout-tip }
- number of shots, $n = 20$, probability of scoring, $p = 0.3$

$$z \sim \textrm{binomial}(20, 0.3) $$
:::

::: { .callout-caution title="mean and sd"}
$$ \textrm{mean} = n \times p $$
$$ \textrm{sd} = \sqrt{n \times p \times (1-p)}$$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data. E.g., does $p$ differ between two conditions?
:::

:::

::: { .column width="40%" }
![](assets/images/basketball.png)
:::

::::

::: { .notes }
- Imagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot
- The distribution of shots you scored would follow a binomial distribution with $n=20$ and $p=0.3$
- You'd expect to score, on average, $20 \times 0.3 = 6$ times
- The standard deviation we'd expect would be $\sqrt{20 \times 0.3 \times (1 - 0.3)} = \sqrt{6 \times (0.7)} = \sqrt{4.2} = 2.05$
  - So you'd expect to score about 4 to 8 shots, most of the time
:::

## Poisson distribution 1

:::: { .columns }

::: { .column width="60%" }
> In prior experiments the frequency of calcium events in WKY was 3.8 $\pm$ 1.1 events/field/min compared to 18.9 $\pm$ 7.1 in SHR

::: { .callout-warning title="This is not normal (or binomial)" }
Something that happens a certain number of times in a fixed interval generates a _Poisson distribution_. 

This is different from a normal or binomial distribution.
:::

:::

::: { .column width="40%" }
![](assets/images/calcium_events.png)
:::

::::

::: { .notes }
- Suppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR
  - You count the number of times the calcium spikes in a minute, in your field of view
- This data is _rate_ data
  - A count of events per unit (here, unit time and unit area) _interval_
  
- The idealised representation of data generated by this process is a **Poisson distribution**
  - This differs from a normal or binomial distribution
:::

## Poisson distribution 2 { .smaller }

$$z \sim \textrm{poisson}(\lambda)$$

::: { .callout-tip title="Poisson distribution" }
$$ \textrm{mean} = \lambda $$
$$ \textrm{sd} = \sqrt{\lambda} $$
:::

::: { .callout-warning title="Expectation ($\lambda$)"}
- Only one parameter is provided, $\lambda$: the rate with which the measured event happens

- Suppose a county has population 100,000, and average rate of cancer is 45.2mn people each year
  

$$z \sim \textrm{poisson}(45,200,000/100,000) = \textrm{poisson}(4.52) $$
:::

::: { .callout-important title="Design note"}
You need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data

- E.g., does $\lambda$ differ between two conditions?
:::


::: { .notes }
- Count or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the _Poisson distribution_
- Examples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.

- The distribution takes only one parameter: the **expectation**, lambda

- If your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions
:::

## Binomial and Poisson distributions { .smaller }

```{r binomial_poisson}
#| fig-align: center

library(ggpubr)

ggarrange(binomial_dist, poisson_dist, ncol=2)
```

::: { .callout-important title="Some important features"}
- All measured values (and $n$) are positive whole numbers or zero; $\lambda$, $p$ may be positive _real numbers_ or zero
- The distributions may not be unimodal
- The mean is not always the peak value (_mode_)
- The distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)
:::

::: { .notes }
- Binomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings
  - The distributions are only defined at whole values - the lines are to guide your eye

- For binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases
  - More attempts also increases the variance of the distribution

- For poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value

- **In both cases, all values - counts/rates or successes - are positive whole numbers**
:::

## Distributions in Practice

::: { .callout-caution title="Distributions are starting points" }
- Distributions arise from and represent distinct _generation processes_ (relate this to your biological system)
  - Normal distributions are generated by sums, differences, and averages
  - Poisson distributions are generated by counts (per unit interval)
  - Binomial distributions are generated by success/failure outcomes
- **Design experiments with analyses that reflect these processes**
:::

::: { .callout-warning }
- All statistical distributions are **idealisations that ignore many features of real data**
- No real world data should be expected to exactly match _any_ statistical distribution
- Poisson models tend to need adjustment for _overdispersion_
:::

::: { .notes }
- The different distributions we've looked at arise from distinct generation processes that have parallels in the real world
  - Normal distributions arise when independent values are summed, or we take differences or averages of them
  - Poisson distributions arise from counts, or rates (i.e. counts per unit)
  - Binomial distributions arise from success/failure outcomes like tossing a coin
  
- We need to take care though, as these distributions are idealised outcomes from those processes
- In the real world there are many other influences that cause collected data to deviate from these idealisations
  - There is **no reason** to expect real world data to exactly match _any_ statistical distribution
  - Though there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)
:::

## Normal Distribution Redux { .smaller }

```{r normal}
#| fig-align: center

# Normal distribution with standard deviation masses indicated
# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html

par(mar=c(2,0,2,0), tck=-.01)
curve(dnorm(x), -4, 4, ylim=c(0, 0.4), xlab="", ylab="", bty="n", yaxs="i", main="normal distribution, mean=0 sd=1", xaxt="n", yaxt="n")
axis(1, c(-4, -3, -2, -1,  0,  1, 2, 3, 4), c("", "-3", "-2", "-1",  "0",  "1", "2", "3", ""), mgp=c(1.5, .5, 0), cex.axis=1.2)
colors <- c("gray70", "gray50", "gray30")
for (i in 3:1){
  grid <- seq(-i, i, .01)
  polygon(c(grid, i, -i), c(dnorm(grid), 0, 0), col=colors[i])
}
text(0, .35*dnorm(0), "68%", cex=1.3)
text(-1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
text(1.5, .3*dnorm(1.5), "13.5%", cex=1.3)
```

::: { .callout-tip title="Probability mass" }
- approximately 50% of the distribution lies in the range $\mu \pm 0.68\sigma$
- approximately 68% of the distribution lies in the range $\mu \pm \sigma$
- approximately 95% of the distribution lies in the range $\mu \pm 2\sigma$
- approximately 99.7% of the distribution lies in the range $\mu \pm 3\sigma$
:::

::: { .notes }
- An intuition worth developing is how much of a (normal) distribution lies within some range of the mean
  - This is going to be useful whenthinking about _p_-values and hypothesis tests

- Here we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.
  - It's a normal distribution, so symmetrical about the mean
  - The area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution
  - The area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution
  
- It will be useful to have this intuition for the next section
:::
