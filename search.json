[
  {
    "objectID": "index.html#we-should-not-cause-unnecessary-suffering",
    "href": "index.html#we-should-not-cause-unnecessary-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "We should not cause unnecessary suffering",
    "text": "We should not cause unnecessary suffering\n\n\n\n\n\n\n\nWe should always minimise suffering\n\n\nThis may mean not performing an experiment at all. Not all new knowledge or understanding is worth causing suffering to obtain it.\nWhere there is sufficient justification to perform an experiment, we are ethically obliged to minimise the amount of distress or suffering that is caused, by designing the experiment to achieve this.\n\n\n\n\n\n\n\n\n\n\n\nWhy we need statistics\n\n\nIt may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.\nBut what is an acceptable (i.e. the least possible) amount of suffering necessary to obtain an informative result?\n\n\n\n\n\n\nNo-one like talking about animal experiments.\nIt’s a difficult, emotive topic that crosses people’s moral red lines.\nAnimal experiments may be the only practical way to gain essential scientific knowledge\nWhatever you believe “suffering” means for an animal, our ethical premise is that this suffering should not be in vain"
  },
  {
    "objectID": "index.html#challenge",
    "href": "index.html#challenge",
    "title": "MP968 Experimental Design Workshop",
    "section": "Challenge",
    "text": "Challenge\n\n\n\n\n\n\n\nQuiz question\n\n\nSuppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. Which of the following choices will cause the least amount of suffering?\n\n\n\n\n\nUse three subjects per group so a standard deviation can be calculated\nUse just enough subjects to establish that the outcome is likely to be correct\nUse just enough subjects to be certain that the outcome is correct\nUse as many subjects as you have available, to avoid wastage\n\n\nWe carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is more likely or less likely to be the null hypothesis than some alternative), rather than 100% certain.\nIf we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering.\nIf we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering.\nIf we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering."
  },
  {
    "objectID": "index.html#how-many-individuals",
    "href": "index.html#how-many-individuals",
    "title": "MP968 Experimental Design Workshop",
    "section": "How many individuals?",
    "text": "How many individuals?\n\n\n\n\n\n\n\nThe appropriate number of subjects\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\n\nWhat assumptions are reasonable?\nWhat is an appropriate level of certainty?\n\nBy convention1 the usual level of certainty for a hypothesis test is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”\n\n\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\nThis may sound like a very flexible statement. What assumptions are reasonable? What is an appropriate level of certainty?\nWe’ll consider these questions again later but, for now, just know that by convention the usual level of certainty is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”._\nNote though that the appropriate level of certainty may change depending on the nature of the question being asked.\n\nConventions are guidelines, not rigid standards, and you should always consider whether a convention is appropriate in your use case"
  },
  {
    "objectID": "index.html#design-experiments-to-minimise-suffering",
    "href": "index.html#design-experiments-to-minimise-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "Design experiments to minimise suffering",
    "text": "Design experiments to minimise suffering\n\n\n\n\n\n\n\nExperimental design and statistics are intertwined\n\n\nOnce a research hypothesis has been devised:\n\nExperimental design is the process of devising a practical way of answering the question\nStatistics informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results\n\n\n\n\n\n\n\n\n\n\n\n\nDesign your experiment for…\n\n\n\nyour population or subject group (e.g. sex, age, prior history, etc.)\nyour intervention (e.g. drug treatment)\nyour contrast or comparison between groups (e.g. lung capacity, drug concentration, etc.)\nyour outcome (i.e. is there a measurable or clinically relevant effect)\n\n\n\n\n\n\nOnce a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.\nStatistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.\nYou should design your experiment specifically for your combination of population/subject group, the intervention you’re applying, the contrast or comparison you’re making, and the outcome you’re expecting to see - specifically a measurable or clinically relevant effect."
  },
  {
    "objectID": "index.html#the-importance-of-experimental-design",
    "href": "index.html#the-importance-of-experimental-design",
    "title": "MP968 Experimental Design Workshop",
    "section": "The importance of experimental design",
    "text": "The importance of experimental design\n\n\n\n“For scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\nWe rely on the reporting of the experiment to know if it was appropriate\n\n\n\n\n\n\n\n\n\n\n\nThe National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.\nIn 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.\nIt did not make for pleasant reading."
  },
  {
    "objectID": "index.html#causes-for-concern-1",
    "href": "index.html#causes-for-concern-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 1",
    "text": "Causes for concern 1\n\n\n\n“Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. […] Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the literature for good examples of experimental design\n\n\n\n\n\n\n\n\n\nThe state of the published literature around animal experiments was not good in 2009.\n\n40% of studies did not state the hypothesis or objective of the experiment\nMost papers did not use randomisation or blinding, although this is an essential practice to avoid bias\nOnly 70% of publications described their statistical methods at all"
  },
  {
    "objectID": "index.html#causes-for-concern-2",
    "href": "index.html#causes-for-concern-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 2",
    "text": "Causes for concern 2\n\n\n\n\n\n\n\nNo publication explained their choice for the number of animals used\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the verbal authority of ‘published scientists’ or ‘experienced scientists’ for good experimental design\n\n\n\n\n\n\n\n\n\nOne of the most shocking pieces of information is that, of the 48 papers surveyed, not a single one explained why they used the number of animals that they did.\nWe cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.\nThese papers are published. They are written, and the experiments conducted, by “experienced scientists”\nBeing an “experienced” or “published” scientist is clearly not a benchmark for good experimental design"
  },
  {
    "objectID": "index.html#very-strong-cause-for-concern",
    "href": "index.html#very-strong-cause-for-concern",
    "title": "MP968 Experimental Design Workshop",
    "section": "Very strong cause for concern",
    "text": "Very strong cause for concern\n\n“Power analysis or other very simple calculations, which are widely used in human clinical trials and are often expected by regulatory authorities in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a scientifically robust and efficient way of determining animal numbers and may ultimately help to prevent animals being used unnecessarily. Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\nImportant\n\n\nAs scientists, you - yourselves - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.\nYou cannot simply rely on the word of “experienced scientists” for this.\n\n\n\n\n\nThe Kilkenny paper does propose solutions to this problem\nThey require the use and reporting of straightforward statistical calculations\nIt is up to you as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods."
  },
  {
    "objectID": "index.html#the-arrive-guidelines",
    "href": "index.html#the-arrive-guidelines",
    "title": "MP968 Experimental Design Workshop",
    "section": "The ARRIVE guidelines",
    "text": "The ARRIVE guidelines\n\n\nThe following year (Kilkenny et al. (2010)) proposed the ARRIVE guidelines: a checklist to help researchers report their animal research transparently and reproducibly.\n\nGood reporting is essential for peer review and to inform future research\nReporting guidelines measurably improve reporting quality\nImproved reporting maximises the output of published research"
  },
  {
    "objectID": "index.html#arrive-guidelines-highlightes",
    "href": "index.html#arrive-guidelines-highlightes",
    "title": "MP968 Experimental Design Workshop",
    "section": "ARRIVE guidelines highlightes",
    "text": "ARRIVE guidelines highlightes\nMany journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (Kilkenny et al. (2010)):\n\n\n\n\n\n\n\nARRIVE guidelines (highlights)\n\n\n\n\nObjectives: primary and any secondary objectives of the study, or specific hypotheses being tested\n\n\nStudy design: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit\n\n\nSample size: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided\n\n\nStatistical methods: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach\n\n\nOutcomes and estimation: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval)."
  },
  {
    "objectID": "index.html#a-vital-step",
    "href": "index.html#a-vital-step",
    "title": "MP968 Experimental Design Workshop",
    "section": "A vital step",
    "text": "A vital step\n\n\n\n\n\n\n\n\nWarning\n\n\n“A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues.”\n\n\n\n\n\n\n\n\n\n\nRecommended reading\n\n\nBate and Clark (2014)"
  },
  {
    "objectID": "index.html#random-variables",
    "href": "index.html#random-variables",
    "title": "MP968 Experimental Design Workshop",
    "section": "Random variables",
    "text": "Random variables\nYour experimental measurements are random variables\n\n\n\n\n\n\nImportant\n\n\nThis does not mean that your measurements are entirely random numbers\n\n\n\n\n\n\n\n\n\nCaution\n\n\nRandom variables are values whose range is subject to some element of chance, e.g. variation between individuals\n\nTail length (e.g. timing of developmental signals, distribution of nutrients)\nBlood concentrations (e.g. circulatory heterogeneity, transient measurement differences)\nSurvival time (e.g. determining point of death)\n\n\n\n\n\n\nWhen you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a random variable\n\nThis doesn’t mean that the number is chosen randomly\n\nWhat it means is that the number is subject to the influence of some kind of random variation\n\nSo tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals\nBlood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration\nSurvival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time"
  },
  {
    "objectID": "index.html#probability-distributions",
    "href": "index.html#probability-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Probability distributions",
    "text": "Probability distributions\nThe probability distribution of a random variable \\(z\\) (e.g. the values you measure in an experiment) takes on some range of values\n\n\n\n\n\n\n\nThe mean of the distribution of \\(z\\)\n\n\n\nThe mean (aka expected value or expectation) is the average of all the values in \\(z\\)\n\nEquivalently: the mean is the value that is obtained on average from a random sample from the distribution\n\nWritten as \\(\\mu_{z}\\) or \\(E(z)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe variance of a distribution of \\(z\\)\n\n\n\nThe variance of the distribution of \\(z\\) represents the expected mean squared difference from the mean \\(\\mu_z\\) (or \\(E(z)\\)) of a random sample from the distribution.\n\n\\(\\textrm{variance} = E((z - \\mu_z)^2)\\)\n\n\n\n\n\n\n\n\nA probability distribution describes the range of values that a random variable - let’s call it \\(z\\) - takes.\n\nYou can think of \\(z\\) being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it\n\nProbability distributions can describe many measures, including\n\nheights of men, incomes of women, political party preference, and so on\n\nThe mean of a probability distribution - its expected value or expectation is the average of all numbers in the distribution\n\nThis may be infinite, and there is an equivalent definition: the mean is the value obtained on average from a random sample taken from the distribution\n\nThe variance of a probability distribution expresses how much the individual values might differ from that mean.\n\nVariance is defined as the mean of the square of the difference between each individual value in the distribution and the mean of the distribution"
  },
  {
    "objectID": "index.html#understanding-variance",
    "href": "index.html#understanding-variance",
    "title": "MP968 Experimental Design Workshop",
    "section": "Understanding variance",
    "text": "Understanding variance\n\n\n\n\n\n\n\nA distribution where all values of \\(z\\) are the same\n\n\n\nEvery single value in the distribution (\\(z\\)) is also the mean value (\\(\\mu_z\\)), therefore\n\n\\[z = \\mu_z \\implies z - \\mu_z = 0 \\implies (z - \\mu_z)^2 = 0\\] \\[\\textrm{variance} = E((z - \\mu_z)^2) = E(0^2) = 0\\]\n\n\n\n\n\n\n\n\n\n\n\nAll other distributions\n\n\nIn every other distribution, there are some values of \\(z\\) that differ so, for at least some values of \\(z\\)\n\\[z \\neq \\mu_z \\implies  z - \\mu_z \\neq 0 \\implies (z - \\mu_z)^2 \\gt 0 \\] \\[\\implies \\textrm{variance} = E((z - \\mu_z)^2) \\gt 0 \\]\n\n\n\n\n\n\nTo get some intuition about the idea of variance, imagine that you have a set of values \\(z\\) that are all the same, so there’s no randomness\nIf the value are all the same, then they all have the same value as the mean of the distribution, and \\(z - \\mu_z = 0\\)\nThis means that, for all \\(z\\), \\((z - \\mu_z)^2\\) is also zero, and the variance is zero.\nBut if any value of \\(z\\) is different then, for at least that value, \\(z - \\mu_z \\neq 0\\), so the square of that value is greater than zero. It follows that the variance must then be greater than zero: \\(E((z - \\mu_z)^2) \\gt 0\\)\nIn any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value"
  },
  {
    "objectID": "index.html#standard-deviation",
    "href": "index.html#standard-deviation",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard deviation",
    "text": "Standard deviation\n\n\n\n\n\n\n\nWhat is standard deviation?\n\n\nThe standard deviation is the square root of the variance\n\\[\\textrm{standard deviation} = \\sigma_z = \\sqrt{\\textrm{variance}} = \\sqrt{E((z - \\mu_z)^2)} \\]\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\nThe standard deviation (unlike variance) is on the same scale as the original distribution\n\nStandard deviation is a more “natural-seeming” interpretation of variation\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe can calculate mean, variance, and standard deviation for any probability distribution.\n\n\n\n\n\nYou are probably more familiar with the standard deviation of a distribution.\nThe standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance\nWhile we can calculate the mean, variance and standard deviation for any distribution, this does not mean that they are equally informative for all distributions\nLet’s look at some examples"
  },
  {
    "objectID": "index.html#normal-distribution-1",
    "href": "index.html#normal-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 1",
    "text": "Normal Distribution 1\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma_z) \\]\n\n\n\n\n\n\nNote\n\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\n\n\n\n\n\n\n\n\nTip\n\n\nMeasurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThere is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!\n\n\n\n\n\nYou may see the normal distribution written like this, as “z is distributed as the Normal distribution with mean \\(\\mu_z\\) and standard deviation \\(\\sigma_z\\).”\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\nValues we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors\n\nThis is a consequence of the Central Limit Theorem\n\nBut there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution\n\nThis is especially the case if there is a large factor affecting variation of the variable"
  },
  {
    "objectID": "index.html#normal-distribution-2",
    "href": "index.html#normal-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 2",
    "text": "Normal Distribution 2\n\n\n\n\n\n\n\nTip\n\n\n\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean\n\n\n\n\n\n\nAs an example, we can look at some real data, the heights of men and women in the US.\n\nHere we have counts of individuals whose heights are measured to the nearest inch\n\nThese are two different random variables: one for men, one for women\nFor both distributions, we can calculate a mean, and a standard deviation (or variance)\n\nTherefore we can calculate a normal distribution representing both men’s and women’s heights (orange curve)\nWe calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)\n\nThe distributions of heights for each sex, separately, follow an approximate normal distribution\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean"
  },
  {
    "objectID": "index.html#non-normal-distribution-3",
    "href": "index.html#non-normal-distribution-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "(Non-)Normal Distribution 3",
    "text": "(Non-)Normal Distribution 3\n\n\n\n\n\n\n\nTip\n\n\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\n\n\n\n\n\n\nBy contrast, the distribution of heights of all adults in the US is not close to a normal curve\nThis is because there is an extraneous factor, sex, that represents much of the total variation in values\n\nWe will come back to this idea later\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\nMost data you receive will not be normally distributed"
  },
  {
    "objectID": "index.html#binomial-distribution-1",
    "href": "index.html#binomial-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 1",
    "text": "Binomial Distribution 1\n\n\nSuppose you’re taking shots in basketball\n\nhow many shots?\nhow likely are you to score?\nwhat is the distribution of the number of successful shots?\n\n\n\n\n\n\n\nTip\n\n\nThis kind of process generates a random variable approximating a probability distribution called a binomial distribution.\nIt is different from a normal distribution.\n\n\n\n\n\n\n\n\nSuppose that instead of measuring height, weight, concentration or something like that, you’re measuring event outcomes\n\nThese do not follow a normal distribution\n\nIf you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding\nThe number of successful shots is going to depend on the number of shots you take, and how likely you are to score\n\nMichael Jordan is much more likely to score any individual attempt than I am\n\nThe number of successful shots is a random variable with a probability distribution\nThis kind of process generates a probability distribution that approximates the binomial distribution\n\nIt’s the same one you get for coin tosses (or any yes/no process)\n\nIt is different from a normal distribution\n\nIf your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution"
  },
  {
    "objectID": "index.html#binomial-distribution-2",
    "href": "index.html#binomial-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 2",
    "text": "Binomial Distribution 2\n\n\n\\[ z \\sim \\textrm{binomial}(n, p) \\]\n\n\n\n\n\n\nTip\n\n\n\nnumber of shots, \\(n = 20\\)\nprobability of scoring, \\(p = 0.3\\)\n\n\\[z \\sim \\textrm{binomial}(20, 0.3) \\]\n\n\n\n\n\n\n\n\n\n\nmean and sd\n\n\n\\[ \\textrm{mean} = n \\times p \\] \\[ \\textrm{sd} = \\sqrt{n \\times p \\times (1-p)}\\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(p\\) differ between two conditions?\n\n\n\n\n\n\n\n\n\n\nImagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot\nThe distribution of shots you scored would follow a binomial distribution with \\(n=20\\) and \\(p=0.3\\)\nYou’d expect to score, on average, \\(20 \\times 0.3 = 6\\) times\nThe standard deviation we’d expect would be \\(\\sqrt{20 \\times 0.3 \\times (1 - 0.3)} = \\sqrt{6 \\times (0.7)} = \\sqrt{4.2} = 2.05\\)\n\nSo you’d expect to score about 4 to 8 shots, most of the time"
  },
  {
    "objectID": "index.html#poisson-distribution-1",
    "href": "index.html#poisson-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 1",
    "text": "Poisson distribution 1\n\n\n\nIn prior experiments the frequency of calcium events in WKY was 3.8 \\(\\pm\\) 1.1 events/field/min compared to 18.9 \\(\\pm\\) 7.1 in SHR\n\n\n\n\n\n\n\n\nThis is not normal (or binomial)\n\n\nSomething that happens a certain number of times in a fixed interval generates a Poisson distribution.\nThis is different from a normal or binomial distribution.\n\n\n\n\n\n\n\n\n\nSuppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR\n\nYou count the number of times the calcium spikes in a minute, in your field of view\n\nThis data is rate data\n\nA count of events per unit (here, unit time and unit area) interval\n\nThe idealised representation of data generated by this process is a Poisson distribution\n\nThis differs from a normal or binomial distribution"
  },
  {
    "objectID": "index.html#poisson-distribution-2",
    "href": "index.html#poisson-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 2",
    "text": "Poisson distribution 2\n\\[z \\sim \\textrm{poisson}(\\lambda)\\]\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\\[ \\textrm{mean} = \\lambda \\] \\[ \\textrm{sd} = \\sqrt{\\lambda} \\]\n\n\n\n\n\n\n\n\n\n\n\nExpectation (\\(\\lambda\\))\n\n\n\nOnly one parameter is provided, \\(\\lambda\\): the rate with which the measured event happens\nSuppose a county has population 100,000, and average rate of cancer is 45.2mn people each year\n\n\\[z \\sim \\textrm{poisson}(45,200,000/100,000) = \\textrm{poisson}(4.52) \\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(\\lambda\\) differ between two conditions?\n\n\n\n\n\n\n\nCount or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the Poisson distribution\nExamples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.\nThe distribution takes only one parameter: the expectation, lambda\nIf your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions"
  },
  {
    "objectID": "index.html#binomial-and-poisson-distributions",
    "href": "index.html#binomial-and-poisson-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial and Poisson distributions",
    "text": "Binomial and Poisson distributions\n\n\n\n\n\n\n\n\nSome important features\n\n\n\nAll measured values are positive whole numbers or zero; means may be positive real numbers or zero\nThe distributions may not be unimodal\nThe mean is not always the peak value (mode)\nThe distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)\n\n\n\n\n\n\n\nBinomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings\n\nThe distributions are only defined at whole values - the lines are to guide your eye\n\nFor binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases\n\nMore attempts also increases the variance of the distribution\n\nFor poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value\nIn both cases, all values - counts/rates or successes - are positive whole numbers"
  },
  {
    "objectID": "index.html#distributions-in-practice",
    "href": "index.html#distributions-in-practice",
    "title": "MP968 Experimental Design Workshop",
    "section": "Distributions in Practice",
    "text": "Distributions in Practice\n\n\n\n\n\n\n\nDistributions are starting points\n\n\n\nDistributions arise from and represent distinct generation processes\n\nNormal distributions are generated by sums, differences, and averages\nPoisson distributions are generated by counts (per unit interval)\nBinomial distributions are generated by success/failure outcomes\n\nDesign experiments with analyses that reflect these processes\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll statistical distributions are idealisations that ignore many features of real data\nNo real world data should be expected to exactly match any statistical distribution\nPoisson models tend to need adjustment for overdispersion\n\n\n\n\n\n\nThe different distributions we’ve looked at arise from distinct generation processes that have parallels in the real world\n\nNormal distributions arise when independent values are summed, or we take differences or averages of them\nPoisson distributions arise from counts, or rates (i.e. counts per unit)\nBinomial distributions arise from success/failure outcomes like tossing a coin\n\nWe need to take care though, as these distributions are idealised outcomes from those processes\nIn the real world there are many other influences that cause collected data to deviate from these idealisations\n\nThere is no reason to expect real world data to exactly match any statistical distribution\nThough there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)"
  },
  {
    "objectID": "index.html#normal-distribution-redux",
    "href": "index.html#normal-distribution-redux",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution Redux",
    "text": "Normal Distribution Redux\n\n\n\n\n\n\n\n\nProbability mass\n\n\n\napproximately 50% of the distribution lies in the range \\(\\mu \\pm 0.68\\sigma\\)\napproximately 68% of the distribution lies in the range \\(\\mu \\pm \\sigma\\)\napproximately 95% of the distribution lies in the range \\(\\mu \\pm 2\\sigma\\)\napproximately 99.7% of the distribution lies in the range \\(\\mu \\pm 3\\sigma\\)\n\n\n\n\n\n\n\nAn intuition worth developing is how much of a (normal) distribution lies within some range of the mean\n\nThis is going to be useful whenthinking about p-values and hypothesis tests\n\nHere we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.\n\nIt’s a normal distribution, so symmetrical about the mean\nThe area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution\nThe area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution\n\nIt will be useful to have this intuition for the next section"
  },
  {
    "objectID": "index.html#parameters",
    "href": "index.html#parameters",
    "title": "MP968 Experimental Design Workshop",
    "section": "Parameters",
    "text": "Parameters\nParameters are unknown numbers that determine a statistical model\n\n\n\n\n\n\n\nA linear regression\n\n\n\\[ y_i = a + b x_i \\]\n\nParameters are:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA normal distribution representing your data\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nParameters are: \\(\\mu_z\\) and \\(\\sigma\\)\n\n\n\n\n\n\n\nParameters are numbers that you don’t know in a statistical model\n\nYou estimate parameter values from the data you collect\n\nSo, if you have a linear regression and are estimating the intercept and gradient in the equation \\(y_i = a + b x_i\\)\n\nThe intercept \\(a\\), and gradient \\(b\\), are the parameters you are estimating\n\nYou may be representing the data you collect by a normal distribution\n\nThis is described by the parameters \\(\\mu_z\\) and \\(\\sigma\\), which you are estimating from your data"
  },
  {
    "objectID": "index.html#estimands",
    "href": "index.html#estimands",
    "title": "MP968 Experimental Design Workshop",
    "section": "Estimands",
    "text": "Estimands\nAn estimand (or quantity of interest) is a value that we are interested in estimating\n\n\n\n\n\n\n\nA linear regression\n\n\n\\[ y_i = a + b x_i\\]\n\nWe want to estimate values for:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\npredicted outcomes at important values of \\(x_i\\)\n\n\nThese are all estimands, and estimates are represented using the “hat” symbol: \\(\\hat{a}\\), \\(\\hat{b}\\), etc.\n\n\n\n\n\n\n\n\n\n\n\nA normal distribution representing your data\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nEstimands are: \\(\\mu_z\\) and \\(\\sigma\\)\n\nMaybe you want to determine the 95% confidence interval - this is also an estimand"
  },
  {
    "objectID": "index.html#standard-errors-and-confidence-intervals",
    "href": "index.html#standard-errors-and-confidence-intervals",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard Errors and Confidence Intervals",
    "text": "Standard Errors and Confidence Intervals\n\nThe standard error is the estimated standard deviation of an estimate\n\nIt is a measure of our uncertainty about the quantity of interest\n\n\n\n\n\n\n\n\nNote\n\n\n\nStandard error gets smaller as sample size gets larger\n\nYou know more about the most likely value, the more data/information you collect\nStandard error tends to zero as sample size gets large enough\n\n\n\n\n\n\nThe confidence interval (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data\n\n\n\n\n\n\n\nImportant\n\n\n\nIn repeated applications, the 50% confidence interval will include the true value 50% of the time\n\nA 95% confidence interval will include the true value 95% of the time\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe usual 95% confidence interval rule of thumb for large samples (assuming a normal distribution) is to take the estimate \\(\\pm\\) two standard errors\n\n\n\n\n\n\nNow I’ve mentioned confidence intervals we’ll need to talk about them"
  },
  {
    "objectID": "index.html#statistical-significance-1",
    "href": "index.html#statistical-significance-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 1",
    "text": "Statistical significance 1\n\nSome scientists choose to consider a result to be “stable” or “real” if it is “statistically significant”\nThey may also consider “non-signifcant” results to be noisy or less reliable\n\n\n\n\n\n\n\nWarning\n\n\nI, and many other statisticians, do not recommend this approach.\nHowever, the concept is widespread and we need to discuss it\n\n\n\n\n\nEven if you’re not very familiar with what it means precisely, I’m sure you’ve come across “statistical” significance in at least one scientific context.\n“Statistical significance” is a decision rule used by some scientists to consider whether a result is “stable” or “real”\n\nThey may also use “statistical significance” to exclude some results as noisy or unreliable\n\nI and many other statisticians do not recommend this approach, and we’ll see why"
  },
  {
    "objectID": "index.html#statistical-significance-2",
    "href": "index.html#statistical-significance-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 2",
    "text": "Statistical significance 2\n\n\n\n\n\n\n\nA common definition\n\n\n\nStatistical significance is conventionally defined as a threshold (commonly stated as a \\(p\\)-value less than 0.05) relative to some null hypothesis or prespecified value that indicates no effect is present.\nE.g., an estimate may be considered “statistically significant at \\(P &lt; 0.05\\)” if it:\n\nlies at least two standard errors from the mean\nis a difference that lies at least two standard errors from zero\n\nMore generally, an estimate is “not statistically significant” if, e.g.\n\nthe observed value can reasonably be explained by chance variation\nit is a difference that lies less than two standard errors from zero\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis process relies on probability distributions\n\n\n\nWe need to relate the measured values in the real world to an appropriate distribution\n\n\n\n\n\n\n\nThe way you’re likely to see statistical significance presented is that some threshold - usually a p-value of less than 0.05 - is applied in a statistical test, relative to some null hypothesis or prespecified value that indicates the absence of an effect\nSo you are likely to see an estimate be considered “statistically significant at P&lt;0.05” if it lies at least two SEs from the mean, or is a difference that lies at least two SEs from zero\nConversely, you will see estimates be considered “not statistically significant at P&lt;0.05” if they lie less than two SEs from the mean, or the observed value can reasonably be explained by chance variation alone\nAs you might have guessed from the talk of standard errors and distances from means, we map our measured values in the real world to statistical probability distributions to calculate these values."
  },
  {
    "objectID": "index.html#a-simple-example-the-experiment",
    "href": "index.html#a-simple-example-the-experiment",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The experiment",
    "text": "A simple example: The experiment\n\n\n\n\n\n\n\nThe experiment\n\n\n\nTwo drugs, \\(C\\) and \\(T\\) lower cholesterol1, and we want to compare their effectiveness\nWe randomise assignment of \\(C\\) and \\(T\\) to members of a single cohort of comparable individuals, whose pre-treatment cholesterol level is assumed to be drawn from the same distribution\nWe measure the post-treatment cholesterol levels \\(y_T\\) and \\(y_C\\) for each individual in the two groups.\nWe calculate the average measured \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\nWe also calculate standard deviation for the two groups, \\(\\sigma_T\\) and \\(\\sigma_C\\)\n\n\n\n\n\n\n\n\nLet’s consider a simple experiment\nWe want to compare the efficacy of two drugs, C and T, in their ability to lower cholesterol\n\nWe assume a uniform pool of individuals, and randomly assign C and T to two equally-sized groups drawn from that cohort\nThis implies that the starting cholesterol level of all individuals is drawn from the same distribution\n\nWe administer the drugs in the same way, for the same period of time, and measure the post-treatment cholesterol level in each individual\n\nWe calculate the mean and standard deviation for the two groups\nThese allow us to estimate the underlying normal distribution of post-treatment cholesterol levels in each group\n\nWe can use the estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\n\n\\(C\\) for control, the current best-in class; \\(T\\) for treatment, the new compound"
  },
  {
    "objectID": "index.html#a-simple-example-the-hypotheses",
    "href": "index.html#a-simple-example-the-hypotheses",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The hypotheses",
    "text": "A simple example: The hypotheses\n\nWe want to know if the treatments have different sizes of effect\n\nIf they do, there should be a difference between the (average) post-treatment cholesterol level in each group\nThe true post-treatment levels are \\(\\theta_T\\) and \\(\\theta_C\\)\nWe have estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for post-treatment levels\n\n\n\n\n\n\n\n\n\nThe hypotheses\n\n\n\nWe are interested in \\(\\theta = \\theta_T - \\theta_C\\), the expected post-test difference in cholesterol between the two groups \\(T\\) and \\(C\\).\nOur null hypothesis (\\(H_0\\)) is that \\(\\theta = 0\\), i.e. there is no difference (\\(\\theta_C = \\theta_T\\))\nOur alternative hypothesis (\\(H_1\\)) is that there is a difference, so \\(\\theta \\neq 0\\), (i.e. \\(\\theta_C \\neq \\theta_T\\))\n\n\n\n\n\n\n\nOur goal in this experiment is to determine whether the treatments have different effects on lowering cholesterol\n\nIf they do, then there should be a difference between the (average) post-treatment cholesterol levels of groups T and C\n\nThere are true post treatment levels for each group: \\(\\theta_T\\) and \\(\\theta_C\\)\n\nBut we have only estimated them as \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\)\nSo we turn to a hypothesis test to determine how likely it is our estimates support there being a difference in the true levels between the two groups\n\nWe set up a null hypothesis that there is no difference between the groups: \\(\\theta = 0\\)\n\nIn this hypothesis, any deviation from zero we see is considered the result of chance variation in the data, or error we cannot eliminate from measurements\n\nWe also define an alternative hypothesis that there is a difference between the groups: \\(\\theta \\neq 0\\)\n\nWe’re not insisting that \\(\\theta\\) is greater than or less than zero, just that it’s not zero\nAnd also that any deviation from zero is not accounted for by the null hypothesis"
  },
  {
    "objectID": "index.html#a-simple-example-the-distribution-1",
    "href": "index.html#a-simple-example-the-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The distribution 1",
    "text": "A simple example: The distribution 1\n\nTo perform a statistical test, we need to assume a distribution and parameters for the null hypothesis\n\nWe can then test the observed estimate against that distribution to see how likely it is that the null hypothesis would have generated it\n\n\n\n\n\n\n\n\n\nThe distribution\n\n\n\nWe use a probability distribution reflecting generation of the null hypothesis: \\(\\theta_C = \\theta_T\\)\n\nThis allows us to define a test statistic \\(T\\) (i.e. a threshold probability of “significance”) in advance\n\nWe test the estimated value from the experiment (\\(\\bar{y}_T - \\bar{y}_C\\)) to calculate a \\(p\\)-value for our estimate: \\(p = \\textrm{Pr}(T(y^{\\textrm{null}}) &gt; T(\\bar{y}_T - \\bar{y}_C))\\)\n\n\n\n\n\n\n\nAt this point we haven’t defined what our null hypothesis distribution should be, but we need to do so\n\nOnce we have this, we can test our observed estimate of \\(\\hat{\\theta} = \\bar{y}_T - \\bar{y}_C\\) against the distribution to see how likely it is to be explained by the null hypothesis\n\nWe need to choose a probability distribution that reflects the process generating the null hypothesis\n\nIn this case, we could shoose a normal distribution, which is typically appropriate for this kind of measurement\n\nThen we can define a test statistic \\(T\\) in advance\n\n\\(T\\) represents a threshold value in the null distribution - one where we would consider a larger difference to be meaningful\nYou may be familiar with setting P&lt;0.05, where the probability of seeing some value in the null distribution is 0.05 or less, and we might choose to set a value of \\(T\\) that corresponds to this\n\nWe then calculate the test statistic corresponding to our actual estimate, and calculate the \\(p\\)-value\n\nThis is the probability of observing our estimate, or a more extreme value, under the assumptions of the null model distribution"
  },
  {
    "objectID": "index.html#a-simple-example-the-null-hypothesis",
    "href": "index.html#a-simple-example-the-null-hypothesis",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The null hypothesis",
    "text": "A simple example: The null hypothesis\n\n\n\n\n\n\n\nThe null hypothesis\n\n\n\nAssume that the true difference \\(\\theta\\) is normally-distributed with \\(\\mu_\\theta=0\\), \\(\\sigma_\\theta=1\\)\n\n\n\n\n\n\n\n\nSuppose that the true difference between means is zero units, with standard error one unit, and that it is normally distributed\nThe probability distribution would look like this"
  },
  {
    "objectID": "index.html#a-simple-example-the-estimated-difference",
    "href": "index.html#a-simple-example-the-estimated-difference",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The estimated difference",
    "text": "A simple example: The estimated difference\n\n\n\n\n\n\n\nObserved between post-treatment levels: \\(\\bar{y}_T - \\bar{y}_C = -1.4\\)\n\n\n\nIs this an unlikely outcome given the null hypothesis?\n\n\n\n\n\n\n\n\nSuppose that we measured the post treatment levels in each group, and found the difference between the means to be -1.4 units\n\nThat is, the \\(T\\) drug group had 1.4 units lower cholesterol than the \\(C\\) group\n\nIs that unlikely, given the null hypothesis?\nWell, we don’t know if it’s unlikely because we haven’t decided what “unlikely” means\n\nTo do that we need to set a significance threshold"
  },
  {
    "objectID": "index.html#a-simple-example-a-significance-threshold",
    "href": "index.html#a-simple-example-a-significance-threshold",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: A significance threshold",
    "text": "A simple example: A significance threshold\n\n\n\n\n\n\n\nWe choose a significance threshold in advance\n\n\n\nSuppose we set a threshold \\(T\\) corresponding to the 90% confidence interval (i.e. \\(P&lt;0.1\\))\n\nIf the estimate is not in the central 90% of the distribution, we’ll say it’s “significant”\n\n\n\n\n\n\n\n\n\nWe must always decide what our significance threshold is in advance\nSuppose that we decide the central 90% of the distribution/null hypothesis is “not significant”\n\nThis is the 90% confidence interval around the mean\nWe can shade this in to see it better"
  },
  {
    "objectID": "index.html#a-simple-example-compare-the-estimate",
    "href": "index.html#a-simple-example-compare-the-estimate",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Compare the estimate",
    "text": "A simple example: Compare the estimate\n\n\n\n\n\n\n\nCompare the estimate to the threshold\n\n\n\nThe estimate lies outwith the threshold, so we call the difference “significant”\n\n\n\n\n\n\n\n\nWe can overlay the observed difference on the distribution here, and see that the estimate lies outside the 90% confidence interval\n\nWith our stated assumption of a \\(P &lt; 0.1\\) significance threshold, we would call this difference “significant”"
  },
  {
    "objectID": "index.html#a-simple-example-another-threshold",
    "href": "index.html#a-simple-example-another-threshold",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Another threshold",
    "text": "A simple example: Another threshold\n\n\n\n\n\n\n\nWe choose a significance threshold in advance\n\n\n\nSuppose we set the threshold \\(T\\) corresponding to the 95% confidence interval (i.e. \\(P&lt;0.05\\)) instead?\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "index.html#a-simple-example-another-outcome",
    "href": "index.html#a-simple-example-another-outcome",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Another outcome",
    "text": "A simple example: Another outcome\n\n\n\n\n\n\n\nCompare the estimate to the threshold\n\n\n\nThe estimate lies within the threshold, so the difference is “not significant”\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "index.html#a-simple-example-what-changed",
    "href": "index.html#a-simple-example-what-changed",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: What changed?",
    "text": "A simple example: What changed?\n\n\n\n\n\n\n\nWhat did not change\n\n\n\nThe null hypothesis was the same\nThe observed estimate of difference was the same\n\n\n\n\n\n\n\n\n\n\n\n\nWhat changed\n\n\n\nOur choice of significance threshold changed\n\n\n\n\n\n\n\n\n\n\n\n\nSignificance threshold choice\n\n\n\nOnce the estimate is known, it is always possible to find a threshold that makes it “significant” or “not significant”\nIt is dishonest to select a threshold deliberately to make your result “significant” or “not significant”\nAlways choose and record (preregister) your threshold for significance ahead of the experiment"
  },
  {
    "objectID": "index.html#tailed-tests-two-tailed",
    "href": "index.html#tailed-tests-two-tailed",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: two-tailed",
    "text": "Tailed tests: two-tailed\n\n\n\n\n\n\n\nUse two tails if direction of change doesn’t matter\n\n\n\nWith a two-tailed hypothesis test, we do not care about the direction of change\n\n\n\n\n\n\n\n\nThe example we’ve been looking at is a two-tailed test\nThis means that we’re concerned with whether the estimate lies within the central mass of the null distribution\n\nThe central 95% of the distribution has 2.5% of the distribution to the left, and 2.5% of the distribution to the right\nIf the estimate lies in either of those extreme regions, it’s in the “extreme 5%” from the mean, and we call it “significant”\nAny estimate lying within the central 95% of the distribution is “not significant”"
  },
  {
    "objectID": "index.html#tailed-tests-one-tailed-left",
    "href": "index.html#tailed-tests-one-tailed-left",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: one-tailed (left)",
    "text": "Tailed tests: one-tailed (left)\n\n\n\n\n\n\n\nUse one-tailed tests when direction matters\n\n\n\nIf we’re testing specifically for a negative difference/reduction, use a left-tailed test\ne.g. if we wanted to know if \\(T\\) reduced post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\n\n\n\n\nWhen we know we want to test for change in a specific direction, we should use a one-tailed test\nWhen checking specifically for a negative change where \\(P &lt; 0.05\\), say that \\(T\\) reduces cholesterol level by more than \\(C\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the right-most 95% of the distribution mass\n\nThis means that results in the left-most 5% of the mass are “significant”\nAny estimate lying in the right-most 95% of the distribution is “not significant”"
  },
  {
    "objectID": "index.html#tailed-tests-one-tailed-right",
    "href": "index.html#tailed-tests-one-tailed-right",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: one-tailed (right)",
    "text": "Tailed tests: one-tailed (right)\n\n\n\n\n\n\n\nUse one-tailed tests when direction matters\n\n\n\nIf we’re testing specifically for a positive difference/increase, use a right-tailed test\ne.g. if we wanted to know if \\(T\\) increased post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\n\n\n\n\nSimilarly, if checking specifically for a postitive change where \\(P &lt; 0.05\\), say that \\(C\\) reduces cholesterol level by more than \\(T\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the left-most 95% of the distribution mass\n\nThis means that results in the right-most 5% of the mass are “significant”\nAny estimate lying in the left-most 95% of the distribution is “not significant”\n\nIt’s possible to dishonestly switch a result between “significant” and “not significant” by choosing to use a one-tailed or two-tailed test"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-1",
    "href": "index.html#problems-with-statistical-significance-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 1",
    "text": "Problems with statistical significance 1\n\n\n\n\n\n\nWarning\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\n\n\nStatistical significance is not the same as practical importance\n\n\n\nSuppose a treatment increased earnings by £10 per year with a standard error of £2.\n\nThis would be statistically, but not practically, significant\n\nSuppose a different treatment increased earnings by £10,000 per year with a standard error of £10,000\n\nThis would not be statistically significant, but could be important in practice"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-2",
    "href": "index.html#problems-with-statistical-significance-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 2",
    "text": "Problems with statistical significance 2\n\n\n\n\n\n\nWarning\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\n\n\nNon-significance is not the same as zero\n\n\n\nSuppose an arterial stent treatment group outperforms the control\n\nmean difference in treadmill time: 16.6s (standard error 9.8)\nthe 95% confidence interval for the effect includes zero, \\(p ≈ 0.20\\)\n\nIt’s not clear whether the net treatment effect is positive or negative\n\nbut we can’t say that stents have no effect"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-3",
    "href": "index.html#problems-with-statistical-significance-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 3",
    "text": "Problems with statistical significance 3\n\n\n\n\n\n\n\nThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\nOnly a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\n\n\n\n\nIt should hopefully be obvious that any significance threshold is arbitrary, and the difference between a value being on one side of the threshold or the other may be arbitrarily small\nLess obviously, consider two independent studies - orange and purple here\n\nThe purple study estimates an effect size of 25 with standard error of 10 units and is “statistically significant” in that it does not include zero at the 1% significance level\nThe orange study estimates an effect size of 10 with standard error of 10 units and is “statistically not significant” as it includes zero even at the 10% significance level"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-4",
    "href": "index.html#problems-with-statistical-significance-4",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 4",
    "text": "Problems with statistical significance 4\n\n\n\n\n\n\n\nThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\nOnly a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\n\n\n\n\nHowever, the effect sizes estimated by the two studies are not different from each other at a “statistical significance” of 95%\n\nIn both cases, the central mass of each distribution includes the mean of the other"
  },
  {
    "objectID": "index.html#sample-size",
    "href": "index.html#sample-size",
    "title": "MP968 Experimental Design Workshop",
    "section": "Sample size",
    "text": "Sample size"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "References",
    "text": "References\n\n\n\n\nBate, Simon T., and Robin A. Clark. 2014. The Design and Statistical Analysis of Animal Experiments. Cambridge University Press.\n\n\nKilkenny, Carol, William J Browne, Innes C Cuthill, Michael Emerson, and Douglas G Altman. 2010. “Improving Bioscience Research Reporting: The ARRIVE Guidelines for Reporting Animal Research.” PLoS Biol. 8 (6): e1000412.\n\n\nKilkenny, Carol, Nick Parsons, Ed Kadyszewski, Michael F W Festing, Innes C Cuthill, Derek Fry, Jane Hutton, and Douglas G Altman. 2009. “Survey of the Quality of Experimental Design, Statistical Analysis and Reporting of Research Using Animals.” PLoS One 4 (11): e7824."
  }
]