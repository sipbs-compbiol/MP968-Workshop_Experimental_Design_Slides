[
  {
    "objectID": "index.html#we-should-not-cause-unnecessary-suffering",
    "href": "index.html#we-should-not-cause-unnecessary-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "We should not cause unnecessary suffering",
    "text": "We should not cause unnecessary suffering\n\n\n\n\n\n\n\nWe should always minimise suffering\n\n\nThis may mean not performing an experiment at all. Not all new knowledge or understanding is worth causing suffering to obtain it.\nWhere there is sufficient justification to perform an experiment, we are ethically obliged to minimise the amount of distress or suffering that is caused, by designing the experiment to achieve this.\n\n\n\n\n\n\n\n\n\n\n\nWhy we need statistics\n\n\nIt may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.\nBut what is an acceptable (i.e. the least possible) amount of suffering necessary to obtain an informative result?\n\n\n\n\n\n\nNo-one likes talking about animal experiments.\nIt’s a difficult, emotive topic that crosses people’s moral red lines.\nAnimal experiments may be the only practical way to gain essential scientific knowledge\nWhatever you believe “suffering” means for an animal, our ethical premise is that this suffering should not be in vain"
  },
  {
    "objectID": "index.html#challenge",
    "href": "index.html#challenge",
    "title": "MP968 Experimental Design Workshop",
    "section": "Challenge",
    "text": "Challenge\n\n\n\n\n\n\n\nQuiz question\n\n\nSuppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. Which of the following choices will cause the least amount of suffering?\n\n\n\n\n\nUse three subjects per group so a standard deviation can be calculated\nUse just enough subjects to establish that the outcome is likely to be correct\nUse just enough subjects to be certain that the outcome is correct\nUse as many subjects as you have available, to avoid wastage\n\n\nWe carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is more likely or less likely to be the null hypothesis than some alternative), rather than 100% certain.\nIf we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering.\nIf we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering.\nIf we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering."
  },
  {
    "objectID": "index.html#how-many-individuals",
    "href": "index.html#how-many-individuals",
    "title": "MP968 Experimental Design Workshop",
    "section": "How many individuals?",
    "text": "How many individuals?\n\n\n\n\n\n\n\nThe appropriate number of subjects\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\n\nWhat assumptions are reasonable?\nWhat is an appropriate level of certainty?\n\nBy convention1 the usual level of certainty for a hypothesis test is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”\n\n\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\nThis may sound like a very flexible statement. What assumptions are reasonable? What is an appropriate level of certainty?\nWe’ll consider these questions again later but, for now, just know that by convention the usual level of certainty is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”._\nNote though that the appropriate level of certainty may change depending on the nature of the question being asked.\n\nConventions are guidelines, not rigid standards, and you should always consider whether a convention is appropriate in your use case"
  },
  {
    "objectID": "index.html#design-experiments-to-minimise-suffering",
    "href": "index.html#design-experiments-to-minimise-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "Design experiments to minimise suffering",
    "text": "Design experiments to minimise suffering\n\n\n\n\n\n\n\nExperimental design and statistics are intertwined\n\n\nOnce a research hypothesis has been devised:\n\nExperimental design is the process of devising a practical way of answering the question\nStatistics informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results\n\n\n\n\n\n\n\n\n\n\n\n\nDesign your experiment for…\n\n\n\nyour population or subject group (e.g. sex, age, prior history, etc.)\nyour intervention (e.g. drug treatment)\nyour contrast or comparison between groups (e.g. lung capacity, drug concentration, etc.)\nyour outcome (i.e. is there a measurable or clinically relevant effect)\n\n\n\n\n\n\nOnce a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.\nStatistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.\nYou should design your experiment specifically for your combination of population/subject group, the intervention you’re applying, the contrast or comparison you’re making, and the outcome you’re expecting to see - specifically a measurable or clinically relevant effect."
  },
  {
    "objectID": "index.html#the-importance-of-experimental-design",
    "href": "index.html#the-importance-of-experimental-design",
    "title": "MP968 Experimental Design Workshop",
    "section": "The importance of experimental design",
    "text": "The importance of experimental design\n\n\n\n“For scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\nWe rely on the reporting of the experiment to know if it was appropriate\n\n\n\n\n\n\n\n\n\n\n\nThe National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.\nIn 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.\nIt did not make for pleasant reading."
  },
  {
    "objectID": "index.html#causes-for-concern-1",
    "href": "index.html#causes-for-concern-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 1",
    "text": "Causes for concern 1\n\n\n\n“Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. […] Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the literature for good examples of experimental design\n\n\n\n\n\n\n\n\n\nThe state of the published literature around animal experiments was not good in 2009.\n\n40% of studies did not state the hypothesis or objective of the experiment\nMost papers did not use randomisation or blinding, although this is an essential practice to avoid bias\nOnly 70% of publications described their statistical methods at all"
  },
  {
    "objectID": "index.html#causes-for-concern-2",
    "href": "index.html#causes-for-concern-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 2",
    "text": "Causes for concern 2\n\n\n\n\n\n\n\nNo publication explained their choice for the number of animals used\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the verbal authority of ‘published scientists’ or ‘experienced scientists’ for good experimental design\n\n\n\n\n\n\n\n\n\nOne of the most shocking pieces of information is that, of the 48 papers surveyed, not a single one explained why they used the number of animals that they did.\nWe cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.\nThese papers are published. They are written, and the experiments conducted, by “experienced scientists”\nBeing an “experienced” or “published” scientist is clearly not a benchmark for good experimental design"
  },
  {
    "objectID": "index.html#very-strong-cause-for-concern",
    "href": "index.html#very-strong-cause-for-concern",
    "title": "MP968 Experimental Design Workshop",
    "section": "Very strong cause for concern",
    "text": "Very strong cause for concern\n\n“Power analysis or other very simple calculations, which are widely used in human clinical trials and are often expected by regulatory authorities in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a scientifically robust and efficient way of determining animal numbers and may ultimately help to prevent animals being used unnecessarily. Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\nImportant\n\n\nAs scientists, you - yourselves - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.\nYou cannot simply rely on the word of “experienced scientists” for this.\n\n\n\n\n\nThe Kilkenny paper does propose solutions to this problem\nThey require the use and reporting of straightforward statistical calculations\nIt is up to you as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods."
  },
  {
    "objectID": "index.html#the-arrive-guidelines",
    "href": "index.html#the-arrive-guidelines",
    "title": "MP968 Experimental Design Workshop",
    "section": "The ARRIVE guidelines",
    "text": "The ARRIVE guidelines\n\n\nThe following year Kilkenny et al. (2010) proposed the ARRIVE guidelines: a checklist to help researchers report their animal research transparently and reproducibly.\n\nGood reporting is essential for peer review and to inform future research\nReporting guidelines measurably improve reporting quality\nImproved reporting maximises the output of published research"
  },
  {
    "objectID": "index.html#arrive-guidelines-highlightes",
    "href": "index.html#arrive-guidelines-highlightes",
    "title": "MP968 Experimental Design Workshop",
    "section": "ARRIVE guidelines highlightes",
    "text": "ARRIVE guidelines highlightes\nMany journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (Kilkenny et al. (2010)):\n\n\n\n\n\n\n\nARRIVE guidelines (highlights)\n\n\n\n\nObjectives: primary and any secondary objectives of the study, or specific hypotheses being tested\n\n\nStudy design: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit\n\n\nSample size: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided\n\n\nStatistical methods: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach\n\n\nOutcomes and estimation: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval)."
  },
  {
    "objectID": "index.html#a-vital-step",
    "href": "index.html#a-vital-step",
    "title": "MP968 Experimental Design Workshop",
    "section": "A vital step",
    "text": "A vital step\n\n\n\n\n\n\n\n\nWarning\n\n\n“A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues.”\n\n\n\n\n\n\n\n\n\n\nRecommended reading\n\n\nBate and Clark (2014)"
  },
  {
    "objectID": "index.html#random-variables",
    "href": "index.html#random-variables",
    "title": "MP968 Experimental Design Workshop",
    "section": "Random variables",
    "text": "Random variables\nYour experimental measurements are random variables\n\n\n\n\n\n\nImportant\n\n\nThis does not mean that your measurements are entirely random numbers\n\n\n\n\n\n\n\n\n\nCaution\n\n\nRandom variables are values whose range is subject to some element of chance, e.g. variation between individuals\n\nTail length (e.g. timing of developmental signals, distribution of nutrients)\nBlood concentrations (e.g. circulatory heterogeneity, transient measurement differences)\nSurvival time (e.g. determining point of death)\n\n\n\n\n\n\nWhen you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a random variable\n\nThis doesn’t mean that the number is chosen randomly\n\nWhat it means is that the number is subject to the influence of some kind of random variation\n\nSo tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals\nBlood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration\nSurvival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time"
  },
  {
    "objectID": "index.html#probability-distributions",
    "href": "index.html#probability-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Probability distributions",
    "text": "Probability distributions\nThe probability distribution of a random variable \\(z\\) (e.g. what you measure in an experiment) takes on some range of values1\n\n\n\n\n\n\n\nThe mean of the distribution of \\(z\\)\n\n\n\nThe mean (aka expected value or expectation) is the average of all the values in \\(z\\)\n\nEquivalently: the mean is the value that is obtained on average from a random sample from the distribution\n\nWritten as \\(\\mu_{z}\\) or \\(E(z)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe variance of a distribution of \\(z\\)\n\n\n\nThe variance of the distribution of \\(z\\) represents the expected mean squared difference from the mean \\(\\mu_z\\) (or \\(E(z)\\)) of a random sample from the distribution.\n\n\\(\\textrm{variance} = E((z - \\mu_z)^2)\\)\n\n\n\n\n\n\n\n\nA probability distribution describes the range of values that a random variable - let’s call it \\(z\\) - takes.\n\nYou can think of \\(z\\) being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it\n\nProbability distributions can describe many measures, including\n\nheights of men, incomes of women, political party preference, and so on\n\nThe mean of a probability distribution - its expected value or expectation is the average of all numbers in the distribution\n\nThis may be infinite, and there is an equivalent definition: the mean is the value obtained on average from a random sample taken from the distribution\n\nThe variance of a probability distribution expresses how much the individual values might differ from that mean.\n\nVariance is defined as the mean of the square of the difference between each individual value in the distribution and the mean of the distribution\n\n\n\nHere we are considering a populations of an infinite number of measurements of \\(z\\)"
  },
  {
    "objectID": "index.html#understanding-variance",
    "href": "index.html#understanding-variance",
    "title": "MP968 Experimental Design Workshop",
    "section": "Understanding variance",
    "text": "Understanding variance\n\n\n\n\n\n\n\nA distribution where all values of \\(z\\) are the same\n\n\n\nEvery single value in the distribution (\\(z\\)) is also the mean value (\\(\\mu_z\\)), therefore\n\n\\[z = \\mu_z \\implies z - \\mu_z = 0 \\implies (z - \\mu_z)^2 = 0\\] \\[\\textrm{variance} = E((z - \\mu_z)^2) = E(0^2) = 0\\]\n\n\n\n\n\n\n\n\n\n\n\nAll other distributions\n\n\nIn every other distribution, there are some values of \\(z\\) that differ so, for at least some values of \\(z\\)\n\\[z \\neq \\mu_z \\implies  z - \\mu_z \\neq 0 \\implies (z - \\mu_z)^2 \\gt 0 \\] \\[\\implies \\textrm{variance} = E((z - \\mu_z)^2) \\gt 0 \\]\n\n\n\n\n\n\nTo get some intuition about the idea of variance, imagine that you have a set of values \\(z\\) that are all the same, so there’s no randomness\nIf the value are all the same, then they all have the same value as the mean of the distribution, and \\(z - \\mu_z = 0\\)\nThis means that, for all \\(z\\), \\((z - \\mu_z)^2\\) is also zero, and the variance is zero.\nBut if any value of \\(z\\) is different then, for at least that value, \\(z - \\mu_z \\neq 0\\), so the square of that value is greater than zero. It follows that the variance must then be greater than zero: \\(E((z - \\mu_z)^2) \\gt 0\\)\nIn any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value"
  },
  {
    "objectID": "index.html#standard-deviation",
    "href": "index.html#standard-deviation",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard deviation",
    "text": "Standard deviation\n\n\n\n\n\n\n\nStandard deviation is the square root of the variance\n\n\n\\[\\textrm{standard deviation} = \\sigma_z = \\sqrt{\\textrm{variance}} = \\sqrt{E((z - \\mu_z)^2)} \\]\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\nThe standard deviation (unlike variance) takes values on the same scale as the original distribution\n\nStandard deviation is a more “natural-seeming” interpretation of variation\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe can calculate mean, variance, and standard deviation for any probability distribution.\n\n\n\n\n\nYou are probably more familiar with the standard deviation of a distribution.\nThe standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance\nWhile we can calculate the mean, variance and standard deviation for any distribution, this does not mean that they are equally informative for all distributions\nLet’s look at some examples"
  },
  {
    "objectID": "index.html#normal-distribution-1",
    "href": "index.html#normal-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 1",
    "text": "Normal Distribution 1\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma_z) \\]\n\n\n\n\n\n\nNote\n\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\n\n\n\n\n\n\n\n\nTip\n\n\nMeasurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThere is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!\n\n\n\n\n\nYou may see the normal distribution written like this, as “z is distributed as the Normal distribution with mean \\(\\mu_z\\) and standard deviation \\(\\sigma_z\\).”\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\nValues we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors\n\nThis is a consequence of the Central Limit Theorem\n\nBut there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution\n\nThis is especially the case if there is a large factor affecting variation of the variable"
  },
  {
    "objectID": "index.html#normal-distribution-2",
    "href": "index.html#normal-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 2",
    "text": "Normal Distribution 2\n\n\n\n\n\n\n\nTip\n\n\n\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean\n\n\n\n\n\n\nAs an example, we can look at some real data, the heights of men and women in the US.\n\nHere we have counts of individuals whose heights are measured to the nearest inch\n\nThese are two different random variables: one for men, one for women\nFor both distributions, we can calculate a mean, and a standard deviation (or variance)\n\nTherefore we can calculate a normal distribution representing both men’s and women’s heights (orange curve)\nWe calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)\n\nThe distributions of heights for each sex, separately, follow an approximate normal distribution\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean"
  },
  {
    "objectID": "index.html#non-normal-distribution-3",
    "href": "index.html#non-normal-distribution-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "(Non-)Normal Distribution 3",
    "text": "(Non-)Normal Distribution 3\n\n\n\n\n\n\n\nTip\n\n\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\n\n\n\n\n\n\nBy contrast, the distribution of heights of all adults in the US is not close to a normal curve\nThis is because there is an extraneous factor, sex, that represents much of the total variation in values\n\nWe will come back to this idea later\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\nMost data you receive will not be normally distributed"
  },
  {
    "objectID": "index.html#binomial-distribution-1",
    "href": "index.html#binomial-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 1",
    "text": "Binomial Distribution 1\n\n\nSuppose you’re taking shots in basketball\n\nhow many shots?\nhow likely are you to score?\nwhat is the distribution of the number of successful shots?\n\n\n\n\n\n\n\nTip\n\n\nThis kind of process generates a random variable approximating a probability distribution called a binomial distribution.\nIt is different from a normal distribution.\n\n\n\n\n\n\n\n\nSuppose that instead of measuring height, weight, concentration or something like that, you’re measuring event outcomes\n\nThese do not follow a normal distribution\n\nIf you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding\nThe number of successful shots is going to depend on the number of shots you take, and how likely you are to score\n\nMichael Jordan is much more likely to score any individual attempt than I am\n\nThe number of successful shots is a random variable with a probability distribution\nThis kind of process generates a probability distribution that approximates the binomial distribution\n\nIt’s the same one you get for coin tosses (or any yes/no process)\n\nIt is different from a normal distribution\n\nIf your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution"
  },
  {
    "objectID": "index.html#binomial-distribution-2",
    "href": "index.html#binomial-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 2",
    "text": "Binomial Distribution 2\n\n\n\\[ z \\sim \\textrm{binomial}(n, p) \\]\n\n\n\n\n\n\nTip\n\n\n\nnumber of shots, \\(n = 20\\), probability of scoring, \\(p = 0.3\\)\n\n\\[z \\sim \\textrm{binomial}(20, 0.3) \\]\n\n\n\n\n\n\n\n\n\n\nmean and sd\n\n\n\\[ \\textrm{mean} = n \\times p \\] \\[ \\textrm{sd} = \\sqrt{n \\times p \\times (1-p)}\\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data. E.g., does \\(p\\) differ between two conditions?\n\n\n\n\n\n\n\n\n\nImagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot\nThe distribution of shots you scored would follow a binomial distribution with \\(n=20\\) and \\(p=0.3\\)\nYou’d expect to score, on average, \\(20 \\times 0.3 = 6\\) times\nThe standard deviation we’d expect would be \\(\\sqrt{20 \\times 0.3 \\times (1 - 0.3)} = \\sqrt{6 \\times (0.7)} = \\sqrt{4.2} = 2.05\\)\n\nSo you’d expect to score about 4 to 8 shots, most of the time"
  },
  {
    "objectID": "index.html#poisson-distribution-1",
    "href": "index.html#poisson-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 1",
    "text": "Poisson distribution 1\n\n\n\nIn prior experiments the frequency of calcium events in WKY was 3.8 \\(\\pm\\) 1.1 events/field/min compared to 18.9 \\(\\pm\\) 7.1 in SHR\n\n\n\n\n\n\n\n\nThis is not normal (or binomial)\n\n\nSomething that happens a certain number of times in a fixed interval generates a Poisson distribution.\nThis is different from a normal or binomial distribution.\n\n\n\n\n\n\n\n\n\nSuppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR\n\nYou count the number of times the calcium spikes in a minute, in your field of view\n\nThis data is rate data\n\nA count of events per unit (here, unit time and unit area) interval\n\nThe idealised representation of data generated by this process is a Poisson distribution\n\nThis differs from a normal or binomial distribution"
  },
  {
    "objectID": "index.html#poisson-distribution-2",
    "href": "index.html#poisson-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 2",
    "text": "Poisson distribution 2\n\\[z \\sim \\textrm{poisson}(\\lambda)\\]\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\\[ \\textrm{mean} = \\lambda \\] \\[ \\textrm{sd} = \\sqrt{\\lambda} \\]\n\n\n\n\n\n\n\n\n\n\n\nExpectation (\\(\\lambda\\))\n\n\n\nOnly one parameter is provided, \\(\\lambda\\): the rate with which the measured event happens\nSuppose a county has population 100,000, and average rate of cancer is 45.2mn people each year\n\n\\[z \\sim \\textrm{poisson}(45,200,000/100,000) = \\textrm{poisson}(4.52) \\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(\\lambda\\) differ between two conditions?\n\n\n\n\n\n\n\nCount or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the Poisson distribution\nExamples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.\nThe distribution takes only one parameter: the expectation, lambda\nIf your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions"
  },
  {
    "objectID": "index.html#binomial-and-poisson-distributions",
    "href": "index.html#binomial-and-poisson-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial and Poisson distributions",
    "text": "Binomial and Poisson distributions\n\n\n\n\n\n\n\n\nSome important features\n\n\n\nAll measured values (and \\(n\\)) are positive whole numbers or zero; \\(\\lambda\\), \\(p\\) may be positive real numbers or zero\nThe distributions may not be unimodal\nThe mean is not always the peak value (mode)\nThe distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)\n\n\n\n\n\n\n\nBinomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings\n\nThe distributions are only defined at whole values - the lines are to guide your eye\n\nFor binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases\n\nMore attempts also increases the variance of the distribution\n\nFor poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value\nIn both cases, all values - counts/rates or successes - are positive whole numbers"
  },
  {
    "objectID": "index.html#distributions-in-practice",
    "href": "index.html#distributions-in-practice",
    "title": "MP968 Experimental Design Workshop",
    "section": "Distributions in Practice",
    "text": "Distributions in Practice\n\n\n\n\n\n\n\nDistributions are starting points\n\n\n\nDistributions arise from and represent distinct generation processes (relate this to your biological system)\n\nNormal distributions are generated by sums, differences, and averages\nPoisson distributions are generated by counts (per unit interval)\nBinomial distributions are generated by success/failure outcomes\n\nDesign experiments with analyses that reflect these processes\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll statistical distributions are idealisations that ignore many features of real data\nNo real world data should be expected to exactly match any statistical distribution\nPoisson models tend to need adjustment for overdispersion\n\n\n\n\n\n\nThe different distributions we’ve looked at arise from distinct generation processes that have parallels in the real world\n\nNormal distributions arise when independent values are summed, or we take differences or averages of them\nPoisson distributions arise from counts, or rates (i.e. counts per unit)\nBinomial distributions arise from success/failure outcomes like tossing a coin\n\nWe need to take care though, as these distributions are idealised outcomes from those processes\nIn the real world there are many other influences that cause collected data to deviate from these idealisations\n\nThere is no reason to expect real world data to exactly match any statistical distribution\nThough there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)"
  },
  {
    "objectID": "index.html#normal-distribution-redux",
    "href": "index.html#normal-distribution-redux",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution Redux",
    "text": "Normal Distribution Redux\n\n\n\n\n\n\n\n\nProbability mass\n\n\n\napproximately 50% of the distribution lies in the range \\(\\mu \\pm 0.68\\sigma\\)\napproximately 68% of the distribution lies in the range \\(\\mu \\pm \\sigma\\)\napproximately 95% of the distribution lies in the range \\(\\mu \\pm 2\\sigma\\)\napproximately 99.7% of the distribution lies in the range \\(\\mu \\pm 3\\sigma\\)\n\n\n\n\n\n\n\nAn intuition worth developing is how much of a (normal) distribution lies within some range of the mean\n\nThis is going to be useful whenthinking about p-values and hypothesis tests\n\nHere we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.\n\nIt’s a normal distribution, so symmetrical about the mean\nThe area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution\nThe area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution\n\nIt will be useful to have this intuition for the next section"
  },
  {
    "objectID": "index.html#parameters",
    "href": "index.html#parameters",
    "title": "MP968 Experimental Design Workshop",
    "section": "Parameters",
    "text": "Parameters\nParameters are unknown numbers that determine a statistical model\n\n\n\n\n\n\n\nA linear regression\n\n\n\\[ y_i = a + b x_i \\]\n\nParameters are:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA normal distribution representing your data\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nParameters are: \\(\\mu_z\\) and \\(\\sigma\\)\n\n\n\n\n\n\n\nParameters are numbers that you don’t know in a statistical model\n\nYou estimate parameter values from the data you collect\n\nSo, if you have a linear regression and are estimating the intercept and gradient in the equation \\(y_i = a + b x_i\\)\n\nThe intercept \\(a\\), and gradient \\(b\\), are the parameters you are estimating\n\nYou may be representing the data you collect by a normal distribution\n\nThis is described by the parameters \\(\\mu_z\\) and \\(\\sigma\\), which you are estimating from your data"
  },
  {
    "objectID": "index.html#estimands",
    "href": "index.html#estimands",
    "title": "MP968 Experimental Design Workshop",
    "section": "Estimands",
    "text": "Estimands\nAn estimand (or quantity of interest) is a value that we are interested in estimating\n\n\n\n\n\n\n\nA linear regression\n\n\n\\[ y_i = a + b x_i\\]\n\nWe want to estimate values for:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\npredicted outcomes at important values of \\(x_i\\)\n\n\nThese are all estimands, and estimates are represented using the “hat” symbol: \\(\\hat{a}\\), \\(\\hat{b}\\), etc.\n\n\n\n\n\n\n\n\n\n\n\nA normal distribution representing your data\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nEstimands are: \\(\\mu_z\\) and \\(\\sigma\\)\n\nMaybe you want to determine the 95% confidence interval - this is also an estimand"
  },
  {
    "objectID": "index.html#standard-errors-and-confidence-intervals",
    "href": "index.html#standard-errors-and-confidence-intervals",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard Errors and Confidence Intervals",
    "text": "Standard Errors and Confidence Intervals\n\nThe standard error is the estimated standard deviation of an estimate\n\nIt is a measure of our uncertainty about the quantity of interest\n\n\n\n\n\n\n\n\nNote\n\n\n\nStandard error gets smaller as sample size gets larger\n\nYou know more about the most likely value, the more data/information you collect\nStandard error tends to zero as sample size gets large enough\n\n\n\n\n\n\nThe confidence interval (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data\n\n\n\n\n\n\n\nImportant\n\n\n\nIn repeated applications, the 50% confidence interval will include the true value 50% of the time\n\nA 95% confidence interval will include the true value 95% of the time\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe usual 95% confidence interval rule of thumb for large samples (assuming a normal distribution) is to take the estimate \\(\\pm\\) two standard errors\n\n\n\n\n\n\nNow I’ve mentioned confidence intervals we’ll need to talk about them"
  },
  {
    "objectID": "index.html#statistical-significance-1",
    "href": "index.html#statistical-significance-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 1",
    "text": "Statistical significance 1\n\nSome scientists choose to consider a result to be “stable” or “real” if it is “statistically significant”\nThey may also consider “non-signifcant” results to be noisy or less reliable\n\n\n\n\n\n\n\nWarning\n\n\nI, and many other statisticians, do not recommend this approach.\nHowever, the concept is widespread and we need to discuss it\n\n\n\n\n\nEven if you’re not very familiar with what it means precisely, I’m sure you’ve come across “statistical” significance in at least one scientific context.\n“Statistical significance” is a decision rule used by some scientists to consider whether a result is “stable” or “real”\n\nThey may also use “statistical significance” to exclude some results as noisy or unreliable\n\nI and many other statisticians do not recommend this approach, and we’ll see why"
  },
  {
    "objectID": "index.html#statistical-significance-2",
    "href": "index.html#statistical-significance-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 2",
    "text": "Statistical significance 2\n\n\n\n\n\n\n\nA common definition\n\n\n\nStatistical significance is conventionally defined as a threshold (commonly, a \\(p\\)-value less than 0.05) relative to some null hypothesis or prespecified value that indicates no effect is present.\nE.g., an estimate may be considered “statistically significant at \\(P &lt; 0.05\\)” if it:\n\nlies at least two standard errors from the mean\nis a difference that lies at least two standard errors from zero\n\nMore generally, an estimate is “not statistically significant” if, e.g.\n\nthe observed value can reasonably be explained by chance variation\nit is a difference that lies less than two standard errors from zero\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost tests rely on probability distributions\n\n\n\nWe need to relate the measured values in the real world to an appropriate distribution that approximates them\n\n\n\n\n\n\n\nThe way you’re likely to see statistical significance presented is that some threshold - usually a p-value of less than 0.05 - is applied in a statistical test, relative to some null hypothesis or prespecified value that indicates the absence of an effect\nSo you are likely to see an estimate be considered “statistically significant at P&lt;0.05” if it lies at least two SEs from the mean, or is a difference that lies at least two SEs from zero\nConversely, you will see estimates be considered “not statistically significant at P&lt;0.05” if they lie less than two SEs from the mean, or the observed value can reasonably be explained by chance variation alone\nAs you might have guessed from the talk of standard errors and distances from means, we map our measured values in the real world to statistical probability distributions to calculate these values."
  },
  {
    "objectID": "index.html#a-simple-example-the-experiment",
    "href": "index.html#a-simple-example-the-experiment",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The experiment",
    "text": "A simple example: The experiment\n\n\n\n\n\n\n\nThe experiment\n\n\n\nTwo drugs, \\(C\\) and \\(T\\) lower cholesterol1, and we want to compare their effectiveness\nWe randomise assignment of \\(C\\) and \\(T\\) to members of a single cohort of comparable individuals, whose pre-treatment cholesterol level is assumed to be drawn from the same distribution (i.e. be approximately the same)\nWe measure the post-treatment cholesterol levels \\(y_T\\) and \\(y_C\\) for each individual in the two groups.\nWe calculate the average measured \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\nWe also calculate standard deviation for the two groups, \\(\\sigma_T\\) and \\(\\sigma_C\\)\n\n\n\n\n\n\n\n\nLet’s consider a simple experiment\nWe want to compare the efficacy of two drugs, C and T, in their ability to lower cholesterol\n\nWe assume a uniform pool of individuals, and randomly assign C and T to two equally-sized groups drawn from that cohort\nThis implies that the starting cholesterol level of all individuals is drawn from the same distribution\n\nWe administer the drugs in the same way, for the same period of time, and measure the post-treatment cholesterol level in each individual\n\nWe calculate the mean and standard deviation for the two groups\nThese allow us to estimate the underlying normal distribution of post-treatment cholesterol levels in each group\n\nWe can use the estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\n\n\\(C\\) for control, the current best-in class; \\(T\\) for treatment, the new compound"
  },
  {
    "objectID": "index.html#a-simple-example-the-hypotheses",
    "href": "index.html#a-simple-example-the-hypotheses",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The hypotheses",
    "text": "A simple example: The hypotheses\n\nWe want to know if the treatments have different sizes of effect\n\nIf they do, there should be a difference between the (average) post-treatment cholesterol level in each group\nThe true post-treatment levels are \\(\\theta_T\\) and \\(\\theta_C\\)\nWe have estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for post-treatment levels\n\n\n\n\n\n\n\n\n\nThe hypotheses\n\n\n\nWe are interested in \\(\\theta = \\theta_T - \\theta_C\\), the expected post-test difference in cholesterol between the two groups \\(T\\) and \\(C\\).\nOur null hypothesis (\\(H_0\\)) is that \\(\\theta = 0\\), i.e. there is no difference (\\(\\theta_C = \\theta_T\\))\nOur alternative hypothesis (\\(H_1\\)) is that there is a difference, so \\(\\theta \\neq 0\\), (i.e. \\(\\theta_C \\neq \\theta_T\\))\n\n\n\n\n\n\n\nOur goal in this experiment is to determine whether the treatments have different effects on lowering cholesterol\n\nIf they do, then there should be a difference between the (average) post-treatment cholesterol levels of groups T and C\n\nThere are true post treatment levels for each group: \\(\\theta_T\\) and \\(\\theta_C\\)\n\nBut we have only estimated them as \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\)\nSo we turn to a hypothesis test to determine how likely it is our estimates support there being a difference in the true levels between the two groups\n\nWe set up a null hypothesis that there is no difference between the groups: \\(\\theta = 0\\)\n\nIn this hypothesis, any deviation from zero we see is considered the result of chance variation in the data, or error we cannot eliminate from measurements\n\nWe also define an alternative hypothesis that there is a difference between the groups: \\(\\theta \\neq 0\\)\n\nWe’re not insisting that \\(\\theta\\) is greater than or less than zero, just that it’s not zero\nAnd also that any deviation from zero is not accounted for by the null hypothesis"
  },
  {
    "objectID": "index.html#a-simple-example-the-distribution-1",
    "href": "index.html#a-simple-example-the-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The distribution 1",
    "text": "A simple example: The distribution 1\n\nTo perform a statistical test, we may assume a distribution and parameters for the null hypothesis\n\nWe can then test the observed estimate against that distribution to see how likely it is that the null hypothesis would have generated it\n\n\n\n\n\n\n\n\n\nThe distribution\n\n\n\nWe use a probability distribution reflecting generation of the null hypothesis: \\(\\theta_C = \\theta_T\\)\n\nThis allows us to define a test statistic \\(T\\) (i.e. a threshold probability of “significance”) in advance\n\nWe test the estimated value from the experiment (\\(\\bar{y}_T - \\bar{y}_C\\)) to calculate a \\(p\\)-value for our estimate: \\(p = \\textrm{Pr}(T(y^{\\textrm{null}}) &gt; T(\\bar{y}_T - \\bar{y}_C))\\)\n\n\n\n\n\n\n\nAt this point we haven’t defined what our null hypothesis distribution should be, but we need to do so\n\nOnce we have this, we can test our observed estimate of \\(\\hat{\\theta} = \\bar{y}_T - \\bar{y}_C\\) against the distribution to see how likely it is to be explained by the null hypothesis\n\nWe need to choose a probability distribution that reflects the process generating the null hypothesis\n\nIn this case, we could shoose a normal distribution, which is typically appropriate for this kind of measurement\n\nThen we can define a test statistic \\(T\\) in advance\n\n\\(T\\) represents a threshold value in the null distribution - one where we would consider a larger difference to be meaningful\nYou may be familiar with setting P&lt;0.05, where the probability of seeing some value in the null distribution is 0.05 or less, and we might choose to set a value of \\(T\\) that corresponds to this\n\nWe then calculate the test statistic corresponding to our actual estimate, and calculate the \\(p\\)-value\n\nThis is the probability of observing our estimate, or a more extreme value, under the assumptions of the null model distribution"
  },
  {
    "objectID": "index.html#a-simple-example-the-null-hypothesis",
    "href": "index.html#a-simple-example-the-null-hypothesis",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The null hypothesis",
    "text": "A simple example: The null hypothesis\n\n\n\n\n\n\n\nThe null hypothesis\n\n\n\nAssume that the true difference \\(\\theta\\) is normally-distributed with \\(\\mu_\\theta=0\\), \\(\\sigma_\\theta=1\\)\n\n\n\n\n\n\n\n\nSuppose that the true difference between means is zero units, with standard error one unit, and that it is normally distributed\nThe probability distribution would look like this"
  },
  {
    "objectID": "index.html#a-simple-example-the-estimated-difference",
    "href": "index.html#a-simple-example-the-estimated-difference",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: The estimated difference",
    "text": "A simple example: The estimated difference\n\n\n\n\n\n\n\nObserved between post-treatment levels: \\(\\bar{y}_T - \\bar{y}_C = -1.4\\)\n\n\n\nIs this an unlikely outcome given the null hypothesis?\n\n\n\n\n\n\n\n\nSuppose that we measured the post treatment levels in each group, and found the difference between the means to be -1.4 units\n\nThat is, the \\(T\\) drug group had 1.4 units lower cholesterol than the \\(C\\) group\n\nIs that unlikely, given the null hypothesis?\nWell, we don’t know if it’s unlikely because we haven’t decided what “unlikely” means\n\nTo do that we need to set a significance threshold"
  },
  {
    "objectID": "index.html#a-simple-example-a-significance-threshold",
    "href": "index.html#a-simple-example-a-significance-threshold",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: A significance threshold",
    "text": "A simple example: A significance threshold\n\n\n\n\n\n\n\nWe choose a significance threshold in advance\n\n\n\nSuppose we set a threshold \\(T\\) corresponding to the 90% confidence interval (i.e. \\(P&lt;0.1\\))\n\nIf the estimate is not in the central 90% of the distribution, we’ll say it’s “significant”\n\n\n\n\n\n\n\n\n\nWe must always decide what our significance threshold is in advance\nSuppose that we decide the central 90% of the distribution/null hypothesis is “not significant”\n\nThis is the 90% confidence interval around the mean\nWe can shade this in to see it better"
  },
  {
    "objectID": "index.html#a-simple-example-compare-the-estimate",
    "href": "index.html#a-simple-example-compare-the-estimate",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Compare the estimate",
    "text": "A simple example: Compare the estimate\n\n\n\n\n\n\n\nCompare the estimate to the threshold\n\n\n\nThe estimate lies outwith the threshold, so we call the difference “significant”\n\n\n\n\n\n\n\n\nWe can overlay the observed difference on the distribution here, and see that the estimate lies outside the 90% confidence interval\n\nWith our stated assumption of a \\(P &lt; 0.1\\) significance threshold, we would call this difference “significant”"
  },
  {
    "objectID": "index.html#a-simple-example-another-threshold",
    "href": "index.html#a-simple-example-another-threshold",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Another threshold",
    "text": "A simple example: Another threshold\n\n\n\n\n\n\n\nWe choose a significance threshold in advance\n\n\n\nSuppose we set the threshold \\(T\\) corresponding to the 95% confidence interval (i.e. \\(P&lt;0.05\\)) instead?\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "index.html#a-simple-example-another-outcome",
    "href": "index.html#a-simple-example-another-outcome",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: Another outcome",
    "text": "A simple example: Another outcome\n\n\n\n\n\n\n\nCompare the estimate to the threshold\n\n\n\nThe estimate lies within the threshold, so the difference is “not significant”\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "index.html#a-simple-example-what-changed",
    "href": "index.html#a-simple-example-what-changed",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example: What changed?",
    "text": "A simple example: What changed?\n\n\n\n\n\n\n\nWhat did not change\n\n\n\nThe null hypothesis was the same\nThe observed estimate of difference was the same\n\n\n\n\n\n\n\n\n\n\n\n\nWhat changed\n\n\n\nOur choice of significance threshold changed\n\n\n\n\n\n\n\n\n\n\n\n\nSignificance threshold choice\n\n\n\nOnce the estimate is known, it is always possible to find a threshold that makes it “significant” or “not significant”\nIt is dishonest to select a threshold deliberately to make your result “significant” or “not significant”\nAlways choose and record (preregister) your threshold for significance ahead of the experiment"
  },
  {
    "objectID": "index.html#tailed-tests-two-tailed",
    "href": "index.html#tailed-tests-two-tailed",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: two-tailed",
    "text": "Tailed tests: two-tailed\n\n\n\n\n\n\n\nUse two tails if direction of change doesn’t matter\n\n\n\nWith a two-tailed hypothesis test, we do not care which direction of change is significant\n\n\n\n\n\n\n\n\nThe example we’ve been looking at is a two-tailed test\nThis means that we’re concerned with whether the estimate lies within the central mass of the null distribution\n\nThe central 95% of the distribution has 2.5% of the distribution to the left, and 2.5% of the distribution to the right\nIf the estimate lies in either of those extreme regions, it’s in the “extreme 5%” from the mean, and we call it “significant”\nAny estimate lying within the central 95% of the distribution is “not significant”"
  },
  {
    "objectID": "index.html#tailed-tests-one-tailed-left",
    "href": "index.html#tailed-tests-one-tailed-left",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: one-tailed (left)",
    "text": "Tailed tests: one-tailed (left)\n\n\n\n\n\n\n\nUse one-tailed tests when direction matters\n\n\n\nIf we’re testing specifically for a significant negative difference/reduction, use a left-tailed test\ne.g. if we wanted to know if \\(T\\) reduced post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\n\n\n\n\nWhen we know we want to test for change in a specific direction, we should use a one-tailed test\nWhen checking specifically for a negative change where \\(P &lt; 0.05\\), say that \\(T\\) reduces cholesterol level by more than \\(C\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the right-most 95% of the distribution mass\n\nThis means that results in the left-most 5% of the mass are “significant”\nAny estimate lying in the right-most 95% of the distribution is “not significant”"
  },
  {
    "objectID": "index.html#tailed-tests-one-tailed-right",
    "href": "index.html#tailed-tests-one-tailed-right",
    "title": "MP968 Experimental Design Workshop",
    "section": "Tailed tests: one-tailed (right)",
    "text": "Tailed tests: one-tailed (right)\n\n\n\n\n\n\n\nUse one-tailed tests when direction matters\n\n\n\nIf we’re testing specifically for a positive difference/increase, use a right-tailed test\ne.g. if we wanted to know if \\(T\\) increased post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\n\n\n\n\nSimilarly, if checking specifically for a postitive change where \\(P &lt; 0.05\\), say that \\(C\\) reduces cholesterol level by more than \\(T\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the left-most 95% of the distribution mass\n\nThis means that results in the right-most 5% of the mass are “significant”\nAny estimate lying in the left-most 95% of the distribution is “not significant”\n\nIt’s possible to dishonestly switch a result between “significant” and “not significant” by choosing to use a one-tailed or two-tailed test"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-1",
    "href": "index.html#problems-with-statistical-significance-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 1",
    "text": "Problems with statistical significance 1\n\n\n\n\n\n\nWarning\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\n\n\nStatistical significance is not the same as practical importance\n\n\n\nSuppose a treatment increased earnings by £10 per year with a standard error of £2 (average salary £25,000).\n\nThis would be statistically, but not practically, significant\n\nSuppose a different treatment increased earnings by £10,000 per year with a standard error of £10,000\n\nThis would not be statistically significant, but could be important in practice"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-2",
    "href": "index.html#problems-with-statistical-significance-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 2",
    "text": "Problems with statistical significance 2\n\n\n\n\n\n\nWarning\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\n\n\nNon-significance is not the same as zero\n\n\n\nSuppose an arterial stent treatment group outperforms the control\n\nmean difference in treadmill time: 16.6s (standard error 9.8)\nthe 95% confidence interval for the effect includes zero, \\(p ≈ 0.20\\)\n\nIt’s not clear whether the net treatment effect is positive or negative\n\nbut we can’t say that stents have no effect"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-3",
    "href": "index.html#problems-with-statistical-significance-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 3",
    "text": "Problems with statistical significance 3\n\n\n\n\n\n\n\nThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\nAt a \\(P&lt;0.05\\) threshold, only a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\n\n\n\n\nIt should hopefully be obvious that any significance threshold is arbitrary, and the difference between a value being on one side of the threshold or the other may be arbitrarily small\nLess obviously, consider two independent studies - orange and purple here\n\nThe purple study estimates an effect size of 25 with standard error of 10 units and is “statistically significant” in that it does not include zero at the 1% significance level\nThe orange study estimates an effect size of 10 with standard error of 10 units and is “statistically not significant” as it includes zero even at the 10% significance level"
  },
  {
    "objectID": "index.html#problems-with-statistical-significance-4",
    "href": "index.html#problems-with-statistical-significance-4",
    "title": "MP968 Experimental Design Workshop",
    "section": "Problems with statistical significance 4",
    "text": "Problems with statistical significance 4\n\n\n\n\n\n\n\nThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\nAt a \\(P&lt;0.05\\) threshold, only a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\n\n\n\n\nHowever, the effect sizes estimated by the two studies are not different from each other at a “statistical significance” of 95%\n\nIn both cases, the central mass of each distribution includes the mean of the other"
  },
  {
    "objectID": "index.html#standard-errors",
    "href": "index.html#standard-errors",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard errors",
    "text": "Standard errors\n\n\n\n\n\n\nImportant\n\n\nWe cannot make an infinite number of measurements of \\(z\\). We can only take a sample.\nThe mean and standard deviation we estimate in an experiment will not match those of the infinitely large population.\n\n\n\n\n\n\n\n\n\n\nStandard Error (of the Mean)\n\n\nThe standard error of the mean reflects the uncertainty in our estimate of the mean.\nWhen estimating the mean of an infinite population, given a simple random sample of size \\(n\\), the standard error is:\n\\[ \\textrm{standard error} = \\sqrt{\\frac{\\textrm{Variance}}{n}} = \\frac{\\textrm{standard deviation}}{\\sqrt{n}} = \\frac{\\sigma}{\\sqrt{n}} \\]\n\n\n\n\n\n\nWhen we perform an experiment we measure a limited number of values\nFor statistical analysis, we consider that these values are randomly drawn from a random distribution containing an infinite number of values\nThe mean, standard deviation, and other values we measure in our experiment will not match the true mean, standard deviation, or whatever of the full infinite distribution\nWe can, though, estimate our confidence in our estimate of the mean by the standard error of the mean, which reflects the uncertainty in our estimate of the true mean of the infinite distribution.\n\nThe standard error is the square root of the variance divided by the size of the sample\nThis assumes that our sample is randomly selected from the population and all samples are independent - the more you violate those assumptions, the less accurate this estimate is\nSince standard deviation is the square root of the variance, we can write this as:\n\nThe standard error of the mean is the standard deviation divided by the square root of sample size"
  },
  {
    "objectID": "index.html#standard-error-and-sample-size",
    "href": "index.html#standard-error-and-sample-size",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard error and sample size",
    "text": "Standard error and sample size\n\n\n\n\n\n\nTip\n\n\nUncertainty in the mean estimate \\(\\mu\\) reduces proportionally to the square root of the number of samples, \\(n\\)\n\n\n\n\n\n\nTo visualise this, we can plot the distribution of our uncertainty in the estimate of the mean, where we’ve estimated that the mean is zero and the standard deviation is one, for a range of sample sizes, n\nAt \\(n=3\\), the uncertainty in our estimate is quite flat and broad\nAs we increase the number of samples \\(n\\), the uncertainty narrows, and the density of the distribution of estimates starts to gather more around the central value of our estimate of the mean.\nThis property holds regardless of our assumptions about the shape of the sampling distribution\n\nBut it only reflects our confidence in the estimate of the mean, not the shape of the underlying distribution!\nThe standard error is less informative about our distribution as the sampling distribution deviates more from a normal distribution"
  },
  {
    "objectID": "index.html#standard-error-and-hypothesis-testing-1",
    "href": "index.html#standard-error-and-hypothesis-testing-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard error and hypothesis testing 1",
    "text": "Standard error and hypothesis testing 1\n\n\n\n\n\n\n\nHypothesis test statistics\n\n\n\nTest statistic \\(t\\) is a point on the distribution representing a significance threshold\n\n\\[ t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}} \\]\n\n\\(Z\\) is some function of the data (difference between estimate and true value); \\(s\\) is standard error of the mean\n\n\n\n\n\n\n\n\n\n\n\n\nThis is true for many hypothesis test methods\n\n\n\n\n\n\n\n\n\nIn most statistical hypothesis tests, we define a test statistic denoting a significance threshold\n\nOften, if the calculated value for our sample lies within that test statistic, we accept the null hypothesis\n\nThe statistic is typically defined as \\(t = \\frac{Z}{s}\\) where \\(t\\) is the test statistic value, \\(Z\\) is some quantity calculated from the data, and \\(s\\) is the standard error of the mean, \\(\\sigma/{\\sqrt{n}}\\)"
  },
  {
    "objectID": "index.html#standard-error-and-hypothesis-testing-2",
    "href": "index.html#standard-error-and-hypothesis-testing-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard error and hypothesis testing 2",
    "text": "Standard error and hypothesis testing 2\n\n\n\n\n\n\n\nOne-sample \\(t\\)-test\n\n\n\\[ t = \\frac{Z}{s} = \\frac{\\bar{X} - \\mu}{\\hat{\\sigma}/{\\sqrt{n}}} = \\frac{\\bar{X} - \\mu}{s(\\bar{X})} \\]\n\n\\(\\bar{X}\\) is the sample mean; \\(\\mu\\) is the hypothesised population mean (being tested)\n\\(\\hat{\\sigma}\\) is the sample standard deviation; \\(n\\) is the sample size; \\(s(\\bar{X})\\) is the standard error of the mean\n\n\n\n\n\n\n\n\n\n\n\n\nWald test\n\n\n\\[ \\sqrt{W} = \\frac{Z}{s} = \\frac{\\hat{\\theta} - \\theta_0}{s(\\hat{\\theta})} \\]\n\n\\(\\hat{\\theta}\\) is the estimated maximising argument of the likelihood function; \\(\\theta_0\\) is the hypothesised value under test\n\\(s(\\hat{\\theta})\\) is the standard error of \\(\\hat{\\theta}\\)\n\n\n\n\n\n\n\nYou may have come across the t-test before?\nThe t-statistic \\(t\\) is calculated as \\(Z/s\\) where \\(Z\\) is the difference between the estimated mean and the hypothesised population mean (the thing you are testing for equality to), and \\(s\\) is the standard error of the estimate of the mean.\nYou’ll likely not have come across the Wald test?\nBut it follows the same form\n\nHere \\(\\hat{\\theta} - \\theta_0\\) is the difference between an estimated most likely value and the hypothesised most likely value\n\\(s\\) is the standard error of the estimate of the most likely value\n\nThe standard error is influenced by (gets smaller with) the square root of sample size in both cases"
  },
  {
    "objectID": "index.html#standard-error-and-hypothesis-testing-3",
    "href": "index.html#standard-error-and-hypothesis-testing-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard error and hypothesis testing 3",
    "text": "Standard error and hypothesis testing 3\n\n\n\n\n\n\n\nHypothesis test statistics\n\n\n\nTest statistic \\(t\\) is a point on the distribution representing a significance threshold\n\n\\[ t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}} \\]\n\n\\(Z\\) is some function of the data (difference between estimate and true value); \\(s\\) is standard error of the mean\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens if we hold \\(Z\\) and \\(\\sigma\\) constant and vary sample size?\n\n\n\n\n\n\n\n\n\nSo what happens if we hold \\(Z\\) and \\(\\sigma\\) constant (i.e. we estimate the same difference from our hypothesised mean value), but vary the number of samples?\nSo as \\(n\\) increases, the standard error \\(\\sigma/{\\sqrt{n}}\\) gets smaller\n\\(Z\\) however remains the same size, so the \\(t\\) statistic gets larger (for the same value of \\(Z\\))."
  },
  {
    "objectID": "index.html#sample-size-and-hypothesis-testing-1",
    "href": "index.html#sample-size-and-hypothesis-testing-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Sample size and hypothesis testing 1",
    "text": "Sample size and hypothesis testing 1\n\n\n\n\n\n\n\nWe reject the null hypothesis when $t &gt; \\(t_\\textrm{crit}\\)\n\n\n\nSuppose we set \\(t_\\textrm{crit} = 2\\) (and \\(\\sigma=1\\), \\(n=3\\))\nWe can calculate \\(t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}}\\) for any value of \\(Z\\)\n\n\n\n\n\n\n\n\nSo suppose we set a confidence interval that gives us a test statistic: t-critical=2\n\nSo any difference \\(Z\\) between our estimated mean and the test value giving a value of t less than 2 is “not significant” and we accept the null hypothesis\nAny difference \\(Z\\) between our estimated mean and the test value giving a value of t greater than 2 is “significant” and we reject the null hypothesis\n\nThe plot now shows the relationship between \\(Z\\), our measured difference from the hypothesised mean, and the value of the resulting test statistic \\(t\\) for a sample size of three measurements\nWith three measurements, we need a Z value of about 1.15 or greater for the line to cross the critical value and for us to reject the null hypothesis\n\nIf we saw a Z value of less than 1.15, we would accept the null hypothesis\nIf we saw a Z value of greater than 1.15, we would reject the null hypothesis"
  },
  {
    "objectID": "index.html#sample-size-and-hypothesis-testing-2",
    "href": "index.html#sample-size-and-hypothesis-testing-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Sample size and hypothesis testing 2",
    "text": "Sample size and hypothesis testing 2\n\n\n\n\n\n\n\nThe difference (\\(Z\\)) we need to see to reject the null varies with sample size\n\n\n\nSet \\(t_\\textrm{crit} = 2\\)\n\\(n=3 \\implies Z_\\textrm{crit} \\approx 1.15\\); \\(n=15 \\implies Z_\\textrm{crit} \\approx 0.5\\)\n\n\n\n\n\n\n\n\nThe plot shows the relationship between \\(Z\\), our measured difference from the hypothesised mean, and the value of the resulting test statistic \\(t\\) for a range of sample sizes from 3 to 15 measurements\nWith three measurements, we need a Z value of about 1.15 or greater for the line to cross the critical value and for us to reject the null hypothesis\nWith 15 measurements, we only need a Z value of about 0.5 to reject the null hypothesis\n\nFive times the sample size means we reject the null hypothesis at about half the size of measured difference\n\nChanging the sample size changes how sensitive our hypothesis test is"
  },
  {
    "objectID": "index.html#statistical-significance-and-effect-size",
    "href": "index.html#statistical-significance-and-effect-size",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance and effect size",
    "text": "Statistical significance and effect size\n\n\n\n\n\n\n\nStatistical significance is not the point\n\n\n\nWe can meet any statistical significance threshold for a difference by sufficiently increasing the sample size\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical significance is not the same as practical importance\n\n\n\nSuppose a treatment increased earnings by £10 per year with a standard error of £2 (average salary £25,000).\n\nThis would be statistically, but not practically, significant\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat matters is effect size\n\n\nIn all our experiments we should be concerned not with “statistical significance,” but with how likely it is that, if there is a meaningful effect of the treatment, our experiment will be able to detect it?\n\nWe need to be concerned with statistical power"
  },
  {
    "objectID": "index.html#statistical-power-1",
    "href": "index.html#statistical-power-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power",
    "text": "Statistical power\n\n\n\n\n\n\nImportant\n\n\nStatistical power is defined as the probability, before a study is performed, that a particular comparison will achieve “statistical significance” at some predetermined level (e.g. \\(P &lt; 0.05\\)) given an assumed true effect size.\n\n\n\n\n\n\n\n\n\n\nThe process\n\n\n\nHypothesise an appropriate effect size (e.g. what effect will improve health?)\nDetermine the \\(p\\)-value threshold you consider “statistically significant”\nMake reasoned assumptions about the variation in the data (e.g. what distribution? what variance?)\nChoose a sample size\nUse probability calculations to determine the chance that your observed \\(p\\)-value will be below the threshold (accept the null hypothesis) for the hypothesised effect size"
  },
  {
    "objectID": "index.html#effect-sizes",
    "href": "index.html#effect-sizes",
    "title": "MP968 Experimental Design Workshop",
    "section": "Effect sizes",
    "text": "Effect sizes\n\n\n\n\n\n\n\nPower analysis depends on an assumed effect size\n\n\n\nThe true effect size is almost never known ahead of time\n\nDetermining the effect size is usually why we’re doing the study\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to choose effect sizes\n\n\n\nTry a range of values consistent with relevant literature\nDetermine what value would be of practical interest (e.g. improvement in outcomes of 10%)\n\n\n\n\n\n\n\n\n\n\n\n\nHow not to choose effect size\n\n\n\nDO NOT USE AN ESTIMATE FROM A SINGLE NOISY STUDY!\n\nNoisy studies suffer from The Winner’s Curse"
  },
  {
    "objectID": "index.html#the-winners-curse-1",
    "href": "index.html#the-winners-curse-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "The Winner’s Curse 1",
    "text": "The Winner’s Curse 1\n\n\n\n\n\n\n\nA low-powered pilot study\n\n\n\nSuppose we ran a small pilot study with only a few individuals\nThe study, by design, has low statistical power\n\nThe variance of the data is relatively large, compared to the true effect size\n\n\n\n\n\n\n\n\nTo see what I mean, suppose that we ran a pilot study for a treatment where the true effect size was only a two percentage point increase in the outcome, but this effect is estimated with a standard error of about eight percentage points.\n\nWe test this against the null hypothesis that the effect size is zero\nWe set up the null hypothesis so that the result of the study is “positive”/“statistically significant” if the measured difference is more than two standard errors away from zero - i.e. outwith the orange shading of the curve\nEven though the actual true mean effect is 2%, the larger standard error means that there is the possibility of the measured result falling outside that central shaded area"
  },
  {
    "objectID": "index.html#the-winners-curse-2",
    "href": "index.html#the-winners-curse-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "The Winner’s Curse 2",
    "text": "The Winner’s Curse 2\n\n\n\n\n\n\n\nYou get a statistically significant result!\n\n\n\nYou think you won, but you lost! (The Winner’s Curse)\n\nThe estimate is either eight times too large (at least 16% instead of 2%) or\nThe estimate has the wrong sign (a negative change instead of positive)\n\n\n\n\n\n\n\n\n\nIn this low-powered experiment, any “statistically significant” result must fall into one of the dark purple sections of the curve at the extremes\n\nThe left-most 2.5% of outcomes or\nThe right-most 7% of outcomes\n\nThe problem is this:\n\n“Statistically significant” results on the right hand side overestimate the size of the true effect by at least eight times (minimum value 16%)\n“Statistically significant” results on the right hand side indicate that the effect is negative - a reduction in effect, when the true effect is small but positive"
  },
  {
    "objectID": "index.html#the-winners-curse-3",
    "href": "index.html#the-winners-curse-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "The Winner’s Curse 3",
    "text": "The Winner’s Curse 3\n\n\n\n\n\n\n\nThe trap\n\n\nAny apparent success of low-powered studies masks larger failure\nWhen signal (effect size) is low and noise (standard error) is high, “statistically significant” results are likely to be wrong.\nLow-power studies tend not to replicate well\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nLow-power studies have essentially no chance of providing useful information\nWe can say this even before data are collected\n\n\n\n\n\n\n\n\n\n\nPublished results tend to be overestimates"
  },
  {
    "objectID": "index.html#statistical-power-and-ethics",
    "href": "index.html#statistical-power-and-ethics",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and ethics",
    "text": "Statistical power and ethics\n\n\n\n\n\n\n\nIt is unethical to under-power animal studies\n\n\n\nUnder-powered in vivo experiments waste time and resources, lead to unnecessary animal suffering and result in erroneous biological conclusions (NC3Rs Experimental Design Assistant guide)\n\n\n\n\n\n\n\n\n\n\n\n\nIt is unethical to over-power animal studies\n\n\n\nEthically, when working with animals we need to conduct a harm–benefit analysis to ensure the animal use is justified for the scientific gain. Experiments should be robust, not use more or fewer animals than necessary, and truly add to the knowledge base of science (Karp and Fry (2021))\n\n\n\n\n\n\n\n\n\n\n\n\nSo how should we appropriately power animal studies?"
  },
  {
    "objectID": "index.html#statistical-power-and-error-1",
    "href": "index.html#statistical-power-and-error-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and error 1",
    "text": "Statistical power and error 1\nWe often refer to two kinds of statistical error\n\n\n\n\n\n\n\nType I Error (\\(\\alpha\\))\n\n\n\nType I error is the probability of rejecting a null hypothesis, when the null hypothesis is true\n\nAlso known as a “false positive error”\n\nRepresented by the Greek letter \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nType II Error (\\(\\beta\\))\n\n\n\nType I error is the probability of accepting a null hypothesis, when the null hypothesis is false\n\nAlso known as a “false negative error”\n\nRepresented by the Greek letter \\(\\beta\\)\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical power is \\(1 - \\beta\\)"
  },
  {
    "objectID": "index.html#statistical-power-and-error-2",
    "href": "index.html#statistical-power-and-error-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and error 2",
    "text": "Statistical power and error 2\nStatistical power needs context: the expected error rates of the experiment at a given effect size, e.g.\n\nThe experiment has 80% power at \\(\\alpha = 0.05\\) for an effect size of 2mM/L\n\n\n\n\n\n\n\n\nHow to read this\n\n\n\n“an effect size of 2mM/L”: we are aiming to detect an effect of at least 2mM/L (e.g. blood glucose concentration)\n“\\(\\alpha = 0.05\\)”: we are using a significance test threshold (\\(\\alpha\\), type I error rate) of \\(P &lt; 0.05\\)\n“80% power”: we expect the study to report a significant effect, where one truly exists, 80% of the time"
  },
  {
    "objectID": "index.html#statistical-power-and-error-3",
    "href": "index.html#statistical-power-and-error-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and error 3",
    "text": "Statistical power and error 3\n\nThe experiment has 80% power at \\(\\alpha = 0.05\\) for an effect size of 2mM/L\n\n\n\n\n\n\n\n\nIf the drug truly has no effect\n\n\n\nThe test has \\(\\alpha = 0.05\\), so we would expect to reject the null hypothesis incorrectly 5% of the time\nIf we ran the experiment 100 times, we would expect to see a result implying that the drug was effective five times\n\n\n\n\n\n\n\n\n\n\n\n\nIf the drug truly has an effect\n\n\n\nThe test has predicted power \\(1 - \\beta = 0.8\\), so the type II error rate \\(\\beta = 0.2\\) and we would expect to accept the null hypothesis incorrectly 20% of the time\nIf we ran the experiment 100 times, we would expect to see a result implying that the drug was effective eighty times"
  },
  {
    "objectID": "index.html#statistical-power-and-sample-size-1",
    "href": "index.html#statistical-power-and-sample-size-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and sample size 1",
    "text": "Statistical power and sample size 1\n\n\n\n\n\n\n\nWhat we need, to calculate appropriate sample size\n\n\n\nAn acceptable false positive rate (type I error, \\(\\alpha\\))\nAn acceptable false negative rate (type II error, \\(\\beta\\))\n\nThis is equivalent to knowing the target statistical power (\\(1 - \\beta\\))\n\nThe expected effect size and variance\nThe statistical test being performed\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWe need this information to calculate an appropriate, ethical sample size"
  },
  {
    "objectID": "index.html#statistical-power-and-sample-size-2",
    "href": "index.html#statistical-power-and-sample-size-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical power and sample size 2",
    "text": "Statistical power and sample size 2\n\n\n\n\n\n\n\nTypical funders’ requirements\n\n\n\nFalse positive rate \\(\\alpha = 0.05\\)\nPower \\(1 - \\beta = 0.8\\) (80% power)\nThese are only a starting point - other values may be more appropriate depending on circumstance\n\n\n\n\n\n\n\n\n\n\n\n\nUnder experimenter control\n\n\n\nEffect size and variance\nThe appropriate statistical approach"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "References",
    "text": "References\n\n\n\n\nBate, Simon T., and Robin A. Clark. 2014. The Design and Statistical Analysis of Animal Experiments. Cambridge University Press.\n\n\nKarp, Natasha A, and Derek Fry. 2021. “What Is the Optimum Design for My Animal Experiment?” BMJ Open Sci. 5 (1): e100126.\n\n\nKilkenny, Carol, William J Browne, Innes C Cuthill, Michael Emerson, and Douglas G Altman. 2010. “Improving Bioscience Research Reporting: The ARRIVE Guidelines for Reporting Animal Research.” PLoS Biol. 8 (6): e1000412.\n\n\nKilkenny, Carol, Nick Parsons, Ed Kadyszewski, Michael F W Festing, Innes C Cuthill, Derek Fry, Jane Hutton, and Douglas G Altman. 2009. “Survey of the Quality of Experimental Design, Statistical Analysis and Reporting of Research Using Animals.” PLoS One 4 (11): e7824."
  },
  {
    "objectID": "06-standard_errors.html",
    "href": "06-standard_errors.html",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2"
  },
  {
    "objectID": "06-standard_errors.html#standard-errors",
    "href": "06-standard_errors.html#standard-errors",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Standard errors",
    "text": "Standard errors\n\n\n\n\n\n\nImportant\n\n\n\nWe cannot make an infinite number of measurements of \\(z\\). We can only take a sample.\nThe mean and standard deviation we estimate in an experiment will not match those of the infinitely large population.\n\n\n\n\n\n\n\n\nTipStandard Error (of the Mean)\n\n\n\nThe standard error of the mean reflects the uncertainty in our estimate of the mean.\nWhen estimating the mean of an infinite population, given a simple random sample of size \\(n\\), the standard error is:\n\\[ \\textrm{standard error} = \\sqrt{\\frac{\\textrm{Variance}}{n}} = \\frac{\\textrm{standard deviation}}{\\sqrt{n}} = \\frac{\\sigma}{\\sqrt{n}} \\]\n\n\n\n\nWhen we perform an experiment we measure a limited number of values\nFor statistical analysis, we consider that these values are randomly drawn from a random distribution containing an infinite number of values\nThe mean, standard deviation, and other values we measure in our experiment will not match the true mean, standard deviation, or whatever of the full infinite distribution\nWe can, though, estimate our confidence in our estimate of the mean by the standard error of the mean, which reflects the uncertainty in our estimate of the true mean of the infinite distribution.\n\nThe standard error is the square root of the variance divided by the size of the sample\nThis assumes that our sample is randomly selected from the population and all samples are independent - the more you violate those assumptions, the less accurate this estimate is\nSince standard deviation is the square root of the variance, we can write this as:\n\nThe standard error of the mean is the standard deviation divided by the square root of sample size"
  },
  {
    "objectID": "06-standard_errors.html#standard-error-and-sample-size",
    "href": "06-standard_errors.html#standard-error-and-sample-size",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Standard error and sample size",
    "text": "Standard error and sample size\n\n\n\n\n\n\nTip\n\n\n\nUncertainty in the mean estimate \\(\\mu\\) reduces proportionally to the square root of the number of samples, \\(n\\)\n\n\n\n# Plots assuming mu = 0, sd = 1\n# So the progressive standard errors are:\n# (n=3, sd=1/3), (n=5, sd=1/5), (n=7, sd=1/7), (n=10, sd=1/10), (n=20, sd=1/20)\nggplot() +\n  shaded_normal(mu=0, sd=1/sqrt(3), zstart=0, zend=1, label=\"mu=0, sd=1, n=3\",\n                textyoffset = 1, fill=\"gold\", color=\"gold4\") +\n  shaded_normal(mu=0, sd=1/sqrt(5), zstart=0, zend=1, label=\"mu=0, sd=1, n=5\",\n                textyoffset = 1, fill=\"coral\", color=\"coral4\") +\n  shaded_normal(mu=0, sd=1/sqrt(7), zstart=0, zend=1, label=\"mu=0, sd=1, n=7\",\n                textyoffset = 1, fill=\"lightskyblue\", color=\"lightskyblue4\") +\n  shaded_normal(mu=0, sd=1/sqrt(10), zstart=0, zend=1, label=\"mu=0, sd=1, n=10\",\n                textyoffset = 1, fill=\"palegreen\", color=\"palegreen4\") +\n  shaded_normal(mu=0, sd=1/sqrt(20), zstart=0, zend=1, label=\"mu=0, sd=1, n=20\",\n                textyoffset = 1, fill=\"mediumpurple\", color=\"mediumpurple4\") +\n  xlab(\"uncertainty in the mean estimate\") + ylab(\"density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTo visualise this, we can plot the distribution of our uncertainty in the estimate of the mean, where we’ve estimated that the mean is zero and the standard deviation is one, for a range of sample sizes, n\nAt \\(n=3\\), the uncertainty in our estimate is quite flat and broad\nAs we increase the number of samples \\(n\\), the uncertainty narrows, and the density of the distribution of estimates starts to gather more around the central value of our estimate of the mean.\nThis property holds regardless of our assumptions about the shape of the sampling distribution\n\nBut it only reflects our confidence in the estimate of the mean, not the shape of the underlying distribution!\nThe standard error is less informative about our distribution as the sampling distribution deviates more from a normal distribution"
  },
  {
    "objectID": "06-standard_errors.html#standard-error-and-hypothesis-testing-1",
    "href": "06-standard_errors.html#standard-error-and-hypothesis-testing-1",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Standard error and hypothesis testing 1",
    "text": "Standard error and hypothesis testing 1\n\n\n\n\n\n\nImportantHypothesis test statistics\n\n\n\n\nTest statistic \\(t\\) is a point on the distribution representing a significance threshold\n\n\\[ t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}} \\]\n\n\\(Z\\) is some function of the data (difference between estimate and true value); \\(s\\) is standard error of the mean\n\n\n\n\n\n\n\n\n\nWarningThis is true for many hypothesis test methods\n\n\n\n\n\n\n\n\nIn most statistical hypothesis tests, we define a test statistic denoting a significance threshold\n\nOften, if the calculated value for our sample lies within that test statistic, we accept the null hypothesis\n\nThe statistic is typically defined as \\(t = \\frac{Z}{s}\\) where \\(t\\) is the test statistic value, \\(Z\\) is some quantity calculated from the data, and \\(s\\) is the standard error of the mean, \\(\\sigma/{\\sqrt{n}}\\)"
  },
  {
    "objectID": "06-standard_errors.html#standard-error-and-hypothesis-testing-2",
    "href": "06-standard_errors.html#standard-error-and-hypothesis-testing-2",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Standard error and hypothesis testing 2",
    "text": "Standard error and hypothesis testing 2\n\n\n\n\n\n\nTipOne-sample \\(t\\)-test\n\n\n\n\\[ t = \\frac{Z}{s} = \\frac{\\bar{X} - \\mu}{\\hat{\\sigma}/{\\sqrt{n}}} = \\frac{\\bar{X} - \\mu}{s(\\bar{X})} \\]\n\n\\(\\bar{X}\\) is the sample mean; \\(\\mu\\) is the hypothesised population mean (being tested)\n\\(\\hat{\\sigma}\\) is the sample standard deviation; \\(n\\) is the sample size; \\(s(\\bar{X})\\) is the standard error of the mean\n\n\n\n\n\n\n\n\n\nNoteWald test\n\n\n\n\\[ \\sqrt{W} = \\frac{Z}{s} = \\frac{\\hat{\\theta} - \\theta_0}{s(\\hat{\\theta})} \\]\n\n\\(\\hat{\\theta}\\) is the estimated maximising argument of the likelihood function; \\(\\theta_0\\) is the hypothesised value under test\n\\(s(\\hat{\\theta})\\) is the standard error of \\(\\hat{\\theta}\\)\n\n\n\n\n\nYou may have come across the t-test before?\nThe t-statistic \\(t\\) is calculated as \\(Z/s\\) where \\(Z\\) is the difference between the estimated mean and the hypothesised population mean (the thing you are testing for equality to), and \\(s\\) is the standard error of the estimate of the mean.\nYou’ll likely not have come across the Wald test?\nBut it follows the same form\n\nHere \\(\\hat{\\theta} - \\theta_0\\) is the difference between an estimated most likely value and the hypothesised most likely value\n\\(s\\) is the standard error of the estimate of the most likely value\n\nThe standard error is influenced by (gets smaller with) the square root of sample size in both cases"
  },
  {
    "objectID": "06-standard_errors.html#standard-error-and-hypothesis-testing-3",
    "href": "06-standard_errors.html#standard-error-and-hypothesis-testing-3",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Standard error and hypothesis testing 3",
    "text": "Standard error and hypothesis testing 3\n\n\n\n\n\n\nImportantHypothesis test statistics\n\n\n\n\nTest statistic \\(t\\) is a point on the distribution representing a significance threshold\n\n\\[ t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}} \\]\n\n\\(Z\\) is some function of the data (difference between estimate and true value); \\(s\\) is standard error of the mean\n\n\n\n\n\n\n\n\n\nWarningWhat happens if we hold \\(Z\\) and \\(\\sigma\\) constant and vary sample size?\n\n\n\n\n\n\n\n\nSo what happens if we hold \\(Z\\) and \\(\\sigma\\) constant (i.e. we estimate the same difference from our hypothesised mean value), but vary the number of samples?\nSo as \\(n\\) increases, the standard error \\(\\sigma/{\\sqrt{n}}\\) gets smaller\n\\(Z\\) however remains the same size, so the \\(t\\) statistic gets larger (for the same value of \\(Z\\))."
  },
  {
    "objectID": "06-standard_errors.html#sample-size-and-hypothesis-testing-1",
    "href": "06-standard_errors.html#sample-size-and-hypothesis-testing-1",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Sample size and hypothesis testing 1",
    "text": "Sample size and hypothesis testing 1\n\n\n\n\n\n\nTipWe reject the null hypothesis when $t &gt; \\(t_\\textrm{crit}\\)\n\n\n\n\nSuppose we set \\(t_\\textrm{crit} = 2\\) (and \\(\\sigma=1\\), \\(n=3\\))\nWe can calculate \\(t = \\frac{Z}{s} = \\frac{Z}{\\sigma/\\sqrt{n}}\\) for any value of \\(Z\\)\n\n\n\n\n# Assuming sd=1\n# z is the difference between the estimate and the hypothesised mean\n# The t-values correspond to: (t1, 3), (t2, 5), (t3, 7), (t4, 10), (t5, 15)\ndfm &lt;- data.frame(z=seq(0.01, 2, by=0.1)) %&gt;%\n  mutate(t1=z/(1/sqrt(3))) %&gt;%\n  mutate(t2=z/(1/sqrt(5))) %&gt;%\n  mutate(t3=z/(1/sqrt(7))) %&gt;%\n  mutate(t4=z/(1/sqrt(10))) %&gt;%\n  mutate(t5=z/(1/sqrt(15))) %&gt;%\n  pivot_longer(cols=c(t1, t2, t3, t4, t5),\n               names_to=\"sample_size\",\n               values_to=\"t_statistic\")\n\nggplot(dfm %&gt;% filter(sample_size==\"t1\"), aes(x=z, y=t_statistic, color=sample_size)) +\n  annotate(\"rect\", xmin=0, xmax=2, ymin=0, ymax=2, fill=\"orange\", alpha=0.2) +\n  annotate(\"segment\", x=0, xend=2, y=2, yend=2, colour=\"orange\", lty=\"dashed\", linewidth=1.5) +\n  annotate(\"text\", x=1.75, y=2.15, label=\"t-critical=2\", color=\"orange\") + \n  geom_line(size=1.25) +\n  annotate(\"text\", x=1.5, y=1, label=\"ACCEPT NULL HYPOTHESIS\", color=\"orange\") + \n  annotate(\"text\", x=1.5, y=3, label=\"REJECT NULL HYPOTHESIS\", color=\"purple\") + \n  xlab(\"Z = (estimate - hypothesised mean)\") +\n  ylab(\"calculated t value\") +\n  xlim(0, 2) +\n  ylim(0, 4) +\n  scale_color_discrete(labels=c(\"n=3\", \"n=5\", \"n=7\", \"n=10\", \"n=15\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSo suppose we set a confidence interval that gives us a test statistic: t-critical=2\n\nSo any difference \\(Z\\) between our estimated mean and the test value giving a value of t less than 2 is “not significant” and we accept the null hypothesis\nAny difference \\(Z\\) between our estimated mean and the test value giving a value of t greater than 2 is “significant” and we reject the null hypothesis\n\nThe plot now shows the relationship between \\(Z\\), our measured difference from the hypothesised mean, and the value of the resulting test statistic \\(t\\) for a sample size of three measurements\nWith three measurements, we need a Z value of about 1.15 or greater for the line to cross the critical value and for us to reject the null hypothesis\n\nIf we saw a Z value of less than 1.15, we would accept the null hypothesis\nIf we saw a Z value of greater than 1.15, we would reject the null hypothesis"
  },
  {
    "objectID": "06-standard_errors.html#sample-size-and-hypothesis-testing-2",
    "href": "06-standard_errors.html#sample-size-and-hypothesis-testing-2",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Sample size and hypothesis testing 2",
    "text": "Sample size and hypothesis testing 2\n\n\n\n\n\n\nTipThe difference (\\(Z\\)) we need to see to reject the null varies with sample size\n\n\n\n\nSet \\(t_\\textrm{crit} = 2\\)\n\\(n=3 \\implies Z_\\textrm{crit} \\approx 1.15\\); \\(n=15 \\implies Z_\\textrm{crit} \\approx 0.5\\)\n\n\n\n\n# Add the other curves\nggplot(dfm, aes(x=z, y=t_statistic, color=sample_size)) +\n  annotate(\"rect\", xmin=0, xmax=2, ymin=0, ymax=2, fill=\"orange\", alpha=0.2) +\n  annotate(\"segment\", x=0, xend=2, y=2, yend=2, colour=\"orange\", lty=\"dashed\", linewidth=1.5) +\n  annotate(\"text\", x=1.75, y=2.15, label=\"t-critical=2\", color=\"orange\") + \n  geom_line(size=1.25) +\n  annotate(\"text\", x=1.5, y=1, label=\"ACCEPT NULL HYPOTHESIS\", color=\"orange\") + \n  annotate(\"text\", x=1.5, y=3, label=\"REJECT NULL HYPOTHESIS\", color=\"purple\") + \n  xlab(\"Z = (estimate - hypothesised mean)\") +\n  ylab(\"calculated t value\") +\n  ylim(0, 4) +\n  scale_color_discrete(labels=c(\"n=3\", \"n=5\", \"n=7\", \"n=10\", \"n=15\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThe plot shows the relationship between \\(Z\\), our measured difference from the hypothesised mean, and the value of the resulting test statistic \\(t\\) for a range of sample sizes from 3 to 15 measurements\nWith three measurements, we need a Z value of about 1.15 or greater for the line to cross the critical value and for us to reject the null hypothesis\nWith 15 measurements, we only need a Z value of about 0.5 to reject the null hypothesis\n\nFive times the sample size means we reject the null hypothesis at about half the size of measured difference\n\nChanging the sample size changes how sensitive our hypothesis test is"
  },
  {
    "objectID": "06-standard_errors.html#statistical-significance-and-effect-size",
    "href": "06-standard_errors.html#statistical-significance-and-effect-size",
    "title": "Standard errors, sample size, and statistical significance",
    "section": "Statistical significance and effect size",
    "text": "Statistical significance and effect size\n\n\n\n\n\n\nImportantStatistical significance is not the point\n\n\n\n\nWe can meet any statistical significance threshold for a difference by sufficiently increasing the sample size\n\n\n\n\n\n\n\n\n\nWarningStatistical significance is not the same as practical importance\n\n\n\n\nSuppose a treatment increased earnings by £10 per year with a standard error of £2 (average salary £25,000).\n\nThis would be statistically, but not practically, significant\n\n\n\n\n\n\n\n\n\n\nTipWhat matters is effect size\n\n\n\nIn all our experiments we should be concerned not with “statistical significance,” but with how likely it is that, if there is a meaningful effect of the treatment, our experiment will be able to detect it?\n\nWe need to be concerned with statistical power"
  },
  {
    "objectID": "04-estimates.html",
    "href": "04-estimates.html",
    "title": "Estimates, standard errors, and confidence intervals",
    "section": "",
    "text": "We’re going to review some statistical terms, to get ready for discussing hypothesis tests\n\n\n\nParameters are unknown numbers that determine a statistical model\n\n\n\n\n\n\nTipA linear regression\n\n\n\n\\[ y_i = a + b x_i \\]\n\nParameters are:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\n\n\n\n\n\n\n\n\n\n\nWarningA normal distribution representing your data\n\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nParameters are: \\(\\mu_z\\) and \\(\\sigma\\)\n\n\n\n\n\nParameters are numbers that you don’t know in a statistical model\n\nYou estimate parameter values from the data you collect\n\nSo, if you have a linear regression and are estimating the intercept and gradient in the equation \\(y_i = a + b x_i\\)\n\nThe intercept \\(a\\), and gradient \\(b\\), are the parameters you are estimating\n\nYou may be representing the data you collect by a normal distribution\n\nThis is described by the parameters \\(\\mu_z\\) and \\(\\sigma\\), which you are estimating from your data\n\n\n\n\n\n\nAn estimand (or quantity of interest) is a value that we are interested in estimating\n\n\n\n\n\n\nTipA linear regression\n\n\n\n\\[ y_i = a + b x_i\\]\n\nWe want to estimate values for:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\npredicted outcomes at important values of \\(x_i\\)\n\n\nThese are all estimands, and estimates are represented using the “hat” symbol: \\(\\hat{a}\\), \\(\\hat{b}\\), etc.\n\n\n\n\n\n\n\n\nWarningA normal distribution representing your data\n\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nEstimands are: \\(\\mu_z\\) and \\(\\sigma\\)\n\nMaybe you want to determine the 95% confidence interval - this is also an estimand\n\n\n\n\n\n\n\n\nThe standard error is the estimated standard deviation of an estimate\n\nIt is a measure of our uncertainty about the quantity of interest\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStandard error gets smaller as sample size gets larger\n\nYou know more about the most likely value, the more data/information you collect\nStandard error tends to zero as sample size gets large enough\n\n\n\n\n\nThe confidence interval (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIn repeated applications, the 50% confidence interval will include the true value 50% of the time\n\nA 95% confidence interval will include the true value 95% of the time\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe usual 95% confidence interval rule of thumb for large samples (assuming a normal distribution) is to take the estimate \\(\\pm\\) two standard errors\n\n\n\n\n\nNow I’ve mentioned confidence intervals we’ll need to talk about them"
  },
  {
    "objectID": "04-estimates.html#parameters",
    "href": "04-estimates.html#parameters",
    "title": "Estimates, standard errors, and confidence intervals",
    "section": "",
    "text": "Parameters are unknown numbers that determine a statistical model\n\n\n\n\n\n\nTipA linear regression\n\n\n\n\\[ y_i = a + b x_i \\]\n\nParameters are:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\n\n\n\n\n\n\n\n\n\n\nWarningA normal distribution representing your data\n\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nParameters are: \\(\\mu_z\\) and \\(\\sigma\\)\n\n\n\n\n\nParameters are numbers that you don’t know in a statistical model\n\nYou estimate parameter values from the data you collect\n\nSo, if you have a linear regression and are estimating the intercept and gradient in the equation \\(y_i = a + b x_i\\)\n\nThe intercept \\(a\\), and gradient \\(b\\), are the parameters you are estimating\n\nYou may be representing the data you collect by a normal distribution\n\nThis is described by the parameters \\(\\mu_z\\) and \\(\\sigma\\), which you are estimating from your data"
  },
  {
    "objectID": "04-estimates.html#estimands",
    "href": "04-estimates.html#estimands",
    "title": "Estimates, standard errors, and confidence intervals",
    "section": "",
    "text": "An estimand (or quantity of interest) is a value that we are interested in estimating\n\n\n\n\n\n\nTipA linear regression\n\n\n\n\\[ y_i = a + b x_i\\]\n\nWe want to estimate values for:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\npredicted outcomes at important values of \\(x_i\\)\n\n\nThese are all estimands, and estimates are represented using the “hat” symbol: \\(\\hat{a}\\), \\(\\hat{b}\\), etc.\n\n\n\n\n\n\n\n\nWarningA normal distribution representing your data\n\n\n\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma) \\]\n\nEstimands are: \\(\\mu_z\\) and \\(\\sigma\\)\n\nMaybe you want to determine the 95% confidence interval - this is also an estimand"
  },
  {
    "objectID": "04-estimates.html#standard-errors-and-confidence-intervals",
    "href": "04-estimates.html#standard-errors-and-confidence-intervals",
    "title": "Estimates, standard errors, and confidence intervals",
    "section": "",
    "text": "The standard error is the estimated standard deviation of an estimate\n\nIt is a measure of our uncertainty about the quantity of interest\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStandard error gets smaller as sample size gets larger\n\nYou know more about the most likely value, the more data/information you collect\nStandard error tends to zero as sample size gets large enough\n\n\n\n\n\nThe confidence interval (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIn repeated applications, the 50% confidence interval will include the true value 50% of the time\n\nA 95% confidence interval will include the true value 95% of the time\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe usual 95% confidence interval rule of thumb for large samples (assuming a normal distribution) is to take the estimate \\(\\pm\\) two standard errors\n\n\n\n\n\nNow I’ve mentioned confidence intervals we’ll need to talk about them"
  },
  {
    "objectID": "02-nc3rs_arrive.html",
    "href": "02-nc3rs_arrive.html",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "“For scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\nImportantWe rely on the reporting of the experiment to know if it was appropriate\n\n\n\n\n\n\n\n\n\n\n\nThe National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.\nIn 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.\nIt did not make for pleasant reading.\n\n\n\n\n\n\n\n“Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. […] Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\n\n\nWarningWe cannot rely on the literature for good examples of experimental design\n\n\n\n\n\n\n\n\nThe state of the published literature around animal experiments was not good in 2009.\n\n40% of studies did not state the hypothesis or objective of the experiment\nMost papers did not use randomisation or blinding, although this is an essential practice to avoid bias\nOnly 70% of publications described their statistical methods at all\n\n\n\n\n\n\n\n\n\n\n\nImportantNo publication explained their choice for the number of animals used\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWe cannot rely on the verbal authority of ‘published scientists’ or ‘experienced scientists’ for good experimental design\n\n\n\n\n\n\n\n\nOne of the most shocking pieces of information is that, of the 48 papers surveyed, not a single one explained why they used the number of animals that they did.\nWe cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.\nThese papers are published. They are written, and the experiments conducted, by “experienced scientists”\nBeing an “experienced” or “published” scientist is clearly not a benchmark for good experimental design\n\n\n\n\n\n\n“Power analysis or other very simple calculations, which are widely used in human clinical trials and are often expected by regulatory authorities in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a scientifically robust and efficient way of determining animal numbers and may ultimately help to prevent animals being used unnecessarily. Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\nImportant\n\n\n\nAs scientists, you - yourselves - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.\nYou cannot simply rely on the word of “experienced scientists” for this.\n\n\n\n\nThe Kilkenny paper does propose solutions to this problem\nThey require the use and reporting of straightforward statistical calculations\nIt is up to you as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods.\n\n\n\n\n\n\n\nThe following year @Kilkenny2010-cp proposed the ARRIVE guidelines: a checklist to help researchers report their animal research transparently and reproducibly.\n\nGood reporting is essential for peer review and to inform future research\nReporting guidelines measurably improve reporting quality\nImproved reporting maximises the output of published research\n\n\n\n\n\n\n\n\nMany journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (@Kilkenny2010-cp):\n\n\n\n\n\n\nTipARRIVE guidelines (highlights)\n\n\n\n\n\nObjectives: primary and any secondary objectives of the study, or specific hypotheses being tested\n\n\nStudy design: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit\n\n\nSample size: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided\n\n\nStatistical methods: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach\n\n\nOutcomes and estimation: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n“A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues.”\n\n\n\n\n\n\n\n\nTipRecommended reading\n\n\n\n@Bate_Clark_2014"
  },
  {
    "objectID": "02-nc3rs_arrive.html#the-importance-of-experimental-design",
    "href": "02-nc3rs_arrive.html#the-importance-of-experimental-design",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "“For scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\nImportantWe rely on the reporting of the experiment to know if it was appropriate\n\n\n\n\n\n\n\n\n\n\n\nThe National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.\nIn 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.\nIt did not make for pleasant reading."
  },
  {
    "objectID": "02-nc3rs_arrive.html#causes-for-concern-1",
    "href": "02-nc3rs_arrive.html#causes-for-concern-1",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "“Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. […] Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\n\n\nWarningWe cannot rely on the literature for good examples of experimental design\n\n\n\n\n\n\n\n\nThe state of the published literature around animal experiments was not good in 2009.\n\n40% of studies did not state the hypothesis or objective of the experiment\nMost papers did not use randomisation or blinding, although this is an essential practice to avoid bias\nOnly 70% of publications described their statistical methods at all"
  },
  {
    "objectID": "02-nc3rs_arrive.html#causes-for-concern-2",
    "href": "02-nc3rs_arrive.html#causes-for-concern-2",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "ImportantNo publication explained their choice for the number of animals used\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWe cannot rely on the verbal authority of ‘published scientists’ or ‘experienced scientists’ for good experimental design\n\n\n\n\n\n\n\n\nOne of the most shocking pieces of information is that, of the 48 papers surveyed, not a single one explained why they used the number of animals that they did.\nWe cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.\nThese papers are published. They are written, and the experiments conducted, by “experienced scientists”\nBeing an “experienced” or “published” scientist is clearly not a benchmark for good experimental design"
  },
  {
    "objectID": "02-nc3rs_arrive.html#very-strong-cause-for-concern",
    "href": "02-nc3rs_arrive.html#very-strong-cause-for-concern",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "“Power analysis or other very simple calculations, which are widely used in human clinical trials and are often expected by regulatory authorities in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a scientifically robust and efficient way of determining animal numbers and may ultimately help to prevent animals being used unnecessarily. Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results.” (@Kilkenny2009-cn)\n\n\n\n\n\n\n\nImportant\n\n\n\nAs scientists, you - yourselves - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.\nYou cannot simply rely on the word of “experienced scientists” for this.\n\n\n\n\nThe Kilkenny paper does propose solutions to this problem\nThey require the use and reporting of straightforward statistical calculations\nIt is up to you as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods."
  },
  {
    "objectID": "02-nc3rs_arrive.html#the-arrive-guidelines",
    "href": "02-nc3rs_arrive.html#the-arrive-guidelines",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "The following year @Kilkenny2010-cp proposed the ARRIVE guidelines: a checklist to help researchers report their animal research transparently and reproducibly.\n\nGood reporting is essential for peer review and to inform future research\nReporting guidelines measurably improve reporting quality\nImproved reporting maximises the output of published research"
  },
  {
    "objectID": "02-nc3rs_arrive.html#arrive-guidelines-highlightes",
    "href": "02-nc3rs_arrive.html#arrive-guidelines-highlightes",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "Many journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (@Kilkenny2010-cp):\n\n\n\n\n\n\nTipARRIVE guidelines (highlights)\n\n\n\n\n\nObjectives: primary and any secondary objectives of the study, or specific hypotheses being tested\n\n\nStudy design: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit\n\n\nSample size: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided\n\n\nStatistical methods: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach\n\n\nOutcomes and estimation: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval)."
  },
  {
    "objectID": "02-nc3rs_arrive.html#a-vital-step",
    "href": "02-nc3rs_arrive.html#a-vital-step",
    "title": "The 2009 NC3Rs systematic survey",
    "section": "",
    "text": "Warning\n\n\n\n“A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues.”\n\n\n\n\n\n\n\n\nTipRecommended reading\n\n\n\n@Bate_Clark_2014"
  },
  {
    "objectID": "01-design.html",
    "href": "01-design.html",
    "title": "Why do we need experimental design?",
    "section": "",
    "text": "Indeed, why do we need experimental design?\nWell, in general it’s because if your experiment is designed poorly you may not get the answer you’re trying to find or - even worse - might get entirely the wrong answer to that question\nBut in this workshop we’re going to focus on why we need experimental design - and its conjoined sibling, statistics - in the context of experiments that involve animals\n\n\n\n\n\n\n\n\n\n\nImportantWe should always minimise suffering\n\n\n\nThis may mean not performing an experiment at all. Not all new knowledge or understanding is worth causing suffering to obtain it.\nWhere there is sufficient justification to perform an experiment, we are ethically obliged to minimise the amount of distress or suffering that is caused, by designing the experiment to achieve this.\n\n\n\n\n\n\n\n\nWarningWhy we need statistics\n\n\n\nIt may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.\nBut what is an acceptable (i.e. the least possible) amount of suffering necessary to obtain an informative result?\n\n\n\n\nNo-one likes talking about animal experiments.\nIt’s a difficult, emotive topic that crosses people’s moral red lines.\nAnimal experiments may be the only practical way to gain essential scientific knowledge\nWhatever you believe “suffering” means for an animal, our ethical premise is that this suffering should not be in vain\n\n\n\n\n\n\n\n\n\n\n\nWarningQuiz question\n\n\n\nSuppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. Which of the following choices will cause the least amount of suffering?\n\n\n\nUse three subjects per group so a standard deviation can be calculated\nUse just enough subjects to establish that the outcome is likely to be correct\nUse just enough subjects to be certain that the outcome is correct\nUse as many subjects as you have available, to avoid wastage\n\n\nWe carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is more likely or less likely to be the null hypothesis than some alternative), rather than 100% certain.\nIf we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering.\nIf we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering.\nIf we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering.\n\n\n\n\n\n\n\n\n\n\nTipThe appropriate number of subjects\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\n\nWhat assumptions are reasonable?\nWhat is an appropriate level of certainty?\n\nBy convention1 the usual level of certainty for a hypothesis test is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\nThis may sound like a very flexible statement. What assumptions are reasonable? What is an appropriate level of certainty?\nWe’ll consider these questions again later but, for now, just know that by convention the usual level of certainty is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”._\nNote though that the appropriate level of certainty may change depending on the nature of the question being asked.\n\n\n\n\n\n\n\n\n\n\nImportantExperimental design and statistics are intertwined\n\n\n\nOnce a research hypothesis has been devised:\n\nExperimental design is the process of devising a practical way of answering the question\nStatistics informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results\n\n\n\n\n\n\n\n\n\nWarningDesign your experiment for…\n\n\n\n\nyour population or subject group (e.g. sex, age, prior history, etc.)\nyour intervention (e.g. drug treatment)\nyour contrast or comparison between groups (e.g. lung capacity, drug concentration, etc.)\nyour outcome (i.e. is there a measurable or clinically relevant effect)\n\n\n\n\nOnce a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.\nStatistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.\nYou should design your experiment specifically for your combination of population/subject group, the intervention you’re applying, the contrast or comparison you’re making, and the outcome you’re expecting to see - specifically a measurable or clinically relevant effect."
  },
  {
    "objectID": "01-design.html#we-should-not-cause-unnecessary-suffering",
    "href": "01-design.html#we-should-not-cause-unnecessary-suffering",
    "title": "Why do we need experimental design?",
    "section": "",
    "text": "ImportantWe should always minimise suffering\n\n\n\nThis may mean not performing an experiment at all. Not all new knowledge or understanding is worth causing suffering to obtain it.\nWhere there is sufficient justification to perform an experiment, we are ethically obliged to minimise the amount of distress or suffering that is caused, by designing the experiment to achieve this.\n\n\n\n\n\n\n\n\nWarningWhy we need statistics\n\n\n\nIt may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.\nBut what is an acceptable (i.e. the least possible) amount of suffering necessary to obtain an informative result?\n\n\n\n\nNo-one likes talking about animal experiments.\nIt’s a difficult, emotive topic that crosses people’s moral red lines.\nAnimal experiments may be the only practical way to gain essential scientific knowledge\nWhatever you believe “suffering” means for an animal, our ethical premise is that this suffering should not be in vain"
  },
  {
    "objectID": "01-design.html#challenge",
    "href": "01-design.html#challenge",
    "title": "Why do we need experimental design?",
    "section": "",
    "text": "WarningQuiz question\n\n\n\nSuppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. Which of the following choices will cause the least amount of suffering?\n\n\n\nUse three subjects per group so a standard deviation can be calculated\nUse just enough subjects to establish that the outcome is likely to be correct\nUse just enough subjects to be certain that the outcome is correct\nUse as many subjects as you have available, to avoid wastage\n\n\nWe carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is more likely or less likely to be the null hypothesis than some alternative), rather than 100% certain.\nIf we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering.\nIf we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering.\nIf we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering."
  },
  {
    "objectID": "01-design.html#how-many-individuals",
    "href": "01-design.html#how-many-individuals",
    "title": "Why do we need experimental design?",
    "section": "",
    "text": "TipThe appropriate number of subjects\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\n\nWhat assumptions are reasonable?\nWhat is an appropriate level of certainty?\n\nBy convention1 the usual level of certainty for a hypothesis test is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\nThis may sound like a very flexible statement. What assumptions are reasonable? What is an appropriate level of certainty?\nWe’ll consider these questions again later but, for now, just know that by convention the usual level of certainty is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”._\nNote though that the appropriate level of certainty may change depending on the nature of the question being asked."
  },
  {
    "objectID": "01-design.html#design-experiments-to-minimise-suffering",
    "href": "01-design.html#design-experiments-to-minimise-suffering",
    "title": "Why do we need experimental design?",
    "section": "",
    "text": "ImportantExperimental design and statistics are intertwined\n\n\n\nOnce a research hypothesis has been devised:\n\nExperimental design is the process of devising a practical way of answering the question\nStatistics informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results\n\n\n\n\n\n\n\n\n\nWarningDesign your experiment for…\n\n\n\n\nyour population or subject group (e.g. sex, age, prior history, etc.)\nyour intervention (e.g. drug treatment)\nyour contrast or comparison between groups (e.g. lung capacity, drug concentration, etc.)\nyour outcome (i.e. is there a measurable or clinically relevant effect)\n\n\n\n\nOnce a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.\nStatistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.\nYou should design your experiment specifically for your combination of population/subject group, the intervention you’re applying, the contrast or comparison you’re making, and the outcome you’re expecting to see - specifically a measurable or clinically relevant effect."
  },
  {
    "objectID": "01-design.html#footnotes",
    "href": "01-design.html#footnotes",
    "title": "Why do we need experimental design?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConventions are guidelines, not rigid standards, and you should always consider whether a convention is appropriate in your use case↩︎"
  },
  {
    "objectID": "03-distributions.html",
    "href": "03-distributions.html",
    "title": "Some Statistical Concepts",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(ggpubr)\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\nlibrary(latex2exp)\n\n# Plot normal curves for of height counts for men and women separately and together\n# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html\n\nheights &lt;- data.frame(height=54:75,\n                      female=c(80,107,296,695,1612,2680,4645,8201,9948,11733,\n                               10270,9942,6181,3990,2131,1154,245,257,0,\n                               0,0,0)*10339/74167,\n                      male=c(0,0,0,0,0,0,0,542,668,1221,2175,4213,5535,7980,\n                             9566,9578,8867,6716,5019,2745,\n                             1464,1263)*9983/67552) %&gt;%\n  pivot_longer(c(female, male), names_to=\"sex\", values_to=\"count\")\n\nheights_f &lt;- ggplot(heights %&gt;% filter(sex==\"female\"), aes(x=height, y=count)) +\n  geom_col() + \n  stat_function(fun=function(x) 1e4 * dnorm(x, 63.7, 2.7),\n                color=\"orange\", lwd=2) +\n  xlim(53, 83) + ylim(0, 1700) +\n  labs(title=\"Heights of female adults (US)\") +\n  theme_minimal()\n\nheights_m &lt;- ggplot(heights %&gt;% filter(sex==\"male\"), aes(x=height, y=count)) +\n  geom_col() + \n  stat_function(fun=function(x) 1e4 * dnorm(x, 69.1, 2.9),\n                color=\"orange\", lwd=2) +  \n  xlim(53, 83) + ylim(0, 1700) +\n  labs(title=\"Heights of male adults (US)\") +\n  theme_minimal()\n\nheights_adults &lt;- ggplot(heights, aes(x=height, y=count)) +\n  geom_col() + \n  stat_function(fun=function(x) 2.1e4 * (0.52 * dnorm(x, 63.7, 2.7) +\n                                       0.48 * dnorm(x, 69.1, 2.9)),\n                color=\"orange\", lwd=2) +\n  xlim(53, 83) + ylim(0, 2100) +\n  labs(title=\"Heights of all adults (US)\") +\n  theme_minimal()\n\n# Example binomial and poisson distributions\n# A data frame of values 1:20, with some binomial and poisson distributed values\nbin_pois_data &lt;- data.frame(count=0:20) %&gt;%\n  mutate(pois1=dpois(count, 1)) %&gt;%\n  mutate(pois4=dpois(count, 4)) %&gt;%\n  mutate(pois10=dpois(count, 10)) %&gt;%\n  mutate(binom0=dbinom(count, 20, 0.05)) %&gt;%\n  mutate(binom1=dbinom(count, 20, 0.3)) %&gt;%\n  mutate(binom2=dbinom(count, 20, 0.5)) %&gt;%\n  mutate(binom3=dbinom(count, 40, 0.5)) %&gt;%\n  pivot_longer(cols=-count, names_to=\"params\", values_to=\"freq\")\n\n# Informative labels for legends\npoisson_labels &lt;- c(\"lambda=1\", \"lambda=4\", \"lambda=10\")\nbinomial_labels &lt;- c(\"n=20, p=0.01\", \"n=20, p=0.3\", \"n=20, p=0.7\", \"n=40, p=0.3\")\n\npoisson_dist &lt;- ggplot(bin_pois_data %&gt;% filter(str_detect(params, \"^pois\")), aes(x=count, color=params)) +\n  geom_line(aes(y=freq), alpha=0.5) +\n  geom_point(aes(y=freq)) +\n  xlim(-3,20) +\n  ylim(0, 0.4) +\n  xlab(\"counts/rate\") +\n  labs(title=\"Poisson distributions\") +\n  scale_color_discrete(labels=poisson_labels) +\n  coord_fixed(ratio=50) +\n  theme_minimal()\n\nbinomial_dist &lt;- ggplot(bin_pois_data %&gt;% filter(str_detect(params, \"^binom\")), aes(x=count, color=params)) +\n  geom_line(aes(y=freq), alpha=0.5) +\n  geom_point(aes(y=freq)) +\n  xlim(-3,20) +\n  ylim(0, 0.4) +\n  xlab(\"successes\") +\n  labs(title=\"Binomial distributions\") +\n  scale_color_discrete(labels=binomial_labels) +\n  coord_fixed(ratio=50) +\n  theme_minimal()"
  },
  {
    "objectID": "03-distributions.html#random-variables",
    "href": "03-distributions.html#random-variables",
    "title": "Some Statistical Concepts",
    "section": "Random variables",
    "text": "Random variables\nYour experimental measurements are random variables\n\n\n\n\n\n\nImportant\n\n\n\nThis does not mean that your measurements are entirely random numbers\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRandom variables are values whose range is subject to some element of chance, e.g. variation between individuals\n\nTail length (e.g. timing of developmental signals, distribution of nutrients)\nBlood concentrations (e.g. circulatory heterogeneity, transient measurement differences)\nSurvival time (e.g. determining point of death)\n\n\n\n\n\nWhen you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a random variable\n\nThis doesn’t mean that the number is chosen randomly\n\nWhat it means is that the number is subject to the influence of some kind of random variation\n\nSo tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals\nBlood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration\nSurvival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time"
  },
  {
    "objectID": "03-distributions.html#probability-distributions",
    "href": "03-distributions.html#probability-distributions",
    "title": "Some Statistical Concepts",
    "section": "Probability distributions",
    "text": "Probability distributions\nThe probability distribution of a random variable \\(z\\) (e.g. what you measure in an experiment) takes on some range of values1\n\n\n\n\n\n\nTipThe mean of the distribution of \\(z\\)\n\n\n\n\nThe mean (aka expected value or expectation) is the average of all the values in \\(z\\)\n\nEquivalently: the mean is the value that is obtained on average from a random sample from the distribution\n\nWritten as \\(\\mu_{z}\\) or \\(E(z)\\)\n\n\n\n\n\n\n\n\n\nTipThe variance of a distribution of \\(z\\)\n\n\n\n\nThe variance of the distribution of \\(z\\) represents the expected mean squared difference from the mean \\(\\mu_z\\) (or \\(E(z)\\)) of a random sample from the distribution.\n\n\\(\\textrm{variance} = E((z - \\mu_z)^2)\\)\n\n\n\n\n\n\nA probability distribution describes the range of values that a random variable - let’s call it \\(z\\) - takes.\n\nYou can think of \\(z\\) being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it\n\nProbability distributions can describe many measures, including\n\nheights of men, incomes of women, political party preference, and so on\n\nThe mean of a probability distribution - its expected value or expectation is the average of all numbers in the distribution\n\nThis may be infinite, and there is an equivalent definition: the mean is the value obtained on average from a random sample taken from the distribution\n\nThe variance of a probability distribution expresses how much the individual values might differ from that mean.\n\nVariance is defined as the mean of the square of the difference between each individual value in the distribution and the mean of the distribution"
  },
  {
    "objectID": "03-distributions.html#understanding-variance",
    "href": "03-distributions.html#understanding-variance",
    "title": "Some Statistical Concepts",
    "section": "Understanding variance",
    "text": "Understanding variance\n\n\n\n\n\n\nCautionA distribution where all values of \\(z\\) are the same\n\n\n\n\nEvery single value in the distribution (\\(z\\)) is also the mean value (\\(\\mu_z\\)), therefore\n\n\\[z = \\mu_z \\implies z - \\mu_z = 0 \\implies (z - \\mu_z)^2 = 0\\] \\[\\textrm{variance} = E((z - \\mu_z)^2) = E(0^2) = 0\\]\n\n\n\n\n\n\n\n\nTipAll other distributions\n\n\n\nIn every other distribution, there are some values of \\(z\\) that differ so, for at least some values of \\(z\\)\n\\[z \\neq \\mu_z \\implies  z - \\mu_z \\neq 0 \\implies (z - \\mu_z)^2 \\gt 0 \\] \\[\\implies \\textrm{variance} = E((z - \\mu_z)^2) \\gt 0 \\]\n\n\n\n\nTo get some intuition about the idea of variance, imagine that you have a set of values \\(z\\) that are all the same, so there’s no randomness\nIf the value are all the same, then they all have the same value as the mean of the distribution, and \\(z - \\mu_z = 0\\)\nThis means that, for all \\(z\\), \\((z - \\mu_z)^2\\) is also zero, and the variance is zero.\nBut if any value of \\(z\\) is different then, for at least that value, \\(z - \\mu_z \\neq 0\\), so the square of that value is greater than zero. It follows that the variance must then be greater than zero: \\(E((z - \\mu_z)^2) \\gt 0\\)\nIn any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value"
  },
  {
    "objectID": "03-distributions.html#standard-deviation",
    "href": "03-distributions.html#standard-deviation",
    "title": "Some Statistical Concepts",
    "section": "Standard deviation",
    "text": "Standard deviation\n\n\n\n\n\n\nWarningStandard deviation is the square root of the variance\n\n\n\n\\[\\textrm{standard deviation} = \\sigma_z = \\sqrt{\\textrm{variance}} = \\sqrt{E((z - \\mu_z)^2)} \\]\n\n\n\n\n\n\n\n\nTipAdvantages\n\n\n\n\nThe standard deviation (unlike variance) takes values on the same scale as the original distribution\n\nStandard deviation is a more “natural-seeming” interpretation of variation\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can calculate mean, variance, and standard deviation for any probability distribution.\n\n\n\n\nYou are probably more familiar with the standard deviation of a distribution.\nThe standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance\nWhile we can calculate the mean, variance and standard deviation for any distribution, this does not mean that they are equally informative for all distributions\nLet’s look at some examples"
  },
  {
    "objectID": "03-distributions.html#normal-distribution-1",
    "href": "03-distributions.html#normal-distribution-1",
    "title": "Some Statistical Concepts",
    "section": "Normal Distribution 1",
    "text": "Normal Distribution 1\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma_z) \\]\n\n\n\n\n\n\nNote\n\n\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\n\n\n\n\n\n\n\nTip\n\n\n\nMeasurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!\n\n\n\n\nYou may see the normal distribution written like this, as “z is distributed as the Normal distribution with mean \\(\\mu_z\\) and standard deviation \\(\\sigma_z\\).”\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\nValues we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors\n\nThis is a consequence of the Central Limit Theorem\n\nBut there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution\n\nThis is especially the case if there is a large factor affecting variation of the variable"
  },
  {
    "objectID": "03-distributions.html#normal-distribution-2",
    "href": "03-distributions.html#normal-distribution-2",
    "title": "Some Statistical Concepts",
    "section": "Normal Distribution 2",
    "text": "Normal Distribution 2\n\n# Plot normal curves for of height counts for men and women\n# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html\nlibrary(ggpubr)\n\nggarrange(heights_f, heights_m, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean\n\n\n\n\n\nAs an example, we can look at some real data, the heights of men and women in the US.\n\nHere we have counts of individuals whose heights are measured to the nearest inch\n\nThese are two different random variables: one for men, one for women\nFor both distributions, we can calculate a mean, and a standard deviation (or variance)\n\nTherefore we can calculate a normal distribution representing both men’s and women’s heights (orange curve)\nWe calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)\n\nThe distributions of heights for each sex, separately, follow an approximate normal distribution\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean"
  },
  {
    "objectID": "03-distributions.html#non-normal-distribution-3",
    "href": "03-distributions.html#non-normal-distribution-3",
    "title": "Some Statistical Concepts",
    "section": "(Non-)Normal Distribution 3",
    "text": "(Non-)Normal Distribution 3\n\n# Plot normal curves for of height counts for all us_adults\n# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html\n\nheights_adults\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\n\n\n\n\n\nBy contrast, the distribution of heights of all adults in the US is not close to a normal curve\nThis is because there is an extraneous factor, sex, that represents much of the total variation in values\n\nWe will come back to this idea later\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\nMost data you receive will not be normally distributed"
  },
  {
    "objectID": "03-distributions.html#binomial-distribution-1",
    "href": "03-distributions.html#binomial-distribution-1",
    "title": "Some Statistical Concepts",
    "section": "Binomial Distribution 1",
    "text": "Binomial Distribution 1\n\n\nSuppose you’re taking shots in basketball\n\nhow many shots?\nhow likely are you to score?\nwhat is the distribution of the number of successful shots?\n\n\n\n\n\n\n\nTip\n\n\n\nThis kind of process generates a random variable approximating a probability distribution called a binomial distribution.\nIt is different from a normal distribution.\n\n\n\n\n\n\n\n\nSuppose that instead of measuring height, weight, concentration or something like that, you’re measuring event outcomes\n\nThese do not follow a normal distribution\n\nIf you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding\nThe number of successful shots is going to depend on the number of shots you take, and how likely you are to score\n\nMichael Jordan is much more likely to score any individual attempt than I am\n\nThe number of successful shots is a random variable with a probability distribution\nThis kind of process generates a probability distribution that approximates the binomial distribution\n\nIt’s the same one you get for coin tosses (or any yes/no process)\n\nIt is different from a normal distribution\n\nIf your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution"
  },
  {
    "objectID": "03-distributions.html#binomial-distribution-2",
    "href": "03-distributions.html#binomial-distribution-2",
    "title": "Some Statistical Concepts",
    "section": "Binomial Distribution 2",
    "text": "Binomial Distribution 2\n\n\n\\[ z \\sim \\textrm{binomial}(n, p) \\]\n\n\n\n\n\n\nTip\n\n\n\n\nnumber of shots, \\(n = 20\\), probability of scoring, \\(p = 0.3\\)\n\n\\[z \\sim \\textrm{binomial}(20, 0.3) \\]\n\n\n\n\n\n\n\n\nCautionmean and sd\n\n\n\n\\[ \\textrm{mean} = n \\times p \\] \\[ \\textrm{sd} = \\sqrt{n \\times p \\times (1-p)}\\]\n\n\n\n\n\n\n\n\nImportantDesign note\n\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data. E.g., does \\(p\\) differ between two conditions?\n\n\n\n\n\n\n\n\nImagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot\nThe distribution of shots you scored would follow a binomial distribution with \\(n=20\\) and \\(p=0.3\\)\nYou’d expect to score, on average, \\(20 \\times 0.3 = 6\\) times\nThe standard deviation we’d expect would be \\(\\sqrt{20 \\times 0.3 \\times (1 - 0.3)} = \\sqrt{6 \\times (0.7)} = \\sqrt{4.2} = 2.05\\)\n\nSo you’d expect to score about 4 to 8 shots, most of the time"
  },
  {
    "objectID": "03-distributions.html#poisson-distribution-1",
    "href": "03-distributions.html#poisson-distribution-1",
    "title": "Some Statistical Concepts",
    "section": "Poisson distribution 1",
    "text": "Poisson distribution 1\n\n\n\nIn prior experiments the frequency of calcium events in WKY was 3.8 \\(\\pm\\) 1.1 events/field/min compared to 18.9 \\(\\pm\\) 7.1 in SHR\n\n\n\n\n\n\n\nWarningThis is not normal (or binomial)\n\n\n\nSomething that happens a certain number of times in a fixed interval generates a Poisson distribution.\nThis is different from a normal or binomial distribution.\n\n\n\n\n\n\n\n\nSuppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR\n\nYou count the number of times the calcium spikes in a minute, in your field of view\n\nThis data is rate data\n\nA count of events per unit (here, unit time and unit area) interval\n\nThe idealised representation of data generated by this process is a Poisson distribution\n\nThis differs from a normal or binomial distribution"
  },
  {
    "objectID": "03-distributions.html#poisson-distribution-2",
    "href": "03-distributions.html#poisson-distribution-2",
    "title": "Some Statistical Concepts",
    "section": "Poisson distribution 2",
    "text": "Poisson distribution 2\n\\[z \\sim \\textrm{poisson}(\\lambda)\\]\n\n\n\n\n\n\nTipPoisson distribution\n\n\n\n\\[ \\textrm{mean} = \\lambda \\] \\[ \\textrm{sd} = \\sqrt{\\lambda} \\]\n\n\n\n\n\n\n\n\nWarningExpectation (\\(\\lambda\\))\n\n\n\n\nOnly one parameter is provided, \\(\\lambda\\): the rate with which the measured event happens\nSuppose a county has population 100,000, and average rate of cancer is 45.2mn people each year\n\n\\[z \\sim \\textrm{poisson}(45,200,000/100,000) = \\textrm{poisson}(4.52) \\]\n\n\n\n\n\n\n\n\nImportantDesign note\n\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(\\lambda\\) differ between two conditions?\n\n\n\n\n\nCount or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the Poisson distribution\nExamples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.\nThe distribution takes only one parameter: the expectation, lambda\nIf your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions"
  },
  {
    "objectID": "03-distributions.html#binomial-and-poisson-distributions",
    "href": "03-distributions.html#binomial-and-poisson-distributions",
    "title": "Some Statistical Concepts",
    "section": "Binomial and Poisson distributions",
    "text": "Binomial and Poisson distributions\n\nlibrary(ggpubr)\n\nggarrange(binomial_dist, poisson_dist, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantSome important features\n\n\n\n\nAll measured values (and \\(n\\)) are positive whole numbers or zero; \\(\\lambda\\), \\(p\\) may be positive real numbers or zero\nThe distributions may not be unimodal\nThe mean is not always the peak value (mode)\nThe distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)\n\n\n\n\n\nBinomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings\n\nThe distributions are only defined at whole values - the lines are to guide your eye\n\nFor binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases\n\nMore attempts also increases the variance of the distribution\n\nFor poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value\nIn both cases, all values - counts/rates or successes - are positive whole numbers"
  },
  {
    "objectID": "03-distributions.html#distributions-in-practice",
    "href": "03-distributions.html#distributions-in-practice",
    "title": "Some Statistical Concepts",
    "section": "Distributions in Practice",
    "text": "Distributions in Practice\n\n\n\n\n\n\nCautionDistributions are starting points\n\n\n\n\nDistributions arise from and represent distinct generation processes (relate this to your biological system)\n\nNormal distributions are generated by sums, differences, and averages\nPoisson distributions are generated by counts (per unit interval)\nBinomial distributions are generated by success/failure outcomes\n\nDesign experiments with analyses that reflect these processes\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nAll statistical distributions are idealisations that ignore many features of real data\nNo real world data should be expected to exactly match any statistical distribution\nPoisson models tend to need adjustment for overdispersion\n\n\n\n\n\nThe different distributions we’ve looked at arise from distinct generation processes that have parallels in the real world\n\nNormal distributions arise when independent values are summed, or we take differences or averages of them\nPoisson distributions arise from counts, or rates (i.e. counts per unit)\nBinomial distributions arise from success/failure outcomes like tossing a coin\n\nWe need to take care though, as these distributions are idealised outcomes from those processes\nIn the real world there are many other influences that cause collected data to deviate from these idealisations\n\nThere is no reason to expect real world data to exactly match any statistical distribution\nThough there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)"
  },
  {
    "objectID": "03-distributions.html#normal-distribution-redux",
    "href": "03-distributions.html#normal-distribution-redux",
    "title": "Some Statistical Concepts",
    "section": "Normal Distribution Redux",
    "text": "Normal Distribution Redux\n\n# Normal distribution with standard deviation masses indicated\n# Example from https://avehtari.github.io/ROS-Examples/CentralLimitTheorem/heightweight.html\n\npar(mar=c(2,0,2,0), tck=-.01)\ncurve(dnorm(x), -4, 4, ylim=c(0, 0.4), xlab=\"\", ylab=\"\", bty=\"n\", yaxs=\"i\", main=\"normal distribution, mean=0 sd=1\", xaxt=\"n\", yaxt=\"n\")\naxis(1, c(-4, -3, -2, -1,  0,  1, 2, 3, 4), c(\"\", \"-3\", \"-2\", \"-1\",  \"0\",  \"1\", \"2\", \"3\", \"\"), mgp=c(1.5, .5, 0), cex.axis=1.2)\ncolors &lt;- c(\"gray70\", \"gray50\", \"gray30\")\nfor (i in 3:1){\n  grid &lt;- seq(-i, i, .01)\n  polygon(c(grid, i, -i), c(dnorm(grid), 0, 0), col=colors[i])\n}\ntext(0, .35*dnorm(0), \"68%\", cex=1.3)\ntext(-1.5, .3*dnorm(1.5), \"13.5%\", cex=1.3)\ntext(1.5, .3*dnorm(1.5), \"13.5%\", cex=1.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipProbability mass\n\n\n\n\napproximately 50% of the distribution lies in the range \\(\\mu \\pm 0.68\\sigma\\)\napproximately 68% of the distribution lies in the range \\(\\mu \\pm \\sigma\\)\napproximately 95% of the distribution lies in the range \\(\\mu \\pm 2\\sigma\\)\napproximately 99.7% of the distribution lies in the range \\(\\mu \\pm 3\\sigma\\)\n\n\n\n\n\nAn intuition worth developing is how much of a (normal) distribution lies within some range of the mean\n\nThis is going to be useful whenthinking about p-values and hypothesis tests\n\nHere we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.\n\nIt’s a normal distribution, so symmetrical about the mean\nThe area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution\nThe area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution\n\nIt will be useful to have this intuition for the next section"
  },
  {
    "objectID": "03-distributions.html#footnotes",
    "href": "03-distributions.html#footnotes",
    "title": "Some Statistical Concepts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we are considering a populations of an infinite number of measurements of \\(z\\)↩︎"
  },
  {
    "objectID": "05-significance.html",
    "href": "05-significance.html",
    "title": "Statistical significance and hypothesis testing",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2"
  },
  {
    "objectID": "05-significance.html#statistical-significance-1",
    "href": "05-significance.html#statistical-significance-1",
    "title": "Statistical significance and hypothesis testing",
    "section": "Statistical significance 1",
    "text": "Statistical significance 1\n\nSome scientists choose to consider a result to be “stable” or “real” if it is “statistically significant”\nThey may also consider “non-signifcant” results to be noisy or less reliable\n\n\n\n\n\n\n\nWarning\n\n\n\nI, and many other statisticians, do not recommend this approach.\nHowever, the concept is widespread and we need to discuss it\n\n\n\n\nEven if you’re not very familiar with what it means precisely, I’m sure you’ve come across “statistical” significance in at least one scientific context.\n“Statistical significance” is a decision rule used by some scientists to consider whether a result is “stable” or “real”\n\nThey may also use “statistical significance” to exclude some results as noisy or unreliable\n\nI and many other statisticians do not recommend this approach, and we’ll see why"
  },
  {
    "objectID": "05-significance.html#statistical-significance-2",
    "href": "05-significance.html#statistical-significance-2",
    "title": "Statistical significance and hypothesis testing",
    "section": "Statistical significance 2",
    "text": "Statistical significance 2\n\n\n\n\n\n\nCautionA common definition\n\n\n\n\nStatistical significance is conventionally defined as a threshold (commonly, a \\(p\\)-value less than 0.05) relative to some null hypothesis or prespecified value that indicates no effect is present.\nE.g., an estimate may be considered “statistically significant at \\(P &lt; 0.05\\)” if it:\n\nlies at least two standard errors from the mean\nis a difference that lies at least two standard errors from zero\n\nMore generally, an estimate is “not statistically significant” if, e.g.\n\nthe observed value can reasonably be explained by chance variation\nit is a difference that lies less than two standard errors from zero\n\n\n\n\n\n\n\n\n\n\nImportantMost tests rely on probability distributions\n\n\n\n\nWe need to relate the measured values in the real world to an appropriate distribution that approximates them\n\n\n\n\n\nThe way you’re likely to see statistical significance presented is that some threshold - usually a p-value of less than 0.05 - is applied in a statistical test, relative to some null hypothesis or prespecified value that indicates the absence of an effect\nSo you are likely to see an estimate be considered “statistically significant at P&lt;0.05” if it lies at least two SEs from the mean, or is a difference that lies at least two SEs from zero\nConversely, you will see estimates be considered “not statistically significant at P&lt;0.05” if they lie less than two SEs from the mean, or the observed value can reasonably be explained by chance variation alone\nAs you might have guessed from the talk of standard errors and distances from means, we map our measured values in the real world to statistical probability distributions to calculate these values."
  },
  {
    "objectID": "05-significance.html#a-simple-example-the-experiment",
    "href": "05-significance.html#a-simple-example-the-experiment",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: The experiment",
    "text": "A simple example: The experiment\n\n\n\n\n\n\nTipThe experiment\n\n\n\n\nTwo drugs, \\(C\\) and \\(T\\) lower cholesterol1, and we want to compare their effectiveness\nWe randomise assignment of \\(C\\) and \\(T\\) to members of a single cohort of comparable individuals, whose pre-treatment cholesterol level is assumed to be drawn from the same distribution (i.e. be approximately the same)\nWe measure the post-treatment cholesterol levels \\(y_T\\) and \\(y_C\\) for each individual in the two groups.\nWe calculate the average measured \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\nWe also calculate standard deviation for the two groups, \\(\\sigma_T\\) and \\(\\sigma_C\\)\n\n\n\n\n\n\nLet’s consider a simple experiment\nWe want to compare the efficacy of two drugs, C and T, in their ability to lower cholesterol\n\nWe assume a uniform pool of individuals, and randomly assign C and T to two equally-sized groups drawn from that cohort\nThis implies that the starting cholesterol level of all individuals is drawn from the same distribution\n\nWe administer the drugs in the same way, for the same period of time, and measure the post-treatment cholesterol level in each individual\n\nWe calculate the mean and standard deviation for the two groups\nThese allow us to estimate the underlying normal distribution of post-treatment cholesterol levels in each group\n\nWe can use the estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\)."
  },
  {
    "objectID": "05-significance.html#a-simple-example-the-hypotheses",
    "href": "05-significance.html#a-simple-example-the-hypotheses",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: The hypotheses",
    "text": "A simple example: The hypotheses\n\nWe want to know if the treatments have different sizes of effect\n\nIf they do, there should be a difference between the (average) post-treatment cholesterol level in each group\nThe true post-treatment levels are \\(\\theta_T\\) and \\(\\theta_C\\)\nWe have estimated means, \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for post-treatment levels\n\n\n\n\n\n\n\n\nWarningThe hypotheses\n\n\n\n\nWe are interested in \\(\\theta = \\theta_T - \\theta_C\\), the expected post-test difference in cholesterol between the two groups \\(T\\) and \\(C\\).\nOur null hypothesis (\\(H_0\\)) is that \\(\\theta = 0\\), i.e. there is no difference (\\(\\theta_C = \\theta_T\\))\nOur alternative hypothesis (\\(H_1\\)) is that there is a difference, so \\(\\theta \\neq 0\\), (i.e. \\(\\theta_C \\neq \\theta_T\\))\n\n\n\n\n\nOur goal in this experiment is to determine whether the treatments have different effects on lowering cholesterol\n\nIf they do, then there should be a difference between the (average) post-treatment cholesterol levels of groups T and C\n\nThere are true post treatment levels for each group: \\(\\theta_T\\) and \\(\\theta_C\\)\n\nBut we have only estimated them as \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\)\nSo we turn to a hypothesis test to determine how likely it is our estimates support there being a difference in the true levels between the two groups\n\nWe set up a null hypothesis that there is no difference between the groups: \\(\\theta = 0\\)\n\nIn this hypothesis, any deviation from zero we see is considered the result of chance variation in the data, or error we cannot eliminate from measurements\n\nWe also define an alternative hypothesis that there is a difference between the groups: \\(\\theta \\neq 0\\)\n\nWe’re not insisting that \\(\\theta\\) is greater than or less than zero, just that it’s not zero\nAnd also that any deviation from zero is not accounted for by the null hypothesis"
  },
  {
    "objectID": "05-significance.html#a-simple-example-the-distribution-1",
    "href": "05-significance.html#a-simple-example-the-distribution-1",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: The distribution 1",
    "text": "A simple example: The distribution 1\n\nTo perform a statistical test, we may assume a distribution and parameters for the null hypothesis\n\nWe can then test the observed estimate against that distribution to see how likely it is that the null hypothesis would have generated it\n\n\n\n\n\n\n\n\nNoteThe distribution\n\n\n\n\nWe use a probability distribution reflecting generation of the null hypothesis: \\(\\theta_C = \\theta_T\\)\n\nThis allows us to define a test statistic \\(T\\) (i.e. a threshold probability of “significance”) in advance\n\nWe test the estimated value from the experiment (\\(\\bar{y}_T - \\bar{y}_C\\)) to calculate a \\(p\\)-value for our estimate: \\(p = \\textrm{Pr}(T(y^{\\textrm{null}}) &gt; T(\\bar{y}_T - \\bar{y}_C))\\)\n\n\n\n\n\nAt this point we haven’t defined what our null hypothesis distribution should be, but we need to do so\n\nOnce we have this, we can test our observed estimate of \\(\\hat{\\theta} = \\bar{y}_T - \\bar{y}_C\\) against the distribution to see how likely it is to be explained by the null hypothesis\n\nWe need to choose a probability distribution that reflects the process generating the null hypothesis\n\nIn this case, we could shoose a normal distribution, which is typically appropriate for this kind of measurement\n\nThen we can define a test statistic \\(T\\) in advance\n\n\\(T\\) represents a threshold value in the null distribution - one where we would consider a larger difference to be meaningful\nYou may be familiar with setting P&lt;0.05, where the probability of seeing some value in the null distribution is 0.05 or less, and we might choose to set a value of \\(T\\) that corresponds to this\n\nWe then calculate the test statistic corresponding to our actual estimate, and calculate the \\(p\\)-value\n\nThis is the probability of observing our estimate, or a more extreme value, under the assumptions of the null model distribution"
  },
  {
    "objectID": "05-significance.html#a-simple-example-the-null-hypothesis",
    "href": "05-significance.html#a-simple-example-the-null-hypothesis",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: The null hypothesis",
    "text": "A simple example: The null hypothesis\n\n\n\n\n\n\nImportantThe null hypothesis\n\n\n\n\nAssume that the true difference \\(\\theta\\) is normally-distributed with \\(\\mu_\\theta=0\\), \\(\\sigma_\\theta=1\\)\n\n\n\n\nnormal_distplot(mu=0, sd=1)\n\n\n\n\n\n\n\n\n\n\nSuppose that the true difference between means is zero units, with standard error one unit, and that it is normally distributed\nThe probability distribution would look like this"
  },
  {
    "objectID": "05-significance.html#a-simple-example-the-estimated-difference",
    "href": "05-significance.html#a-simple-example-the-estimated-difference",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: The estimated difference",
    "text": "A simple example: The estimated difference\n\n\n\n\n\n\nImportantObserved between post-treatment levels: \\(\\bar{y}_T - \\bar{y}_C = -1.4\\)\n\n\n\n\nIs this an unlikely outcome given the null hypothesis?\n\n\n\n\nnormal_distplot(mu=0, sd=1) +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7\", \"darkorange1\", \"darkorange3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nSuppose that we measured the post treatment levels in each group, and found the difference between the means to be -1.4 units\n\nThat is, the \\(T\\) drug group had 1.4 units lower cholesterol than the \\(C\\) group\n\nIs that unlikely, given the null hypothesis?\nWell, we don’t know if it’s unlikely because we haven’t decided what “unlikely” means\n\nTo do that we need to set a significance threshold"
  },
  {
    "objectID": "05-significance.html#a-simple-example-a-significance-threshold",
    "href": "05-significance.html#a-simple-example-a-significance-threshold",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: A significance threshold",
    "text": "A simple example: A significance threshold\n\n\n\n\n\n\nImportantWe choose a significance threshold in advance\n\n\n\n\nSuppose we set a threshold \\(T\\) corresponding to the 90% confidence interval (i.e. \\(P&lt;0.1\\))\n\nIf the estimate is not in the central 90% of the distribution, we’ll say it’s “significant”\n\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.05, zend=0.95) +\n  ci_normal(mu=0, sd=1, ci=0.90)\n\n\n\n\n\n\n\n\n\n\nWe must always decide what our significance threshold is in advance\nSuppose that we decide the central 90% of the distribution/null hypothesis is “not significant”\n\nThis is the 90% confidence interval around the mean\nWe can shade this in to see it better"
  },
  {
    "objectID": "05-significance.html#a-simple-example-compare-the-estimate",
    "href": "05-significance.html#a-simple-example-compare-the-estimate",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: Compare the estimate",
    "text": "A simple example: Compare the estimate\n\n\n\n\n\n\nImportantCompare the estimate to the threshold\n\n\n\n\nThe estimate lies outwith the threshold, so we call the difference “significant”\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.05, zend=0.95) +\n  ci_normal(mu=0, sd=1, ci=0.90) +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7\", \"darkorange1\", \"darkorange3\")\n\n\n\n\n\n\n\n\n\n\nWe can overlay the observed difference on the distribution here, and see that the estimate lies outside the 90% confidence interval\n\nWith our stated assumption of a \\(P &lt; 0.1\\) significance threshold, we would call this difference “significant”"
  },
  {
    "objectID": "05-significance.html#a-simple-example-another-threshold",
    "href": "05-significance.html#a-simple-example-another-threshold",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: Another threshold",
    "text": "A simple example: Another threshold\n\n\n\n\n\n\nImportantWe choose a significance threshold in advance\n\n\n\n\nSuppose we set the threshold \\(T\\) corresponding to the 95% confidence interval (i.e. \\(P&lt;0.05\\)) instead?\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +\n  ci_normal(mu=0, sd=1, ci=0.90) +\n  ci_normal(mu=0, sd=1, ci=0.95)\n\n\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "05-significance.html#a-simple-example-another-outcome",
    "href": "05-significance.html#a-simple-example-another-outcome",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: Another outcome",
    "text": "A simple example: Another outcome\n\n\n\n\n\n\nImportantCompare the estimate to the threshold\n\n\n\n\nThe estimate lies within the threshold, so the difference is “not significant”\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +\n  ci_normal(mu=0, sd=1, ci=0.95) +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7\", \"purple\", \"purple\")\n\n\n\n\n\n\n\n\n\n\nSuppose that we had decided in advance that the central 95% of the distribution/null hypothesis was “not significant” instead\n\nThis is the 95% confidence interval around the mean"
  },
  {
    "objectID": "05-significance.html#a-simple-example-what-changed",
    "href": "05-significance.html#a-simple-example-what-changed",
    "title": "Statistical significance and hypothesis testing",
    "section": "A simple example: What changed?",
    "text": "A simple example: What changed?\n\n\n\n\n\n\nTipWhat did not change\n\n\n\n\nThe null hypothesis was the same\nThe observed estimate of difference was the same\n\n\n\n\n\n\n\n\n\nWarningWhat changed\n\n\n\n\nOur choice of significance threshold changed\n\n\n\n\n\n\n\n\n\nImportantSignificance threshold choice\n\n\n\n\nOnce the estimate is known, it is always possible to find a threshold that makes it “significant” or “not significant”\nIt is dishonest to select a threshold deliberately to make your result “significant” or “not significant”\nAlways choose and record (preregister) your threshold for significance ahead of the experiment"
  },
  {
    "objectID": "05-significance.html#tailed-tests-two-tailed",
    "href": "05-significance.html#tailed-tests-two-tailed",
    "title": "Statistical significance and hypothesis testing",
    "section": "Tailed tests: two-tailed",
    "text": "Tailed tests: two-tailed\n\n\n\n\n\n\nTipUse two tails if direction of change doesn’t matter\n\n\n\n\nWith a two-tailed hypothesis test, we do not care which direction of change is significant\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.025, zend=0.975) +\n  ci_normal(mu=0, sd=1, ci=0.95) +\n  add_x_marker(-2.1, 0.35, \"estimate = -2.1, significant\", \"darkorange1\", \"darkorange3\") +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7, not significant\", \"purple\", \"purple\") +\n  add_x_marker(1.2, 0.3, \"estimate = 1.2, not significant\", \"purple\", \"purple\") +\n  add_x_marker(3, 0.35, \"estimate = 3, significant\", \"darkorange1\", \"darkorange3\")\n\n\n\n\n\n\n\n\n\n\nThe example we’ve been looking at is a two-tailed test\nThis means that we’re concerned with whether the estimate lies within the central mass of the null distribution\n\nThe central 95% of the distribution has 2.5% of the distribution to the left, and 2.5% of the distribution to the right\nIf the estimate lies in either of those extreme regions, it’s in the “extreme 5%” from the mean, and we call it “significant”\nAny estimate lying within the central 95% of the distribution is “not significant”"
  },
  {
    "objectID": "05-significance.html#tailed-tests-one-tailed-left",
    "href": "05-significance.html#tailed-tests-one-tailed-left",
    "title": "Statistical significance and hypothesis testing",
    "section": "Tailed tests: one-tailed (left)",
    "text": "Tailed tests: one-tailed (left)\n\n\n\n\n\n\nTipUse one-tailed tests when direction matters\n\n\n\n\nIf we’re testing specifically for a significant negative difference/reduction, use a left-tailed test\ne.g. if we wanted to know if \\(T\\) reduced post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0.05, zend=1) +\n  add_x_marker(-2.1, 0.35, \"estimate = -2.1, significant\", \"purple\", \"purple\") +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7, significant\", \"purple\", \"purple\") +\n  add_x_marker(1.2, 0.3, \"estimate = 1.2, not significant\", \"darkorange1\", \"darkorange3\") +\n  add_x_marker(3, 0.35, \"estimate = 3, not significant\", \"darkorange1\", \"darkorange3\")\n\n\n\n\n\n\n\n\n\n\nWhen we know we want to test for change in a specific direction, we should use a one-tailed test\nWhen checking specifically for a negative change where \\(P &lt; 0.05\\), say that \\(T\\) reduces cholesterol level by more than \\(C\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the right-most 95% of the distribution mass\n\nThis means that results in the left-most 5% of the mass are “significant”\nAny estimate lying in the right-most 95% of the distribution is “not significant”"
  },
  {
    "objectID": "05-significance.html#tailed-tests-one-tailed-right",
    "href": "05-significance.html#tailed-tests-one-tailed-right",
    "title": "Statistical significance and hypothesis testing",
    "section": "Tailed tests: one-tailed (right)",
    "text": "Tailed tests: one-tailed (right)\n\n\n\n\n\n\nTipUse one-tailed tests when direction matters\n\n\n\n\nIf we’re testing specifically for a positive difference/increase, use a right-tailed test\ne.g. if we wanted to know if \\(T\\) increased post-test levels with respect to \\(C\\) at a threshold of \\(P &lt; 0.05\\)\n\n\n\n\nnormal_distplot(mu=0, sd=1) + \n  shade_normal(mu=0, sd=1, zstart=0, zend=0.95) +\n  add_x_marker(-2.1, 0.35, \"estimate = -2.1, not significant\", \"darkorange1\", \"darkorange3\") +\n  add_x_marker(-1.7, 0.3, \"estimate = -1.7, not significant\", \"darkorange1\", \"darkorange3\") +\n  add_x_marker(1.2, 0.3, \"estimate = 1.2, not significant\", \"darkorange1\", \"darkorange3\") +\n  add_x_marker(3, 0.35, \"estimate = 3, significant\", \"purple\", \"purple\")\n\n\n\n\n\n\n\n\n\n\nSimilarly, if checking specifically for a postitive change where \\(P &lt; 0.05\\), say that \\(C\\) reduces cholesterol level by more than \\(T\\), we would use a left-tailed test\nThis means that we’re concerned with whether the estimate lies outside the left-most 95% of the distribution mass\n\nThis means that results in the right-most 5% of the mass are “significant”\nAny estimate lying in the left-most 95% of the distribution is “not significant”\n\nIt’s possible to dishonestly switch a result between “significant” and “not significant” by choosing to use a one-tailed or two-tailed test"
  },
  {
    "objectID": "05-significance.html#problems-with-statistical-significance-1",
    "href": "05-significance.html#problems-with-statistical-significance-1",
    "title": "Statistical significance and hypothesis testing",
    "section": "Problems with statistical significance 1",
    "text": "Problems with statistical significance 1\n\n\n\n\n\n\nWarning\n\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\nImportantStatistical significance is not the same as practical importance\n\n\n\n\nSuppose a treatment increased earnings by £10 per year with a standard error of £2 (average salary £25,000).\n\nThis would be statistically, but not practically, significant\n\nSuppose a different treatment increased earnings by £10,000 per year with a standard error of £10,000\n\nThis would not be statistically significant, but could be important in practice"
  },
  {
    "objectID": "05-significance.html#problems-with-statistical-significance-2",
    "href": "05-significance.html#problems-with-statistical-significance-2",
    "title": "Statistical significance and hypothesis testing",
    "section": "Problems with statistical significance 2",
    "text": "Problems with statistical significance 2\n\n\n\n\n\n\nWarning\n\n\n\nIt is a common error to summarise comparisons by statistical significance into “significant” and “non-significant” results\n\n\n\n\n\n\n\n\nImportantNon-significance is not the same as zero\n\n\n\n\nSuppose an arterial stent treatment group outperforms the control\n\nmean difference in treadmill time: 16.6s (standard error 9.8)\nthe 95% confidence interval for the effect includes zero, \\(p ≈ 0.20\\)\n\nIt’s not clear whether the net treatment effect is positive or negative\n\nbut we can’t say that stents have no effect"
  },
  {
    "objectID": "05-significance.html#problems-with-statistical-significance-3",
    "href": "05-significance.html#problems-with-statistical-significance-3",
    "title": "Statistical significance and hypothesis testing",
    "section": "Problems with statistical significance 3",
    "text": "Problems with statistical significance 3\n\n\n\n\n\n\nImportantThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\n\nAt a \\(P&lt;0.05\\) threshold, only a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\nggplot() +\n  shaded_normal(10, 10, 0.1, 1, fill=\"orange\", color=\"darkorange3\", label=\"mean=10, sd=10\", textoffset=0.001) +\n  shaded_normal(25, 10, 0.01, 1, fill=\"purple\", color=\"purple\", label=\"mean=25, sd=10\", textoffset=0.001) +\n  annotate(\"segment\", colour=\"black\", lty=\"dashed\", x=0, xend=0, y=0, yend=0.05) +\n  xlim(-20, 60) +\n  ylim(-0.001, 0.05) +\n  xlab(\"effect estimate\") + \n  ylab(\"density\") +\n  theme_minimal()\n\nWarning: Removed 500 rows containing non-finite outside the scale range\n(`stat_align()`).\n\n\n\n\n\n\n\n\n\n\n\nIt should hopefully be obvious that any significance threshold is arbitrary, and the difference between a value being on one side of the threshold or the other may be arbitrarily small\nLess obviously, consider two independent studies - orange and purple here\n\nThe purple study estimates an effect size of 25 with standard error of 10 units and is “statistically significant” in that it does not include zero at the 1% significance level\nThe orange study estimates an effect size of 10 with standard error of 10 units and is “statistically not significant” as it includes zero even at the 10% significance level"
  },
  {
    "objectID": "05-significance.html#problems-with-statistical-significance-4",
    "href": "05-significance.html#problems-with-statistical-significance-4",
    "title": "Statistical significance and hypothesis testing",
    "section": "Problems with statistical significance 4",
    "text": "Problems with statistical significance 4\n\n\n\n\n\n\nImportantThe difference between ‘significant’ and ‘not significant’ is not statistically significant\n\n\n\n\nAt a \\(P&lt;0.05\\) threshold, only a small change is required to move from \\(P &lt; 0.051\\) to \\(P &lt; 0.049\\)\nLarge changes in significance can correspond to non-significant differences in the underlying variables\n\n\n\n\nggplot() +\n  shaded_normal(10, 10, 0.025, 0.975, fill=\"orange\", color=\"darkorange3\", label=\"mean=10, sd=10\", textoffset=0.001) +\n  shaded_normal(25, 10, 0.025, 0.975, fill=\"purple\", color=\"purple\", label=\"mean=25, sd=10\", textoffset=0.001) +\n  annotate(\"segment\", colour=\"black\", lty=\"dashed\", x=0, xend=0, y=0, yend=0.05) +\n  xlim(-20, 60) +\n  ylim(-0.001, 0.05) +\n  xlab(\"effect estimate\") + \n  ylab(\"density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHowever, the effect sizes estimated by the two studies are not different from each other at a “statistical significance” of 95%\n\nIn both cases, the central mass of each distribution includes the mean of the other"
  },
  {
    "objectID": "05-significance.html#footnotes",
    "href": "05-significance.html#footnotes",
    "title": "Statistical significance and hypothesis testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(C\\) for control, the current best-in class; \\(T\\) for treatment, the new compound↩︎"
  },
  {
    "objectID": "07-power.html",
    "href": "07-power.html",
    "title": "Statistical power",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2"
  },
  {
    "objectID": "07-power.html#statistical-power-1",
    "href": "07-power.html#statistical-power-1",
    "title": "Statistical power",
    "section": "Statistical power",
    "text": "Statistical power\n\n\n\n\n\n\nImportant\n\n\n\nStatistical power is defined as the probability, before a study is performed, that a particular comparison will achieve “statistical significance” at some predetermined level (e.g. \\(P &lt; 0.05\\)) given an assumed true effect size.\n\n\n\n\n\n\n\n\nWarningThe process\n\n\n\n\nHypothesise an appropriate effect size (e.g. what effect will improve health?)\nDetermine the \\(p\\)-value threshold you consider “statistically significant”\nMake reasoned assumptions about the variation in the data (e.g. what distribution? what variance?)\nChoose a sample size\nUse probability calculations to determine the chance that your observed \\(p\\)-value will be below the threshold (accept the null hypothesis) for the hypothesised effect size"
  },
  {
    "objectID": "07-power.html#effect-sizes",
    "href": "07-power.html#effect-sizes",
    "title": "Statistical power",
    "section": "Effect sizes",
    "text": "Effect sizes\n\n\n\n\n\n\nWarningPower analysis depends on an assumed effect size\n\n\n\n\nThe true effect size is almost never known ahead of time\n\nDetermining the effect size is usually why we’re doing the study\n\n\n\n\n\n\n\n\n\n\nTipHow to choose effect sizes\n\n\n\n\nTry a range of values consistent with relevant literature\nDetermine what value would be of practical interest (e.g. improvement in outcomes of 10%)\n\n\n\n\n\n\n\n\n\nImportantHow not to choose effect size\n\n\n\n\nDO NOT USE AN ESTIMATE FROM A SINGLE NOISY STUDY!\n\nNoisy studies suffer from The Winner’s Curse"
  },
  {
    "objectID": "07-power.html#the-winners-curse-1",
    "href": "07-power.html#the-winners-curse-1",
    "title": "Statistical power",
    "section": "The Winner’s Curse 1",
    "text": "The Winner’s Curse 1\n\n\n\n\n\n\nCautionA low-powered pilot study\n\n\n\n\nSuppose we ran a small pilot study with only a few individuals\nThe study, by design, has low statistical power\n\nThe variance of the data is relatively large, compared to the true effect size\n\n\n\n\n\n# Shaded normal curve for the Winner's curse - a low-powered pilot study\n# True effect size is 2%, SE=8\nggplot() + \n  shaded_normal(mu=2, sd=8, zstart=0.025, zend=0.93, xlabels=FALSE) +\n  add_x_marker(x=2, y=0.06, linecolor=\"purple\", textcolor=\"purple\", label=\"true effect size\", textoffset=0.002) +\n  add_x_marker(x=0, y=0.05, linecolor=\"darkorange\", textcolor=\"darkorange\", label=\"null effect size\", textoffset=0.002) +\n  xlim(-30, 30) +\n  xlab(\"Estimated effect size (percent)\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nTo see what I mean, suppose that we ran a pilot study for a treatment where the true effect size was only a two percentage point increase in the outcome, but this effect is estimated with a standard error of about eight percentage points.\n\nWe test this against the null hypothesis that the effect size is zero\nWe set up the null hypothesis so that the result of the study is “positive”/“statistically significant” if the measured difference is more than two standard errors away from zero - i.e. outwith the orange shading of the curve\nEven though the actual true mean effect is 2%, the larger standard error means that there is the possibility of the measured result falling outside that central shaded area"
  },
  {
    "objectID": "07-power.html#the-winners-curse-2",
    "href": "07-power.html#the-winners-curse-2",
    "title": "Statistical power",
    "section": "The Winner’s Curse 2",
    "text": "The Winner’s Curse 2\n\n\n\n\n\n\nCautionYou get a statistically significant result!\n\n\n\n\nYou think you won, but you lost! (The Winner’s Curse)\n\nThe estimate is either eight times too large (at least 16% instead of 2%) or\nThe estimate has the wrong sign (a negative change instead of positive)\n\n\n\n\n\n# Shaded normal curve for the Winner's curse - a low-powered pilot study\n# True effect size is 2%, SE=8\nggplot() + \n  shaded_normal(mu=2, sd=8, zstart=0.025, zend=0.93, xlabels=FALSE) +\n  add_x_marker(x=2, y=0.06, linecolor=\"purple\", textcolor=\"purple\", label=\"true effect size\", textoffset=0.002) +\n  add_x_marker(x=0, y=0.05, linecolor=\"darkorange\", textcolor=\"darkorange\", label=\"null effect size\", textoffset=0.002) +\n  shade_normal(mu=2, sd=8, zstart=0.93, zend=1, xlabels=FALSE, fill=\"plum4\") +\n  shade_normal(mu=2, sd=8, zstart=0.0, zend=0.025, xlabels=FALSE, fill=\"plum4\") +\n  annotate(\"text\", x=20, y=0.02, color=\"purple\",\n           label=c(\"You think you won \\n but your estimate \\n is over eight times \\n too high! \\n (≈7%) \")) + \n  annotate(\"text\", x=-18, y=0.02, color=\"purple\",\n           label=c(\"You think you won \\n but your estimate \\n has the wrong sign! \\n (≈2.5%)\")) + \n  xlim(-30, 30) +\n  xlab(\"Estimated effect size (percent)\") +\n  theme_minimal()\n\nWarning: Removed 400 rows containing non-finite outside the scale range\n(`stat_align()`).\n\n\n\n\n\n\n\n\n\n\n\nIn this low-powered experiment, any “statistically significant” result must fall into one of the dark purple sections of the curve at the extremes\n\nThe left-most 2.5% of outcomes or\nThe right-most 7% of outcomes\n\nThe problem is this:\n\n“Statistically significant” results on the right hand side overestimate the size of the true effect by at least eight times (minimum value 16%)\n“Statistically significant” results on the right hand side indicate that the effect is negative - a reduction in effect, when the true effect is small but positive"
  },
  {
    "objectID": "07-power.html#the-winners-curse-3",
    "href": "07-power.html#the-winners-curse-3",
    "title": "Statistical power",
    "section": "The Winner’s Curse 3",
    "text": "The Winner’s Curse 3\n\n\n\n\n\n\nImportantThe trap\n\n\n\nAny apparent success of low-powered studies masks larger failure\nWhen signal (effect size) is low and noise (standard error) is high, “statistically significant” results are likely to be wrong.\nLow-power studies tend not to replicate well\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLow-power studies have essentially no chance of providing useful information\nWe can say this even before data are collected\n\n\n\n\n\n\n\n\nCautionPublished results tend to be overestimates"
  },
  {
    "objectID": "07-power.html#statistical-power-and-ethics",
    "href": "07-power.html#statistical-power-and-ethics",
    "title": "Statistical power",
    "section": "Statistical power and ethics",
    "text": "Statistical power and ethics\n\n\n\n\n\n\nImportantIt is unethical to under-power animal studies\n\n\n\n\nUnder-powered in vivo experiments waste time and resources, lead to unnecessary animal suffering and result in erroneous biological conclusions (NC3Rs Experimental Design Assistant guide)\n\n\n\n\n\n\n\n\n\nCautionIt is unethical to over-power animal studies\n\n\n\n\nEthically, when working with animals we need to conduct a harm–benefit analysis to ensure the animal use is justified for the scientific gain. Experiments should be robust, not use more or fewer animals than necessary, and truly add to the knowledge base of science (@Karp2021-lc)\n\n\n\n\n\n\n\n\n\nWarningSo how should we appropriately power animal studies?"
  },
  {
    "objectID": "07-power.html#statistical-power-and-error-1",
    "href": "07-power.html#statistical-power-and-error-1",
    "title": "Statistical power",
    "section": "Statistical power and error 1",
    "text": "Statistical power and error 1\nWe often refer to two kinds of statistical error\n\n\n\n\n\n\nTipType I Error (\\(\\alpha\\))\n\n\n\n\nType I error is the probability of rejecting a null hypothesis, when the null hypothesis is true\n\nAlso known as a “false positive error”\n\nRepresented by the Greek letter \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\nNoteType II Error (\\(\\beta\\))\n\n\n\n\nType I error is the probability of accepting a null hypothesis, when the null hypothesis is false\n\nAlso known as a “false negative error”\n\nRepresented by the Greek letter \\(\\beta\\)\n\n\n\n\n\n\n\n\n\nImportantStatistical power is \\(1 - \\beta\\)"
  },
  {
    "objectID": "07-power.html#statistical-power-and-error-2",
    "href": "07-power.html#statistical-power-and-error-2",
    "title": "Statistical power",
    "section": "Statistical power and error 2",
    "text": "Statistical power and error 2\nStatistical power needs context: the expected error rates of the experiment at a given effect size, e.g.\n\nThe experiment has 80% power at \\(\\alpha = 0.05\\) for an effect size of 2mM/L\n\n\n\n\n\n\n\nTipHow to read this\n\n\n\n\n“an effect size of 2mM/L”: we are aiming to detect an effect of at least 2mM/L (e.g. blood glucose concentration)\n“\\(\\alpha = 0.05\\)”: we are using a significance test threshold (\\(\\alpha\\), type I error rate) of \\(P &lt; 0.05\\)\n“80% power”: we expect the study to report a significant effect, where one truly exists, 80% of the time"
  },
  {
    "objectID": "07-power.html#statistical-power-and-error-3",
    "href": "07-power.html#statistical-power-and-error-3",
    "title": "Statistical power",
    "section": "Statistical power and error 3",
    "text": "Statistical power and error 3\n\nThe experiment has 80% power at \\(\\alpha = 0.05\\) for an effect size of 2mM/L\n\n\n\n\n\n\n\nImportantIf the drug truly has no effect\n\n\n\n\nThe test has \\(\\alpha = 0.05\\), so we would expect to reject the null hypothesis incorrectly 5% of the time\nIf we ran the experiment 100 times, we would expect to see a result implying that the drug was effective five times\n\n\n\n\n\n\n\n\n\nTipIf the drug truly has an effect\n\n\n\n\nThe test has predicted power \\(1 - \\beta = 0.8\\), so the type II error rate \\(\\beta = 0.2\\) and we would expect to accept the null hypothesis incorrectly 20% of the time\nIf we ran the experiment 100 times, we would expect to see a result implying that the drug was effective eighty times"
  },
  {
    "objectID": "07-power.html#statistical-power-and-sample-size-1",
    "href": "07-power.html#statistical-power-and-sample-size-1",
    "title": "Statistical power",
    "section": "Statistical power and sample size 1",
    "text": "Statistical power and sample size 1\n\n\n\n\n\n\nTipWhat we need, to calculate appropriate sample size\n\n\n\n\nAn acceptable false positive rate (type I error, \\(\\alpha\\))\nAn acceptable false negative rate (type II error, \\(\\beta\\))\n\nThis is equivalent to knowing the target statistical power (\\(1 - \\beta\\))\n\nThe expected effect size and variance\nThe statistical test being performed\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need this information to calculate an appropriate, ethical sample size"
  },
  {
    "objectID": "07-power.html#statistical-power-and-sample-size-2",
    "href": "07-power.html#statistical-power-and-sample-size-2",
    "title": "Statistical power",
    "section": "Statistical power and sample size 2",
    "text": "Statistical power and sample size 2\n\n\n\n\n\n\nImportantTypical funders’ requirements\n\n\n\n\nFalse positive rate \\(\\alpha = 0.05\\)\nPower \\(1 - \\beta = 0.8\\) (80% power)\nThese are only a starting point - other values may be more appropriate depending on circumstance\n\n\n\n\n\n\n\n\n\nWarningUnder experimenter control\n\n\n\n\nEffect size and variance\nThe appropriate statistical approach"
  }
]