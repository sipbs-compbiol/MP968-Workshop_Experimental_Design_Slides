[
  {
    "objectID": "index.html#we-should-not-cause-unnecessary-suffering",
    "href": "index.html#we-should-not-cause-unnecessary-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "We should not cause unnecessary suffering",
    "text": "We should not cause unnecessary suffering\n\n\n\n\n\n\n\nWe should always minimise suffering\n\n\nThis may mean not performing an experiment at all. Not all new knowledge or understanding is worth causing suffering to obtain it.\nWhere there is sufficient justification to perform an experiment, we are ethically obliged to minimise the amount of distress or suffering that is caused, by designing the experiment to achieve this.\n\n\n\n\n\n\n\n\n\n\n\nWhy we need statistics\n\n\nIt may be easy to tell whether an animal is well-treated, or whether an experiment is necessary.\nBut what is an acceptable (i.e. the least possible) amount of suffering necessary to obtain an informative result?\n\n\n\n\n\n\nNo-one like talking about animal experiments.\nIt’s a difficult, emotive topic that crosses people’s moral red lines.\nAnimal experiments may be the only practical way to gain essential scientific knowledge\nWhatever you believe “suffering” means for an animal, our ethical premise is that this suffering should not be in vain"
  },
  {
    "objectID": "index.html#challenge",
    "href": "index.html#challenge",
    "title": "MP968 Experimental Design Workshop",
    "section": "Challenge",
    "text": "Challenge\n\n\n\n\n\n\n\nQuiz question\n\n\nSuppose you are running a necessary and useful experiment with animal subjects, where the use of animals is morally justified. You are comparing a treatment group to a control group. Which of the following choices will cause the least amount of suffering?\n\n\n\n\n\nUse three subjects per group so a standard deviation can be calculated\nUse just enough subjects to establish that the outcome is likely to be correct\nUse just enough subjects to be certain that the outcome is correct\nUse as many subjects as you have available, to avoid wastage\n\n\nWe carry out experiments to obtain answers to our scientific hypotheses, but the answers we obtain are rarely if ever 100% certain. We usually aim to obtain answers that are very likely to be correct (think about what a statistical hypothesis test means: that the explanation is more likely or less likely to be the null hypothesis than some alternative), rather than 100% certain.\nIf we use too few subjects we may still be able to perform a statistical test, but the results of the experiment will be more uncertain and may be more likely to be incorrect than correct. The use of animal subjects in an experiment that is unlikely to give a correct answer (e.g. because too few animals are used) causes unnecessary suffering.\nIf we attempt to obtain a 100% certain - or nearly so - result we may need to use many more subjects - possibly tens or hundreds more - than are required to obtain a result about which we are (say) 80% certain. The use of animal subjects to obtain a level of certainty greater than is needed to answer the question reasonably causes unnecessary suffering.\nIf we use the number of subjects that are available, just because it is convenient, then we may not know how likely the experiment is to give us a correct answer. The use of animal subjects in an experiment where you do not know how likely you are to get a correct answer is likely to cause unnecessary suffering."
  },
  {
    "objectID": "index.html#how-many-individuals",
    "href": "index.html#how-many-individuals",
    "title": "MP968 Experimental Design Workshop",
    "section": "How many individuals?",
    "text": "How many individuals?\n\n\n\n\n\n\n\nThe appropriate number of subjects\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\n\nWhat assumptions are reasonable?\nWhat is an appropriate level of certainty?\n\nBy convention1 the usual level of certainty for a hypothesis test is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”\n\n\n\n\n\nThe appropriate number of animal subjects to use in an experiment is always the smallest number that - given reasonable assumptions - will satisfactorily give the correct result to the desired level of certainty.\nThis may sound like a very flexible statement. What assumptions are reasonable? What is an appropriate level of certainty?\nWe’ll consider these questions again later but, for now, just know that by convention the usual level of certainty is: “we have an 80% chance of getting the correct true/false answer for the hypothesis being tested”._\nNote though that the appropriate level of certainty may change depending on the nature of the question being asked.\n\nConventions are guidelines, not rigid standards, and you should always consider whether a convention is appropriate in your use case"
  },
  {
    "objectID": "index.html#design-experiments-to-minimise-suffering",
    "href": "index.html#design-experiments-to-minimise-suffering",
    "title": "MP968 Experimental Design Workshop",
    "section": "Design experiments to minimise suffering",
    "text": "Design experiments to minimise suffering\n\n\n\n\n\n\n\nExperimental design and statistics are intertwined\n\n\nOnce a research hypothesis has been devised:\n\nExperimental design is the process of devising a practical way of answering the question\nStatistics informs the choices of variables, controls, numbers of individuals and groups, and the appropriate analysis of results\n\n\n\n\n\n\n\n\n\n\n\n\nDesign your experiment for…\n\n\n\nyour population or subject group (e.g. sex, age, prior history, etc.)\nyour intervention (e.g. drug treatment)\nyour contrast or comparison between groups (e.g. lung capacity, drug concentration, etc.)\nyour outcome (i.e. is there a measurable or clinically relevant effect)\n\n\n\n\n\n\nOnce a research hypothesis has been devised, Experimental Design is the process by which the practical means of answering that question is constructed. The design should aim to exclude extraneous or confounding influences on the experiment such that the causal factors are isolated and measurable, and any difference in outcome as a result of changing those factors (the “signals”) can also be measured cleanly.\nStatistics is the branch of applied science that allows us to make probabilistic inferences about our certainty in the “signal” - measurements, comparisons and experimental outcomes - even in the face of natural variations in processes and “noise,” and the way we choose small groups to represent populations.\nYou should design your experiment specifically for your combination of population/subject group, the intervention you’re applying, the contrast or comparison you’re making, and the outcome you’re expecting to see - specifically a measurable or clinically relevant effect."
  },
  {
    "objectID": "index.html#the-importance-of-experimental-design",
    "href": "index.html#the-importance-of-experimental-design",
    "title": "MP968 Experimental Design Workshop",
    "section": "The importance of experimental design",
    "text": "The importance of experimental design\n\n\n\n“For scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\nWe rely on the reporting of the experiment to know if it was appropriate\n\n\n\n\n\n\n\n\n\n\n\nThe National Centre for the Replacement, Refinement, and Reduction of Animals in Research (NC3Rs) was established in 2004 as the UK’s national organisation for the 3Rs (Reduction, Replacement, Refinement). It works with scientists to replace the use of animals by developing new approaches and technologies or, where use of animals is unavoidable, to reduce the number of animals used in each experiment and to minimise any pain, suffering or distress that the animals may experience.\nIn 2009, the NC3Rs published a systematic survey (Kilkenny et al. (2009)) of the quality of reporting, experimental design, and statistical analysis of recently-published biomedical research using animals.\nIt did not make for pleasant reading."
  },
  {
    "objectID": "index.html#causes-for-concern-1",
    "href": "index.html#causes-for-concern-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 1",
    "text": "Causes for concern 1\n\n\n\n“Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. […] Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the literature for good examples of experimental design\n\n\n\n\n\n\n\n\n\nThe state of the published literature around animal experiments was not good in 2009.\n\n40% of studies did not state the hypothesis or objective of the experiment\nMost papers did not use randomisation or blinding, although this is an essential practice to avoid bias\nOnly 70% of publications described their statistical methods at all"
  },
  {
    "objectID": "index.html#causes-for-concern-2",
    "href": "index.html#causes-for-concern-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Causes for concern 2",
    "text": "Causes for concern 2\n\n\n\n\n\n\n\nNo publication explained their choice for the number of animals used\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe cannot rely on the verbal authority of ‘published scientists’ or ‘experienced scientists’ for good experimental design\n\n\n\n\n\n\n\n\n\nOne of the most shocking pieces of information is that, of the 48 papers surveyed, not a single one explained why they used the number of animals that they did.\nWe cannot therefore be assured that the number of animals used was chosen to minimise suffering, or to obtain a statistically justifiable result.\nThese papers are published. They are written, and the experiments conducted, by “experienced scientists”\nBeing an “experienced” or “published” scientist is clearly not a benchmark for good experimental design"
  },
  {
    "objectID": "index.html#very-strong-cause-for-concern",
    "href": "index.html#very-strong-cause-for-concern",
    "title": "MP968 Experimental Design Workshop",
    "section": "Very strong cause for concern",
    "text": "Very strong cause for concern\n\n“Power analysis or other very simple calculations, which are widely used in human clinical trials and are often expected by regulatory authorities in some animal studies, can help to determine an appropriate number of animals to use in an experiment in order to detect a biologically important effect if there is one. This is a scientifically robust and efficient way of determining animal numbers and may ultimately help to prevent animals being used unnecessarily. Many of the studies that did report the number of animals used reported the numbers inconsistently between the methods and results sections. The reason for this is unclear, but this does pose a significant problem when analysing, interpreting and repeating the results.” (Kilkenny et al. (2009))\n\n\n\n\n\n\n\nImportant\n\n\nAs scientists, you - yourselves - need to understand the principles behind the statistical tests you use, in order to choose appropriate tests and methods, and to use appropriate measures to minimise animal suffering and obtain meaningful results.\nYou cannot simply rely on the word of “experienced scientists” for this.\n\n\n\n\n\nThe Kilkenny paper does propose solutions to this problem\nThey require the use and reporting of straightforward statistical calculations\nIt is up to you as scientists to maintain your integrity and that of the experiment, in abiding by good practice and choosing appropriate tests and methods."
  },
  {
    "objectID": "index.html#the-arrive-guidelines",
    "href": "index.html#the-arrive-guidelines",
    "title": "MP968 Experimental Design Workshop",
    "section": "The ARRIVE guidelines",
    "text": "The ARRIVE guidelines\n\n\nThe following year (Kilkenny et al. (2010)) proposed the ARRIVE guidelines: a checklist to help researchers report their animal research transparently and reproducibly.\n\nGood reporting is essential for peer review and to inform future research\nReporting guidelines measurably improve reporting quality\nImproved reporting maximises the output of published research"
  },
  {
    "objectID": "index.html#arrive-guidelines-highlightes",
    "href": "index.html#arrive-guidelines-highlightes",
    "title": "MP968 Experimental Design Workshop",
    "section": "ARRIVE guidelines highlightes",
    "text": "ARRIVE guidelines highlightes\nMany journals now routinely request information in the ARRIVE framework, often as electronic supplementary information. The framework covers 20 items including the following (Kilkenny et al. (2010)):\n\n\n\n\n\n\n\nARRIVE guidelines (highlights)\n\n\n\n\nObjectives: primary and any secondary objectives of the study, or specific hypotheses being tested\n\n\nStudy design: brief details of the study design, including the number of experimental and control groups, any steps taken to minimise the effects of subjective bias, and the experimental unit\n\n\nSample size: the total number of animals used in each experiment and the number of animals in each experimental group; how the number of animals was decided\n\n\nStatistical methods: details of the statistical methods used for each analysis; methods used to assess whether the data met the assumptions of the statistical approach\n\n\nOutcomes and estimation: results for each analysis carried out, with a measure of precision (e.g., standard error or confidence interval)."
  },
  {
    "objectID": "index.html#a-vital-step",
    "href": "index.html#a-vital-step",
    "title": "MP968 Experimental Design Workshop",
    "section": "A vital step",
    "text": "A vital step\n\n\n\n\n\n\n\n\nWarning\n\n\n“A key step in tackling these issues is to ensure that the next generation of scientists are aware of what makes for good practice in experimental design and animal research, and that they are not led into poor or inappropriate practices by more senior scientists without a proper grasp of these issues.”\n\n\n\n\n\n\n\n\n\n\nRecommended reading\n\n\nBate and Clark (2014)"
  },
  {
    "objectID": "index.html#random-variables",
    "href": "index.html#random-variables",
    "title": "MP968 Experimental Design Workshop",
    "section": "Random variables",
    "text": "Random variables\nYour experimental measurements are random variables\n\n\n\n\n\n\nImportant\n\n\nThis does not mean that your measurements are entirely random numbers\n\n\n\n\n\n\n\n\n\nCaution\n\n\nRandom variables are values whose range is subject to some element of chance, e.g. variation between individuals\n\nTail length (e.g. timing of developmental signals, distribution of nutrients)\nBlood concentrations (e.g. circulatory heterogeneity, transient measurement differences)\nSurvival time (e.g. determining point of death)\n\n\n\n\n\n\nWhen you take a measurement - tail length, concentration of something in blood, survival time, whatever - you are recording the value of a random variable\n\nThis doesn’t mean that the number is chosen randomly\n\nWhat it means is that the number is subject to the influence of some kind of random variation\n\nSo tail length might be under the influence of overall growth, genetic propensity, and some random redistribution of nutrients or random timing of developmental signals\nBlood concentration might not be entirely uniform, so the measurement is subject to random variations throughout the circulatory system, or transient effects on how you measure the concentration\nSurvival time might not be measured 100% accurately - it can difficult to measure the point of death exactly, so there may be some random variation around the actual time"
  },
  {
    "objectID": "index.html#probability-distributions",
    "href": "index.html#probability-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Probability distributions",
    "text": "Probability distributions\nThe probability distribution of a random variable \\(z\\) (e.g. the values you measure in an experiment) takes on some range of values\n\n\n\n\n\n\n\nThe mean of the distribution of \\(z\\)\n\n\n\nThe mean (aka expected value or expectation) is the average of all the values in \\(z\\)\n\nEquivalently: the mean is the value that is obtained on average from a random sample from the distribution\n\nWritten as \\(\\mu_{z}\\) or \\(E(z)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe variance of a distribution of \\(z\\)\n\n\n\nThe variance of the distribution of \\(z\\) represents the expected mean squared difference from the mean \\(\\mu_z\\) (or \\(E(z)\\)) of a random sample from the distribution.\n\n\\(\\textrm{variance} = E((z - \\mu_z)^2)\\)\n\n\n\n\n\n\n\n\nA probability distribution describes the range of values that a random variable - let’s call it \\(z\\) - takes.\n\nYou can think of \\(z\\) being a single ball drawn from a bag containing an infinite number of balls, each ball with a number written on it\n\nProbability distributions can describe many measures, including\n\nheights of men, incomes of women, political party preference, and so on\n\nThe mean of a probability distribution - its expected value or expectation is the average of all numbers in the distribution\n\nThis may be infinite, and there is an equivalent definition: the mean is the value obtained on average from a random sample taken from the distribution\n\nThe variance of a probability distribution expresses how much the individual values might differ from that mean.\n\nVariance is defined as the mean of the square of the difference between each individual value in the distribution and the mean of the distribution"
  },
  {
    "objectID": "index.html#understanding-variance",
    "href": "index.html#understanding-variance",
    "title": "MP968 Experimental Design Workshop",
    "section": "Understanding variance",
    "text": "Understanding variance\n\n\n\n\n\n\n\nA distribution where all values of \\(z\\) are the same\n\n\n\nEvery single value in the distribution (\\(z\\)) is also the mean value (\\(\\mu_z\\)), therefore\n\n\\[z = \\mu_z \\implies z - \\mu_z = 0 \\implies (z - \\mu_z)^2 = 0\\] \\[\\textrm{variance} = E((z - \\mu_z)^2) = E(0^2) = 0\\]\n\n\n\n\n\n\n\n\n\n\n\nAll other distributions\n\n\nIn every other distribution, there are some values of \\(z\\) that differ so, for at least some values of \\(z\\)\n\\[z \\neq \\mu_z \\implies  z - \\mu_z \\neq 0 \\implies (z - \\mu_z)^2 \\gt 0 \\] \\[\\implies \\textrm{variance} = E((z - \\mu_z)^2) \\gt 0 \\]\n\n\n\n\n\n\nTo get some intuition about the idea of variance, imagine that you have a set of values \\(z\\) that are all the same, so there’s no randomness\nIf the value are all the same, then they all have the same value as the mean of the distribution, and \\(z - \\mu_z = 0\\)\nThis means that, for all \\(z\\), \\((z - \\mu_z)^2\\) is also zero, and the variance is zero.\nBut if any value of \\(z\\) is different then, for at least that value, \\(z - \\mu_z \\neq 0\\), so the square of that value is greater than zero. It follows that the variance must then be greater than zero: \\(E((z - \\mu_z)^2) \\gt 0\\)\nIn any dataset you meet, you are unlikely to have all values be identical, and so the variance is going to take a positive value"
  },
  {
    "objectID": "index.html#standard-deviation",
    "href": "index.html#standard-deviation",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard deviation",
    "text": "Standard deviation\n\n\n\n\n\n\n\nWhat is standard deviation?\n\n\nThe standard deviation is the square root of the variance\n\\[\\textrm{standard deviation} = \\sigma_z = \\sqrt{\\textrm{variance}} = \\sqrt{E((z - \\mu_z)^2)} \\]\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\nThe standard deviation (unlike variance) is on the same scale as the original distribution\n\nStandard deviation is a more “natural-seeming” interpretation of variation\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe can calculate mean, variance, and standard deviation for any probability distribution.\n\n\n\n\n\nYou are probably more familiar with the standard deviation of a distribution.\nThe standard deviation is on the same kind of scale as the values of the distribution, which makes this an easier value to interpret than the variance\nWhile we can calculate the mean, variance and standard deviation for any distribution, this does not mean that they are equally informative for all distributions\nLet’s look at some examples"
  },
  {
    "objectID": "index.html#normal-distribution-1",
    "href": "index.html#normal-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 1",
    "text": "Normal Distribution 1\n\\[ z \\sim \\textrm{normal}(\\mu_z, \\sigma_z) \\]\n\n\n\n\n\n\nNote\n\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\n\n\n\n\n\n\n\n\nTip\n\n\nMeasurements of variables whose value is the sum of many small, independent, additive factors may follow a normal distribution\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThere is no reason to expect that a random variable representing direct measurements in the world will be normally distributed!\n\n\n\n\n\nYou may see the normal distribution written like this, as “z is distributed as the Normal distribution with mean \\(\\mu_z\\) and standard deviation \\(\\sigma_z\\).”\n\nWe only need to know the mean and standard deviation to define a unique normal distribution\n\nValues we measure in the real world should follow an approximate normal distribution if each measured value is the sum of many small, independent, additive factors\n\nThis is a consequence of the Central Limit Theorem\n\nBut there is no reason to expect that any random variable representing direct measurement will have this property, or follow a normal distribution\n\nThis is especially the case if there is a large factor affecting variation of the variable"
  },
  {
    "objectID": "index.html#normal-distribution-2",
    "href": "index.html#normal-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution 2",
    "text": "Normal Distribution 2\n\n\n\n\n\n\n\nTip\n\n\n\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so standard deviation describes variability equally well on both sides of the mean\n\n\n\n\n\n\nAs an example, we can look at some real data, the heights of men and women in the US.\n\nHere we have counts of individuals whose heights are measured to the nearest inch\n\nThese are two different random variables: one for men, one for women\nFor both distributions, we can calculate a mean, and a standard deviation (or variance)\n\nTherefore we can calculate a normal distribution representing both men’s and women’s heights (orange curve)\nWe calculate the mean heights for men and women (63.7 and 69.1 inches), and also the standard deviations (2.7 and 2.9 inches)\n\nThe distributions of heights for each sex, separately, follow an approximate normal distribution\nFor a normal distribution, the mean value is the value at the peak of the curve\nThe curve is symmetrical, so the standard deviation describes variability equally well on both sides of the mean"
  },
  {
    "objectID": "index.html#non-normal-distribution-3",
    "href": "index.html#non-normal-distribution-3",
    "title": "MP968 Experimental Design Workshop",
    "section": "(Non-)Normal Distribution 3",
    "text": "(Non-)Normal Distribution 3\n\n\n\n\n\n\n\nTip\n\n\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\n\n\n\n\n\n\nBy contrast, the distribution of heights of all adults in the US is not close to a normal curve\nThis is because there is an extraneous factor, sex, that represents much of the total variation in values\n\nWe will come back to this idea later\n\nHere, the mean may not be the same value as the peak of the curve (i.e. the mode)\nThe curve is asymmetrical, so standard deviation does not describe variation equally well on either side of the mean\nMost data you receive will not be normally distributed"
  },
  {
    "objectID": "index.html#binomial-distribution-1",
    "href": "index.html#binomial-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 1",
    "text": "Binomial Distribution 1\n\n\nSuppose you’re taking shots in basketball\n\nhow many shots?\nhow likely are you to score?\nwhat is the distribution of the number of successful shots?\n\n\n\n\n\n\n\nTip\n\n\nThis kind of process generates a random variable approximating a probability distribution called a binomial distribution.\nIt is different from a normal distribution.\n\n\n\n\n\n\n\n\nSuppose that instead of measuring height, weight, concentration or something like that, you’re measuring event outcomes\n\nThese do not follow a normal distribution\n\nIf you take a bunch of basketball shots (equivalent to our experimental events), each one has some probability of succeeding\nThe number of successful shots is going to depend on the number of shots you take, and how likely you are to score\n\nMichael Jordan is much more likely to score any individual attempt than I am\n\nThe number of successful shots is a random variable with a probability distribution\nThis kind of process generates a probability distribution that approximates the binomial distribution\n\nIt’s the same one you get for coin tosses (or any yes/no process)\n\nIt is different from a normal distribution\n\nIf your underlying biological process resembles coin tosses or basketball shots, you need to design your experiment and analysis to be using an appropriate statistical test, such as one based on the binomial distribution"
  },
  {
    "objectID": "index.html#binomial-distribution-2",
    "href": "index.html#binomial-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial Distribution 2",
    "text": "Binomial Distribution 2\n\n\n\\[ z \\sim \\textrm{binomial}(n, p) \\]\n\n\n\n\n\n\nTip\n\n\n\nnumber of shots, \\(n = 20\\)\nprobability of scoring, \\(p = 0.3\\)\n\n\\[z \\sim \\textrm{binomial}(20, 0.3) \\]\n\n\n\n\n\n\n\n\n\n\nmean and sd\n\n\n\\[ \\textrm{mean} = n \\times p \\] \\[ \\textrm{sd} = \\sqrt{n \\times p \\times (1-p)}\\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(p\\) differ between two conditions?\n\n\n\n\n\n\n\n\n\n\nImagine you took 20 shots at basketball, and had a probability of 0.3 of scoring any one shot\nThe distribution of shots you scored would follow a binomial distribution with \\(n=20\\) and \\(p=0.3\\)\nYou’d expect to score, on average, \\(20 \\times 0.3 = 6\\) times\nThe standard deviation we’d expect would be \\(\\sqrt{20 \\times 0.3 \\times (1 - 0.3)} = \\sqrt{6 \\times (0.7)} = \\sqrt{4.2} = 2.05\\)\n\nSo you’d expect to score about 4 to 8 shots, most of the time"
  },
  {
    "objectID": "index.html#poisson-distribution-1",
    "href": "index.html#poisson-distribution-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 1",
    "text": "Poisson distribution 1\n\n\n\nIn prior experiments the frequency of calcium events in WKY was 3.8 \\(\\pm\\) 1.1 events/field/min compared to 18.9 \\(\\pm\\) 7.1 in SHR\n\n\n\n\n\n\n\n\nThis is not normal (or binomial)\n\n\nSomething that happens a certain number of times in a fixed interval generates a Poisson distribution.\nThis is different from a normal or binomial distribution.\n\n\n\n\n\n\n\n\n\nSuppose you run an experiment to measure calcium spiking events in tissue for two different mouse lines, WKY and SHR\n\nYou count the number of times the calcium spikes in a minute, in your field of view\n\nThis data is rate data\n\nA count of events per unit (here, unit time and unit area) interval\n\nThe idealised representation of data generated by this process is a Poisson distribution\n\nThis differs from a normal or binomial distribution"
  },
  {
    "objectID": "index.html#poisson-distribution-2",
    "href": "index.html#poisson-distribution-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Poisson distribution 2",
    "text": "Poisson distribution 2\n\\[z \\sim \\textrm{poisson}(\\lambda)\\]\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\\[ \\textrm{mean} = \\lambda \\] \\[ \\textrm{sd} = \\sqrt{\\lambda} \\]\n\n\n\n\n\n\n\n\n\n\n\nExpectation (\\(\\lambda\\))\n\n\n\nOnly one parameter is provided, \\(\\lambda\\): the rate with which the measured event happens\nSuppose a county has population 100,000, and average rate of cancer is 45.2mn people each year\n\n\\[z \\sim \\textrm{poisson}(45,200,000/100,000) = \\textrm{poisson}(4.52) \\]\n\n\n\n\n\n\n\n\n\n\n\nDesign note\n\n\nYou need to design your experiments and analyses to reflect the appropriate process/probability distributions of your data\n\nE.g., does \\(\\lambda\\) differ between two conditions?\n\n\n\n\n\n\n\nCount or rate data, i.e. discrete events that happen in a given duration, volume or area, generate the Poisson distribution\nExamples of this kind of data include the number of cases of cancer in a county, the number of red cars you see on your journey into university, or the number of calcium concentration spikes you count in a survey period.\nThe distribution takes only one parameter: the expectation, lambda\nIf your experiment generates data of this kind, you need to use a test that distinguishes between the value of lambda between the two conditions"
  },
  {
    "objectID": "index.html#binomial-and-poisson-distributions",
    "href": "index.html#binomial-and-poisson-distributions",
    "title": "MP968 Experimental Design Workshop",
    "section": "Binomial and Poisson distributions",
    "text": "Binomial and Poisson distributions\n\n\n\n\n\n\n\n\nSome important features\n\n\n\nAll measured values are positive whole numbers or zero; means may be positive real numbers or zero\nThe distributions are unimodal\nThe mean is not always the peak value (mode)\nThe distributions are not always symmetrical (so sd may not describe variation equally either side of the mean)\n\n\n\n\n\n\n\nBinomial and poisson distributions are quite closely related, and they look like this for the indicated parameter settings\nFor binomial distributions the expected successes shift rightwards as the probability of success increases, and as the number of attempts increases\n\nMore attempts also increases the variance of the distribution\n\nFor poisson distributions, the peak of the distribution shifts rightwards as the expectation increases in value\nIn both cases, all values - counts/rates or successes - are positive whole numbers"
  },
  {
    "objectID": "index.html#distributions-in-practice",
    "href": "index.html#distributions-in-practice",
    "title": "MP968 Experimental Design Workshop",
    "section": "Distributions in Practice",
    "text": "Distributions in Practice\n\n\n\n\n\n\n\nDistributions are starting points\n\n\n\nDistributions arise from and represent distinct generation processes\n\nNormal distributions are good for sums, differences, and averages\nPoisson distributions are good for counts\nBinomial distributions are good for success/failure outcomes\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll statistical distributions are idealisations that ignore many features of real data\nNo real world data should be expected to exactly match any statistical distribution\nPoisson models tend to need adjustment for overdispersion\n\n\n\n\n\n\nThe different distributions we’ve looked at arise from distinct generation processes that have parallels in the real world\n\nNormal distributions arise when independent values are summed, or we take differences or averages of them\nPoisson distributions arise from counts, or rates (i.e. counts per unit)\nBinomial distributions arise from success/failure outcomes like tossing a coin\n\nWe need to take care though, as these distributions are idealised outcomes from those processes\nIn the real world there are many other influences that cause collected data to deviate from these idealisations\n\nThere is no reason to expect real world data to exactly match any statistical distribution\nThough there are some principles: we ought not to use a normal distribution to model count data (as the normal distribution can drop below zero where counts cannot)"
  },
  {
    "objectID": "index.html#normal-distribution-redux",
    "href": "index.html#normal-distribution-redux",
    "title": "MP968 Experimental Design Workshop",
    "section": "Normal Distribution Redux",
    "text": "Normal Distribution Redux\n\n\n\n\n\n\n\n\nProbability mass\n\n\n\napproximately 50% of the distribution lies in the range \\(\\mu \\pm 0.68\\sigma\\)\napproximately 68% of the distribution lies in the range \\(\\mu \\pm \\sigma\\)\napproximately 95% of the distribution lies in the range \\(\\mu \\pm 2\\sigma\\)\napproximately 99.7% of the distribution lies in the range \\(\\mu \\pm 3\\sigma\\)\n\n\n\n\n\n\n\nAn intuition worth developing is how much of a (normal) distribution lies within some range of the mean\nHere we have a normal distribution with a mean of zero and a standard deviation of 1, so the values on the x-axis represent standard deviations from the mean.\n\nThe area contained within the +1 and -1 standard deviation limits account for ≈68% of the total area of the distribution\nThe area contained within the +2 and -2 standard deviation limits account for ≈95% of the total area of the distribution\n\nIt will be useful to have this intuition for the next section"
  },
  {
    "objectID": "index.html#parameters",
    "href": "index.html#parameters",
    "title": "MP968 Experimental Design Workshop",
    "section": "Parameters",
    "text": "Parameters\nParameters are unknown numbers that determine a statistical model\n\n\n\n\n\n\n\nA linear regression\n\n\n\nSuppose you’re fitting a linear regression\n\nErrors \\(\\epsilon_i\\) are normally distributed with mean 0 and standard deviation \\(\\sigma\\)\n\n\n\\[ y_i = a + b x_i + \\epsilon_i \\]\n\nParameters are:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\n\\(\\sigma\\) (the error standard deviation)"
  },
  {
    "objectID": "index.html#estimands",
    "href": "index.html#estimands",
    "title": "MP968 Experimental Design Workshop",
    "section": "Estimands",
    "text": "Estimands\nAn estimand (or quantity of interest) is some summary of parameters that we are interested in estimating\n\n\n\n\n\n\n\nA linear regression\n\n\n\nFor the same linear regression\n\n\\[ y_i = a + b x_i + \\epsilon_i \\]\n\nWe want to estimate values for:\n\n\\(a\\) (the intercept)\n\\(b\\) (the gradient)\npredicted outcomes at important values of \\(x_i\\)\n\n\nThese are all estimands, and estimates are represented using the “hat” symbol: \\(\\hat{a}\\), \\(\\hat{b}\\), etc."
  },
  {
    "objectID": "index.html#standard-errors-and-confidence-intervals",
    "href": "index.html#standard-errors-and-confidence-intervals",
    "title": "MP968 Experimental Design Workshop",
    "section": "Standard Errors and Confidence Intervals",
    "text": "Standard Errors and Confidence Intervals\n\nThe standard error is the estimated standard deviation of an estimate\n\nIt is a measure of our uncertainty about the quaantity of interest\n\n\n\n\n\n\n\n\nNote\n\n\n\nStandard error gets smaller as sample size gets larger\n\nStandard error tends to zero as sample size gets large enough\n\n\n\n\n\n\nThe confidence interval (or CI) represents a range of values of a parameter or estimand that are roughly consistent with the data\n\n\n\n\n\n\n\nNote\n\n\n\nThe usual 95% confidence interval for large samples (assuming a normal distribution) is to take the estimate \\(\\pm\\) two standard errors"
  },
  {
    "objectID": "index.html#statistical-significance-1",
    "href": "index.html#statistical-significance-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 1",
    "text": "Statistical significance 1\n\nSome scientists choose to consider a result to be “stable” or “real” if it is statistically significant\nThey may also consider “non-signifcant” results to be noisy or less reliable\n\n\n\n\n\n\n\nWarning\n\n\nI, and many other statisticians, do not recommend this approach.\nHowever, the concept is widespread and we need to discuss it\n\n\n\n\n\nEven if you’re not very familiar with what it means precisely, I’m sure you’ve come across “statistical” significance in at least one scientific context.\n“Statistical significance” is a decision rule used by some scientists to consider whether a result is “stable” or “real”\n\nThey may also use “statistical significance” to exclude some results as noisy or unreliable\n\nI and many other statisticians do not recommend this approach, and we’ll see why"
  },
  {
    "objectID": "index.html#statistical-significance-2",
    "href": "index.html#statistical-significance-2",
    "title": "MP968 Experimental Design Workshop",
    "section": "Statistical significance 2",
    "text": "Statistical significance 2\n\n\n\n\n\n\n\nA common definition\n\n\n\nStatistical significance is conventionally defined as a threshold (commonly a \\(p\\)-value less than 0.05) relative to some null hypothesis or prespecified value, which would indicate no effect present.\nE.g., an estimate may be considered “statistically significant at \\(P &lt; 0.05\\)” if it:\n\nlies at least two standard errors1 from the mean\nor is a difference that lies at least two standard errors from zero\n\nMore generally, an estimate is “not statistically significant” if, e.g.\n\nthe observed value can reasonably be explained by chance variation\nis a difference that lies less than two standard errors from zero\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis process relies on probability distributions\n\n\n\nWe need to relate the measured values in the real world to an appropriate distribution\n\n\n\n\n\n\n\nThe way you’re likely to see statistical significance presented is that some threshold - usually a p-value of less than 0.05 - is applied in a statistical test, relative to some null hypothesis or prespecified value that indicates the absence of an effect\nSo you are likely to see an estimate be considered “statistically significant at P&lt;0.05” if it lies at least two SEs from the mean, or is a difference that lies at least two SEs from zero\nConversely, you will see estimates be considered “not statistically significant at P&lt;0.05” if they lie less than two SEs from the mean, or the observed value can reasonably be explained by chance variation alone\nAs you might have guessed from the talk of standard errors and distances from means, we map our measured values in the real world to statistical probability distributions to calculate these values.\n\n\nRemember the probability mass of the normal distribution?"
  },
  {
    "objectID": "index.html#a-simple-example",
    "href": "index.html#a-simple-example",
    "title": "MP968 Experimental Design Workshop",
    "section": "A simple example",
    "text": "A simple example\n\n\n\n\n\n\n\nThe experiment\n\n\n\nTwo drugs, \\(C\\) and \\(T\\) lower cholesterol1, and we want to compare their effectiveness\nWe randomise assignment of \\(C\\) and \\(T\\) to members of a single cohort of comparable individuals, whose pre-treatment cholesterol level is assumed to be drawn from the same distribution\nWe measure the average post-treatment cholesterol levels \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\) for the treatment and control groups as estimates for the true post-treatment levels \\(\\theta_T\\) and \\(\\theta_C\\).\n\n\n\n\n\n\n\n\n\n\n\n\nThe hypotheses\n\n\n\nWe are interested in \\(\\theta = \\theta_T - \\theta_C\\), the expected post-test difference in cholesterol between the two groups \\(T\\) and \\(C\\).\nOur null hypothesis is that \\(\\theta = 0\\), i.e. there is no difference (\\(\\theta_C = \\theta_T\\))\nOur alternative hypothesis is that there is a difference, so \\(\\theta \\neq 0\\), (i.e. \\(\\theta_C \\neq \\theta_T\\))\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution\n\n\n\nWe need to define a distribution for the null hypothesis\n\nThis allows us to define a test statistic (e.g. \\(T\\) for a t-test)\n\nThe distribution should be a reasonable representation of the process that generates the null hypothesis value\n\n\n\n\n\n\n\nLet’s consider a\n\n\n\\(C\\) for control, the current best-in class; \\(T\\) for treatment, the new compound"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "MP968 Experimental Design Workshop",
    "section": "References",
    "text": "References\n\n\n\n\nBate, Simon T., and Robin A. Clark. 2014. The Design and Statistical Analysis of Animal Experiments. Cambridge University Press.\n\n\nKilkenny, Carol, William J Browne, Innes C Cuthill, Michael Emerson, and Douglas G Altman. 2010. “Improving Bioscience Research Reporting: The ARRIVE Guidelines for Reporting Animal Research.” PLoS Biol. 8 (6): e1000412.\n\n\nKilkenny, Carol, Nick Parsons, Ed Kadyszewski, Michael F W Festing, Innes C Cuthill, Derek Fry, Jane Hutton, and Douglas G Altman. 2009. “Survey of the Quality of Experimental Design, Statistical Analysis and Reporting of Research Using Animals.” PLoS One 4 (11): e7824."
  }
]